#!/usr/bin/env python3
"""
å›¾åˆ†å‰²ç­–ç•¥è¯¦ç»†é…ç½®å’Œå®ç°
å±•ç¤ºå¦‚ä½•åœ¨JAXä¸­è¿›è¡Œç²¾ç¡®çš„å›¾åˆ†å‰²å’Œå‚æ•°åˆ†ç‰‡
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from functools import partial

# è®¾ç½®JAXç¯å¢ƒ
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
# æ¸…ç†å¯èƒ½å­˜åœ¨çš„æœ‰é—®é¢˜çš„XLA_FLAGS
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

try:
    import jax
    import jax.numpy as jnp
    from jax import random, devices
    from jax.sharding import Mesh, PartitionSpec, NamedSharding
    from jax.experimental import mesh_utils
    import flax.linen as nn
    import numpy as np
    print(f"âœ… JAX {jax.__version__} å›¾åˆ†å‰²æ¨¡å¼")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class GraphPartitionConfig:
    """å›¾åˆ†å‰²é…ç½®"""
    num_devices: int = 4
    mesh_shape: Tuple[int, int] = (2, 2)
    data_axis: str = 'data'
    model_axis: str = 'model'
    parameter_threshold: int = 512  # å¤§äºæ­¤é˜ˆå€¼çš„å‚æ•°è¿›è¡Œåˆ†ç‰‡
    
    # åˆ†ç‰‡ç­–ç•¥é…ç½®
    embedding_sharding: str = 'vocab'      # vocabç»´åº¦åˆ†ç‰‡
    attention_sharding: str = 'heads'      # æ³¨æ„åŠ›å¤´åˆ†ç‰‡
    mlp_sharding: str = 'hidden'           # éšè—å±‚åˆ†ç‰‡
    output_sharding: str = 'vocab'         # è¾“å‡ºå±‚åˆ†ç‰‡

class DetailedGraphPartitioner:
    """è¯¦ç»†çš„å›¾åˆ†å‰²å®ç°"""
    
    def __init__(self, config: GraphPartitionConfig):
        self.config = config
        self.devices = jax.devices()[:config.num_devices]
        self.mesh = None
        self.sharding_specs = {}
        
    def create_device_mesh(self):
        """åˆ›å»ºè®¾å¤‡ç½‘æ ¼"""
        print(f"\nğŸ”§ åˆ›å»ºè®¾å¤‡ç½‘æ ¼")
        print("-" * 30)
        
        try:
            if len(self.devices) >= 4:
                # 2x2ç½‘æ ¼ç”¨äº4ä¸ªGPU
                mesh_devices = mesh_utils.create_device_mesh(self.config.mesh_shape)
                self.mesh = Mesh(mesh_devices, axis_names=(self.config.data_axis, self.config.model_axis))
                print(f"âœ… åˆ›å»º {self.config.mesh_shape} mesh")
            elif len(self.devices) == 2:
                # 2x1ç½‘æ ¼ç”¨äº2ä¸ªGPU
                mesh_devices = mesh_utils.create_device_mesh((2, 1))
                self.mesh = Mesh(mesh_devices, axis_names=(self.config.data_axis, self.config.model_axis))
                print(f"âœ… åˆ›å»º (2,1) mesh")
            else:
                print(f"âš ï¸ è®¾å¤‡æ•°é‡ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºmesh")
                return False
                
            print(f"   è®¾å¤‡ç½‘æ ¼å½¢çŠ¶: {self.mesh.shape}")
            print(f"   è½´åç§°: {self.mesh.axis_names}")
            
            # æ‰“å°è®¾å¤‡åˆ†é…
            for i, device in enumerate(self.mesh.devices.flat):
                row, col = divmod(i, self.mesh.shape[1])
                print(f"   ä½ç½®({row},{col}): {device}")
                
            return True
            
        except Exception as e:
            print(f"âŒ ç½‘æ ¼åˆ›å»ºå¤±è´¥: {e}")
            return False
    
    def define_sharding_strategies(self):
        """å®šä¹‰è¯¦ç»†çš„åˆ†ç‰‡ç­–ç•¥"""
        print(f"\nğŸ“‹ å®šä¹‰åˆ†ç‰‡ç­–ç•¥")
        print("-" * 30)
        
        if not self.mesh:
            print("âš ï¸ æœªåˆ›å»ºmeshï¼Œæ— æ³•å®šä¹‰åˆ†ç‰‡ç­–ç•¥")
            return
        
        # ä¸ºä¸åŒç»„ä»¶å®šä¹‰åˆ†ç‰‡è§„åˆ™
        strategies = {
            # åµŒå…¥å±‚åˆ†ç‰‡ç­–ç•¥
            'embedding': {
                'weight': PartitionSpec(self.config.model_axis, None),  # è¯æ±‡è¡¨ç»´åº¦åˆ†ç‰‡
                'description': 'è¯æ±‡è¡¨åœ¨modelè½´åˆ†ç‰‡ï¼Œé™ä½å•è®¾å¤‡å†…å­˜å‹åŠ›'
            },
            
            # æ³¨æ„åŠ›å±‚åˆ†ç‰‡ç­–ç•¥
            'attention': {
                'qkv_weight': PartitionSpec(None, self.config.model_axis),  # æ³¨æ„åŠ›å¤´åˆ†ç‰‡
                'qkv_bias': PartitionSpec(self.config.model_axis),
                'output_weight': PartitionSpec(self.config.model_axis, None),
                'output_bias': PartitionSpec(None),  # å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
                'description': 'æ³¨æ„åŠ›å¤´åœ¨modelè½´åˆ†ç‰‡ï¼Œå®ç°å¤´å¹¶è¡Œ'
            },
            
            # MLPå±‚åˆ†ç‰‡ç­–ç•¥
            'mlp': {
                'dense1_weight': PartitionSpec(None, self.config.model_axis),  # éšè—å±‚åˆ†ç‰‡
                'dense1_bias': PartitionSpec(self.config.model_axis),
                'dense2_weight': PartitionSpec(self.config.model_axis, None),
                'dense2_bias': PartitionSpec(None),
                'description': 'MLPéšè—å±‚åœ¨modelè½´åˆ†ç‰‡ï¼Œå¹³è¡¡è®¡ç®—è´Ÿè½½'
            },
            
            # LayerNormåˆ†ç‰‡ç­–ç•¥
            'layernorm': {
                'scale': PartitionSpec(None),  # å¤åˆ¶
                'bias': PartitionSpec(None),   # å¤åˆ¶
                'description': 'LayerNormå‚æ•°å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡'
            },
            
            # è¾“å‡ºå±‚åˆ†ç‰‡ç­–ç•¥
            'output': {
                'weight': PartitionSpec(self.config.model_axis, None),  # è¯æ±‡è¡¨åˆ†ç‰‡
                'description': 'è¾“å‡ºå±‚è¯æ±‡è¡¨ç»´åº¦åˆ†ç‰‡'
            },
            
            # æ•°æ®åˆ†ç‰‡ç­–ç•¥
            'data': {
                'input_ids': PartitionSpec(self.config.data_axis, None),     # batchåˆ†ç‰‡
                'attention_mask': PartitionSpec(self.config.data_axis, None, None),
                'logits': PartitionSpec(self.config.data_axis, None, self.config.model_axis),
                'description': 'è¾“å…¥æ•°æ®åœ¨dataè½´åˆ†ç‰‡ï¼Œå®ç°æ•°æ®å¹¶è¡Œ'
            }
        }
        
        self.sharding_specs = strategies
        
        # æ‰“å°åˆ†ç‰‡ç­–ç•¥
        for component, specs in strategies.items():
            print(f"\nğŸ” {component.upper()} åˆ†ç‰‡ç­–ç•¥:")
            print(f"   æè¿°: {specs['description']}")
            for param_name, spec in specs.items():
                if param_name != 'description':
                    print(f"   {param_name}: {spec}")
    
    def analyze_parameter_distribution(self, model_config):
        """åˆ†æå‚æ•°åˆ†å¸ƒ"""
        print(f"\nğŸ“Š å‚æ•°åˆ†å¸ƒåˆ†æ")
        print("-" * 30)
        
        # æ¨¡æ‹ŸGPT-1.5Bå‚æ•°
        vocab_size = 50257
        n_embd = 1600
        n_layer = 48
        n_head = 25
        
        params = {
            'embedding': {
                'token_embedding': (vocab_size, n_embd),
                'position_embedding': (2048, n_embd)
            },
            'transformer_blocks': {},
            'output': {
                'lm_head': (n_embd, vocab_size)
            }
        }
        
        # æ¯ä¸ªTransformerå—çš„å‚æ•°
        for layer_idx in range(n_layer):
            layer_params = {
                'attention': {
                    'qkv_weight': (n_embd, 3 * n_embd),
                    'qkv_bias': (3 * n_embd,),
                    'output_weight': (n_embd, n_embd),
                    'output_bias': (n_embd,)
                },
                'mlp': {
                    'dense1_weight': (n_embd, 4 * n_embd),
                    'dense1_bias': (4 * n_embd,),
                    'dense2_weight': (4 * n_embd, n_embd),
                    'dense2_bias': (n_embd,)
                },
                'layernorm1': {
                    'scale': (n_embd,),
                    'bias': (n_embd,)
                },
                'layernorm2': {
                    'scale': (n_embd,),
                    'bias': (n_embd,)
                }
            }
            params['transformer_blocks'][f'layer_{layer_idx}'] = layer_params
        
        # åˆ†ææ¯ä¸ªè®¾å¤‡çš„å‚æ•°åˆ†å¸ƒ
        device_params = {f'device_{i}': 0 for i in range(len(self.devices))}
        total_params = 0
        
        print(f"ğŸ“ˆ å‚æ•°åˆ†å¸ƒç»Ÿè®¡:")
        
        # è®¡ç®—åµŒå…¥å±‚å‚æ•°
        embed_params = vocab_size * n_embd + 2048 * n_embd
        sharded_embed = embed_params // len(self.devices)  # è¯æ±‡è¡¨åˆ†ç‰‡
        for i in range(len(self.devices)):
            device_params[f'device_{i}'] += sharded_embed
        total_params += embed_params
        print(f"   åµŒå…¥å±‚: {embed_params:,} å‚æ•° â†’ æ¯è®¾å¤‡: {sharded_embed:,}")
        
        # è®¡ç®—Transformerå±‚å‚æ•°
        layer_param_count = (
            n_embd * 3 * n_embd +  # QKV
            n_embd * n_embd +      # attention output
            n_embd * 4 * n_embd +  # MLP dense1
            4 * n_embd * n_embd +  # MLP dense2
            4 * n_embd             # biases + layernorms
        )
        
        transformer_params = n_layer * layer_param_count
        # æ³¨æ„åŠ›å¤´å’ŒMLPåˆ†ç‰‡
        sharded_transformer = transformer_params // len(self.devices)
        for i in range(len(self.devices)):
            device_params[f'device_{i}'] += sharded_transformer
        total_params += transformer_params
        print(f"   Transformer: {transformer_params:,} å‚æ•° â†’ æ¯è®¾å¤‡: {sharded_transformer:,}")
        
        # è®¡ç®—è¾“å‡ºå±‚å‚æ•°
        output_params = n_embd * vocab_size
        sharded_output = output_params // len(self.devices)  # è¯æ±‡è¡¨åˆ†ç‰‡
        for i in range(len(self.devices)):
            device_params[f'device_{i}'] += sharded_output
        total_params += output_params
        print(f"   è¾“å‡ºå±‚: {output_params:,} å‚æ•° â†’ æ¯è®¾å¤‡: {sharded_output:,}")
        
        print(f"\nğŸ“Š è®¾å¤‡è´Ÿè½½å¹³è¡¡:")
        for device, count in device_params.items():
            percentage = (count / total_params) * 100
            memory_gb = count * 4 / (1024**3)  # float32
            print(f"   {device}: {count:,} å‚æ•° ({percentage:.1f}%) â‰ˆ {memory_gb:.2f}GB")
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e9:.2f}B)")
        print(f"   å¹³å‡æ¯è®¾å¤‡: {total_params//len(self.devices):,}")
        print(f"   è´Ÿè½½å‡è¡¡åº¦: {(min(device_params.values())/max(device_params.values()))*100:.1f}%")
        
        return {
            'total_params': total_params,
            'device_distribution': device_params,
            'memory_per_device_gb': max(device_params.values()) * 4 / (1024**3)
        }
    
    def demonstrate_sharding_execution(self):
        """æ¼”ç¤ºåˆ†ç‰‡æ‰§è¡Œè¿‡ç¨‹"""
        print(f"\nğŸš€ åˆ†ç‰‡æ‰§è¡Œæ¼”ç¤º")
        print("-" * 30)
        
        if not self.mesh:
            print("âš ï¸ æœªåˆ›å»ºmeshï¼Œæ— æ³•æ¼”ç¤ºåˆ†ç‰‡")
            return None
            
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ¨¡å‹å‚æ•°
        key = jax.random.PRNGKey(42)
        
        # æ¨¡æ‹Ÿå‚æ•°å­—å…¸
        params = {
            'embedding': {
                'weight': jax.random.normal(key, (50257, 1600))  # è¯æ±‡è¡¨ x åµŒå…¥ç»´åº¦
            },
            'attention': {
                'qkv_weight': jax.random.normal(key, (1600, 4800)),  # 3 * n_embd
                'output_weight': jax.random.normal(key, (1600, 1600))
            },
            'mlp': {
                'dense1_weight': jax.random.normal(key, (1600, 6400)),  # 4 * n_embd
                'dense2_weight': jax.random.normal(key, (6400, 1600))
            }
        }
        
        print(f"ğŸ“¦ åŸå§‹å‚æ•°å½¢çŠ¶:")
        for component, comp_params in params.items():
            for param_name, param in comp_params.items():
                print(f"   {component}.{param_name}: {param.shape}")
        
        # åº”ç”¨åˆ†ç‰‡
        with self.mesh:
            sharded_params = {}
            
            for component, comp_params in params.items():
                sharded_params[component] = {}
                
                for param_name, param in comp_params.items():
                    # æ ¹æ®ç»„ä»¶ç±»å‹é€‰æ‹©åˆ†ç‰‡ç­–ç•¥
                    if component == 'embedding' and 'weight' in param_name:
                        spec = PartitionSpec(self.config.model_axis, None)  # è¯æ±‡è¡¨åˆ†ç‰‡
                    elif component == 'attention':
                        if 'qkv' in param_name:
                            spec = PartitionSpec(None, self.config.model_axis)  # æ³¨æ„åŠ›å¤´åˆ†ç‰‡
                        else:
                            spec = PartitionSpec(self.config.model_axis, None)
                    elif component == 'mlp':
                        if 'dense1' in param_name:
                            spec = PartitionSpec(None, self.config.model_axis)  # éšè—å±‚åˆ†ç‰‡
                        else:
                            spec = PartitionSpec(self.config.model_axis, None)
                    else:
                        spec = PartitionSpec()  # ä¸åˆ†ç‰‡
                    
                    # åˆ›å»ºåˆ†ç‰‡
                    sharding = NamedSharding(self.mesh, spec)
                    sharded_param = jax.device_put(param, sharding)
                    sharded_params[component][param_name] = sharded_param
                    
                    print(f"   âœ… {component}.{param_name}: {spec} â†’ å·²åˆ†ç‰‡")
        
        print(f"\nğŸ¯ åˆ†ç‰‡æ‰§è¡Œå®Œæˆ!")
        return sharded_params
    
    def create_performance_prediction(self):
        """åˆ›å»ºæ€§èƒ½é¢„æµ‹"""
        print(f"\nğŸ“ˆ æ€§èƒ½é¢„æµ‹åˆ†æ")
        print("-" * 30)
        
        # åŸºäº4ä¸ªRTX 3090çš„æ€§èƒ½é¢„æµ‹
        gpu_memory_gb = 24
        gpu_compute_tflops = 35.6  # RTX 3090ç†è®ºå³°å€¼
        
        # æ¨¡å‹é…ç½®
        vocab_size = 50257
        n_embd = 1600
        n_layer = 48
        batch_size = 32
        seq_len = 512
        
        # è®¡ç®—å†…å­˜éœ€æ±‚
        param_memory = 1.5e9 * 4 / (1024**3)  # 1.5Bå‚æ•°ï¼Œfloat32
        activation_memory = batch_size * seq_len * n_embd * 4 / (1024**3)
        
        # å•GPU vs å¤šGPUå¯¹æ¯”
        scenarios = {
            'å•GPU': {
                'devices': 1,
                'param_memory_per_gpu': param_memory,
                'activation_memory_per_gpu': activation_memory,
                'compute_efficiency': 0.6,  # å•GPUæ•ˆç‡
                'communication_overhead': 0.0
            },
            'æ•°æ®å¹¶è¡Œ(4GPU)': {
                'devices': 4,
                'param_memory_per_gpu': param_memory,  # æ¯ä¸ªGPUéƒ½æœ‰å®Œæ•´å‚æ•°
                'activation_memory_per_gpu': activation_memory / 4,  # æ¿€æ´»å€¼åˆ†ç‰‡
                'compute_efficiency': 0.8,  # æ•°æ®å¹¶è¡Œæ•ˆç‡
                'communication_overhead': 0.1  # AllReduceé€šä¿¡
            },
            'æ¨¡å‹å¹¶è¡Œ(4GPU)': {
                'devices': 4,
                'param_memory_per_gpu': param_memory / 4,  # å‚æ•°åˆ†ç‰‡
                'activation_memory_per_gpu': activation_memory,  # å®Œæ•´æ¿€æ´»å€¼
                'compute_efficiency': 0.7,  # æ¨¡å‹å¹¶è¡Œæ•ˆç‡ï¼ˆé€šä¿¡å¼€é”€è¾ƒå¤§ï¼‰
                'communication_overhead': 0.2  # å‚æ•°é€šä¿¡
            },
            'æ··åˆå¹¶è¡Œ(4GPU)': {
                'devices': 4,
                'param_memory_per_gpu': param_memory / 2,  # 2x2æ··åˆ
                'activation_memory_per_gpu': activation_memory / 2,
                'compute_efficiency': 0.85,  # æœ€ä¼˜æ•ˆç‡
                'communication_overhead': 0.15  # å¹³è¡¡çš„é€šä¿¡
            }
        }
        
        print(f"ğŸ’¾ å†…å­˜ä½¿ç”¨é¢„æµ‹:")
        for scenario, config in scenarios.items():
            total_memory = config['param_memory_per_gpu'] + config['activation_memory_per_gpu']
            memory_utilization = (total_memory / gpu_memory_gb) * 100
            
            print(f"\n   {scenario}:")
            print(f"     å‚æ•°å†…å­˜: {config['param_memory_per_gpu']:.2f}GB/GPU")
            print(f"     æ¿€æ´»å†…å­˜: {config['activation_memory_per_gpu']:.2f}GB/GPU")
            print(f"     æ€»å†…å­˜: {total_memory:.2f}GB/GPU")
            print(f"     å†…å­˜åˆ©ç”¨ç‡: {memory_utilization:.1f}%")
            
            # æ€§èƒ½é¢„æµ‹
            theoretical_tflops = gpu_compute_tflops * config['devices']
            effective_tflops = theoretical_tflops * config['compute_efficiency'] * (1 - config['communication_overhead'])
            speedup = effective_tflops / (gpu_compute_tflops * 0.6)  # ç›¸å¯¹äºå•GPU
            
            print(f"     ç†è®ºç®—åŠ›: {theoretical_tflops:.1f} TFLOPS")
            print(f"     æœ‰æ•ˆç®—åŠ›: {effective_tflops:.1f} TFLOPS")
            print(f"     ç›¸å¯¹åŠ é€Ÿ: {speedup:.2f}x")
        
        return scenarios

def main():
    """ä¸»å‡½æ•° - è¯¦ç»†å›¾åˆ†å‰²åˆ†æ"""
    print(f"ğŸ” è¯¦ç»†å›¾åˆ†å‰²ç­–ç•¥åˆ†æ")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®
    config = GraphPartitionConfig(
        num_devices=len(jax.devices()),
        mesh_shape=(2, 2) if len(jax.devices()) >= 4 else (2, 1)
    )
    
    partitioner = DetailedGraphPartitioner(config)
    
    try:
        # 1. åˆ›å»ºè®¾å¤‡ç½‘æ ¼
        mesh_success = partitioner.create_device_mesh()
        
        if mesh_success:
            # 2. å®šä¹‰åˆ†ç‰‡ç­–ç•¥
            partitioner.define_sharding_strategies()
            
            # 3. åˆ†æå‚æ•°åˆ†å¸ƒ
            param_analysis = partitioner.analyze_parameter_distribution(config)
            
            # 4. æ¼”ç¤ºåˆ†ç‰‡æ‰§è¡Œ
            sharded_params = partitioner.demonstrate_sharding_execution()
            
            # 5. æ€§èƒ½é¢„æµ‹
            performance_prediction = partitioner.create_performance_prediction()
            
            # 6. ä¿å­˜ç»“æœ
            results = {
                'config': config.__dict__,
                'mesh_info': {
                    'shape': partitioner.mesh.shape,
                    'axis_names': partitioner.mesh.axis_names,
                    'device_count': len(partitioner.devices)
                },
                'parameter_analysis': param_analysis,
                'performance_prediction': performance_prediction,
                'sharding_specs': {
                    component: {k: str(v) for k, v in specs.items() if k != 'description'}
                    for component, specs in partitioner.sharding_specs.items()
                }
            }
            
            results_file = Path("detailed_graph_partition_analysis.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜: {results_file}")
            
            # 7. æ€»ç»“
            print(f"\nğŸ¯ å›¾åˆ†å‰²åˆ†ææ€»ç»“")
            print("=" * 40)
            print(f"ğŸ“Š å…³é”®æŒ‡æ ‡:")
            print(f"   è®¾å¤‡æ•°é‡: {len(partitioner.devices)}")
            print(f"   ç½‘æ ¼é…ç½®: {partitioner.mesh.shape}")
            print(f"   æ€»å‚æ•°é‡: {param_analysis['total_params']/1e9:.2f}B")
            print(f"   æ¯è®¾å¤‡å†…å­˜: {param_analysis['memory_per_device_gb']:.2f}GB")
            
            print(f"\nğŸ’¡ å…³é”®ä¼˜åŠ¿:")
            print(f"   1. ç²¾ç¡®çš„å‚æ•°åˆ†ç‰‡å‡å°‘å•è®¾å¤‡å†…å­˜å‹åŠ›")
            print(f"   2. æ³¨æ„åŠ›å¤´å¹¶è¡Œæé«˜è®¡ç®—æ•ˆç‡")
            print(f"   3. æ··åˆå¹¶è¡Œç­–ç•¥å¹³è¡¡å†…å­˜å’Œè®¡ç®—")
            print(f"   4. è´Ÿè½½å‡è¡¡ç¡®ä¿è®¾å¤‡åˆ©ç”¨ç‡")
            
        else:
            print(f"âŒ æ— æ³•åˆ›å»ºè®¾å¤‡ç½‘æ ¼ï¼Œè¯·æ£€æŸ¥GPUè®¾å¤‡æ•°é‡")
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
