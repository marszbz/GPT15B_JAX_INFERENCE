{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8cc89c",
   "metadata": {},
   "source": [
    "# Multi-GPU JAX Parallelization for GPT-1.5B Inference\n",
    "\n",
    "本笔记本专门用于分析和优化GPT-1.5B JAX推理系统的多GPU并行化策略。\n",
    "\n",
    "## 目标\n",
    "1. 分析当前的多头注意力实现\n",
    "2. 实现JAX mesh和分片策略\n",
    "3. 优化多GPU内存使用\n",
    "4. 提高推理吞吐量\n",
    "5. 解决mesh创建问题\n",
    "\n",
    "## 环境信息\n",
    "- 系统: Ubuntu 22.04\n",
    "- GPU: 4x RTX 3090\n",
    "- JAX版本: 0.6.1 with CUDA support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9ac47",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "导入JAX、Flax、NumPy以及其他必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "# 设置JAX环境（必须在导入JAX之前）\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'\n",
    "# 清理可能存在的XLA_FLAGS\n",
    "if 'XLA_FLAGS' in os.environ:\n",
    "    del os.environ['XLA_FLAGS']\n",
    "\n",
    "# 导入JAX相关包\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, pmap, devices, device_count\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "import flax.linen as nn\n",
    "from flax import jax_utils\n",
    "import numpy as np\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {len(jax.devices())}\")\n",
    "for i, device in enumerate(jax.devices()):\n",
    "    print(f\"  Device {i}: {device}\")\n",
    "\n",
    "# 检查是否有CUDA支持\n",
    "if jax.devices()[0].platform == 'gpu':\n",
    "    print(\"✅ CUDA support detected\")\n",
    "else:\n",
    "    print(\"⚠️ No CUDA support detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd187f8",
   "metadata": {},
   "source": [
    "## Section 2: Define Configuration and Model Classes\n",
    "\n",
    "定义GPT配置和多头注意力类的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62570c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProgressiveGPTConfig:\n",
    "    \"\"\"渐进式GPT配置\"\"\"\n",
    "    vocab_size: int = 50257\n",
    "    n_positions: int = 1024\n",
    "    n_embd: int = 768\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    dropout: float = 0.1\n",
    "    use_bias: bool = True\n",
    "    \n",
    "    def get_param_count(self) -> int:\n",
    "        \"\"\"估算参数量\"\"\"\n",
    "        # 嵌入层: vocab_size * n_embd + n_positions * n_embd\n",
    "        embed_params = self.vocab_size * self.n_embd + self.n_positions * self.n_embd\n",
    "        \n",
    "        # 每个transformer层的参数\n",
    "        # 注意力: 4 * n_embd^2 (qkv + output projection)\n",
    "        # MLP: 2 * n_embd * (4 * n_embd) = 8 * n_embd^2\n",
    "        # LayerNorm: 2 * n_embd * 2 = 4 * n_embd\n",
    "        layer_params = (4 * self.n_embd * self.n_embd + \n",
    "                       8 * self.n_embd * self.n_embd + \n",
    "                       4 * self.n_embd)\n",
    "        \n",
    "        # 总参数 = 嵌入 + 层数 * 每层参数 + 最终LM头\n",
    "        total_params = embed_params + self.n_layer * layer_params + self.vocab_size * self.n_embd\n",
    "        return total_params\n",
    "\n",
    "\n",
    "class ProgressiveMultiHeadAttention(nn.Module):\n",
    "    \"\"\"渐进式多头注意力\"\"\"\n",
    "    config: ProgressiveGPTConfig\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # QKV投影\n",
    "        qkv = nn.Dense(3 * self.config.n_embd, use_bias=self.config.use_bias)(x)\n",
    "        qkv = qkv.reshape(B, T, 3, self.config.n_head, C // self.config.n_head)\n",
    "        qkv = qkv.transpose(2, 0, 3, 1, 4)  # (3, B, nh, T, hs)\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # 每个的形状: (B, nh, T, hs)\n",
    "        \n",
    "        # 注意力权重 - 正确的转置维度\n",
    "        # k.shape = (B, nh, T, hs), 我们想转置最后两个维度 T 和 hs\n",
    "        att = (q @ k.transpose(0, 1, 3, 2)) * (1.0 / jnp.sqrt(k.shape[-1]))\n",
    "        \n",
    "        # 应用因果掩码\n",
    "        if mask is not None:\n",
    "            att = jnp.where(mask, att, -jnp.inf)\n",
    "        \n",
    "        att = jax.nn.softmax(att, axis=-1)\n",
    "        \n",
    "        # 应用注意力到值\n",
    "        y = att @ v  # (B, nh, T, hs)\n",
    "        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)\n",
    "        \n",
    "        # 输出投影\n",
    "        return nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)(y)\n",
    "\n",
    "print(\"✅ Model classes defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ac036",
   "metadata": {},
   "source": [
    "## Section 3: Multi-GPU Mesh Creation and Analysis\n",
    "\n",
    "创建多GPU mesh并分析当前的并行化策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gpu_setup():\n",
    "    \"\"\"分析当前GPU设置\"\"\"\n",
    "    devices = jax.devices()\n",
    "    print(f\"GPU Analysis:\")\n",
    "    print(f\"  Total devices: {len(devices)}\")\n",
    "    print(f\"  Device platform: {devices[0].platform}\")\n",
    "    \n",
    "    # 获取设备内存信息\n",
    "    for i, device in enumerate(devices):\n",
    "        print(f\"  Device {i}: {device}\")\n",
    "        print(f\"    Platform: {device.platform}\")\n",
    "        print(f\"    Device kind: {device.device_kind}\")\n",
    "    \n",
    "    return devices\n",
    "\n",
    "def create_device_mesh(devices):\n",
    "    \"\"\"创建设备mesh用于并行化\"\"\"\n",
    "    if len(devices) == 1:\n",
    "        print(\"⚠️ Only 1 device available, no parallelization possible\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 尝试创建2D mesh: (data_parallel, model_parallel)\n",
    "        if len(devices) == 4:\n",
    "            # 4个GPU: 2x2 mesh或者4x1 mesh\n",
    "            mesh_shape = (2, 2)\n",
    "            mesh_devices = mesh_utils.create_device_mesh(mesh_shape)\n",
    "            mesh = Mesh(mesh_devices, axis_names=('data', 'model'))\n",
    "            print(f\"✅ Created 2x2 mesh with axes ('data', 'model')\")\n",
    "        elif len(devices) == 2:\n",
    "            mesh_shape = (2, 1)\n",
    "            mesh_devices = mesh_utils.create_device_mesh(mesh_shape)\n",
    "            mesh = Mesh(mesh_devices, axis_names=('data', 'model'))\n",
    "            print(f\"✅ Created 2x1 mesh with axes ('data', 'model')\")\n",
    "        else:\n",
    "            # 对于其他情况，使用1D mesh\n",
    "            mesh_devices = np.array(devices).reshape(-1, 1)\n",
    "            mesh = Mesh(mesh_devices, axis_names=('data',))\n",
    "            print(f\"✅ Created 1D mesh with {len(devices)} devices\")\n",
    "        \n",
    "        print(f\"  Mesh shape: {mesh.shape}\")\n",
    "        print(f\"  Mesh axis names: {mesh.axis_names}\")\n",
    "        return mesh\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create mesh: {e}\")\n",
    "        return None\n",
    "\n",
    "# 分析GPU设置\n",
    "devices = analyze_gpu_setup()\n",
    "mesh = create_device_mesh(devices)\n",
    "\n",
    "if mesh:\n",
    "    print(f\"\\n🎯 Mesh created successfully!\")\n",
    "    print(f\"  Can use data parallelism: {'data' in mesh.axis_names}\")\n",
    "    print(f\"  Can use model parallelism: {'model' in mesh.axis_names}\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ No mesh created, will use single device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652a79c",
   "metadata": {},
   "source": [
    "## Section 4: Initialize Model Configuration for Testing\n",
    "\n",
    "创建用于测试的模型配置实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建不同规模的测试配置\n",
    "configs = {\n",
    "    'small': ProgressiveGPTConfig(\n",
    "        n_embd=512, n_layer=8, n_head=8, n_positions=512\n",
    "    ),\n",
    "    'medium': ProgressiveGPTConfig(\n",
    "        n_embd=768, n_layer=12, n_head=12, n_positions=1024\n",
    "    ),\n",
    "    'large': ProgressiveGPTConfig(\n",
    "        n_embd=1024, n_layer=24, n_head=16, n_positions=1024\n",
    "    ),\n",
    "    'xlarge': ProgressiveGPTConfig(\n",
    "        n_embd=1280, n_layer=36, n_head=20, n_positions=1024\n",
    "    )\n",
    "}\n",
    "\n",
    "# 打印配置信息\n",
    "for name, config in configs.items():\n",
    "    param_count = config.get_param_count()\n",
    "    print(f\"{name.upper()} Config:\")\n",
    "    print(f\"  Embedding dim: {config.n_embd}\")\n",
    "    print(f\"  Layers: {config.n_layer}\")\n",
    "    print(f\"  Heads: {config.n_head}\")\n",
    "    print(f\"  Max positions: {config.n_positions}\")\n",
    "    print(f\"  Estimated params: {param_count:,} ({param_count/1e6:.1f}M)\")\n",
    "    print(f\"  Estimated memory: {param_count * 4 / (1024**3):.2f} GB\")\n",
    "    print()\n",
    "\n",
    "# 选择测试配置\n",
    "test_config = configs['medium']\n",
    "print(f\"🎯 Using MEDIUM config for testing\")\n",
    "print(f\"  Parameters: {test_config.get_param_count()/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f923d",
   "metadata": {},
   "source": [
    "## Section 5: Test Attention Mechanism with Dummy Data\n",
    "\n",
    "使用虚拟数据测试多头注意力机制的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578eb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(config, batch_size=2, seq_len=32):\n",
    "    \"\"\"创建测试数据\"\"\"\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # 创建输入数据 (batch_size, seq_len, n_embd)\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, config.n_embd))\n",
    "    \n",
    "    # 创建因果掩码\n",
    "    mask = jnp.tril(jnp.ones((seq_len, seq_len)))[None, None, :, :]\n",
    "    mask = mask.astype(jnp.bool_)\n",
    "    \n",
    "    return x, mask\n",
    "\n",
    "def test_attention_basic(config):\n",
    "    \"\"\"基础注意力测试\"\"\"\n",
    "    print(f\"🧪 Testing attention mechanism...\")\n",
    "    \n",
    "    # 创建模型和测试数据\n",
    "    attention = ProgressiveMultiHeadAttention(config)\n",
    "    x, mask = create_test_data(config)\n",
    "    \n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Mask shape: {mask.shape}\")\n",
    "    \n",
    "    # 初始化参数\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    params = attention.init(key, x, mask)\n",
    "    \n",
    "    # 计算参数量\n",
    "    param_count = sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
    "    print(f\"  Attention params: {param_count:,} ({param_count/1e6:.2f}M)\")\n",
    "    \n",
    "    # 前向传播\n",
    "    start_time = time.time()\n",
    "    output = attention.apply(params, x, mask)\n",
    "    jax.block_until_ready(output)\n",
    "    forward_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Forward pass time: {forward_time*1000:.2f}ms\")\n",
    "    \n",
    "    # 验证输出\n",
    "    assert output.shape == x.shape, f\"Output shape mismatch: {output.shape} vs {x.shape}\"\n",
    "    assert not jnp.any(jnp.isnan(output)), \"Output contains NaN values\"\n",
    "    assert jnp.all(jnp.isfinite(output)), \"Output contains infinite values\"\n",
    "    \n",
    "    print(f\"  ✅ Attention mechanism test passed!\")\n",
    "    \n",
    "    return params, output\n",
    "\n",
    "# 运行基础测试\n",
    "params, output = test_attention_basic(test_config)\n",
    "print(f\"\\n📊 Basic attention test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c62a5a",
   "metadata": {},
   "source": [
    "## Section 6: Multi-GPU Parallelization Testing\n",
    "\n",
    "测试多GPU并行化策略和性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sharded_attention(config, mesh):\n",
    "    \"\"\"创建分片的注意力模型\"\"\"\n",
    "    if mesh is None:\n",
    "        print(\"⚠️ No mesh available, using single device\")\n",
    "        return None, None\n",
    "    \n",
    "    attention = ProgressiveMultiHeadAttention(config)\n",
    "    \n",
    "    # 定义分片规范\n",
    "    # 输入: (batch, seq_len, embd) -> 在batch维度上分片\n",
    "    input_sharding = NamedSharding(mesh, PartitionSpec('data', None, None))\n",
    "    \n",
    "    # 参数分片策略\n",
    "    # 注意力权重可以在head维度上分片\n",
    "    param_sharding = NamedSharding(mesh, PartitionSpec())\n",
    "    \n",
    "    return attention, input_sharding\n",
    "\n",
    "def benchmark_parallel_attention(config, mesh, num_runs=5):\n",
    "    \"\"\"基准测试并行注意力\"\"\"\n",
    "    print(f\"🏃 Benchmarking parallel attention...\")\n",
    "    \n",
    "    if mesh is None:\n",
    "        print(\"⚠️ No mesh available, skipping parallel benchmark\")\n",
    "        return None\n",
    "    \n",
    "    # 创建分片模型\n",
    "    attention, input_sharding = create_sharded_attention(config, mesh)\n",
    "    if attention is None:\n",
    "        return None\n",
    "    \n",
    "    with mesh:\n",
    "        # 创建更大的测试数据来体现并行化优势\n",
    "        batch_size = len(jax.devices()) * 4  # 每个设备处理4个batch\n",
    "        seq_len = 128\n",
    "        \n",
    "        key = jax.random.PRNGKey(42)\n",
    "        x = jax.random.normal(key, (batch_size, seq_len, config.n_embd))\n",
    "        mask = jnp.tril(jnp.ones((seq_len, seq_len)))[None, None, :, :]\n",
    "        mask = mask.astype(jnp.bool_)\n",
    "        \n",
    "        print(f\"  Parallel test data shape: {x.shape}\")\n",
    "        print(f\"  Total elements: {x.size:,}\")\n",
    "        \n",
    "        # 分片输入数据\n",
    "        x_sharded = jax.device_put(x, input_sharding)\n",
    "        \n",
    "        # 初始化参数\n",
    "        params = attention.init(key, x, mask)\n",
    "        \n",
    "        # JIT编译的并行前向传播\n",
    "        @jax.jit\n",
    "        def parallel_forward(params, x, mask):\n",
    "            return attention.apply(params, x, mask)\n",
    "        \n",
    "        # 预热\n",
    "        print(f\"  Warming up JIT compilation...\")\n",
    "        for _ in range(3):\n",
    "            output = parallel_forward(params, x_sharded, mask)\n",
    "            jax.block_until_ready(output)\n",
    "        \n",
    "        # 基准测试\n",
    "        print(f\"  Running {num_runs} benchmark iterations...\")\n",
    "        times = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            output = parallel_forward(params, x_sharded, mask)\n",
    "            jax.block_until_ready(output)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            print(f\"    Run {i+1}: {(end_time - start_time)*1000:.2f}ms\")\n",
    "        \n",
    "        results = {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times),\n",
    "            'throughput': (batch_size * seq_len) / np.mean(times),  # tokens/second\n",
    "            'batch_size': batch_size,\n",
    "            'seq_len': seq_len,\n",
    "            'num_devices': len(jax.devices())\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📊 Parallel Attention Benchmark Results:\")\n",
    "        print(f\"  Mean time: {results['mean_time']*1000:.2f}ms ± {results['std_time']*1000:.2f}ms\")\n",
    "        print(f\"  Throughput: {results['throughput']:.1f} tokens/s\")\n",
    "        print(f\"  Batch size: {results['batch_size']}\")\n",
    "        print(f\"  Devices used: {results['num_devices']}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# 运行并行基准测试\n",
    "if mesh is not None:\n",
    "    parallel_results = benchmark_parallel_attention(test_config, mesh)\n",
    "else:\n",
    "    print(\"⚠️ Skipping parallel benchmark due to no mesh\")\n",
    "    parallel_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfa584",
   "metadata": {},
   "source": [
    "## Section 7: Memory Usage Analysis\n",
    "\n",
    "分析内存使用模式和优化策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage(config):\n",
    "    \"\"\"分析内存使用情况\"\"\"\n",
    "    print(f\"💾 Memory Usage Analysis\")\n",
    "    \n",
    "    # 计算理论内存需求\n",
    "    param_count = config.get_param_count()\n",
    "    param_memory_gb = param_count * 4 / (1024**3)  # float32\n",
    "    \n",
    "    print(f\"\\nModel Memory Requirements:\")\n",
    "    print(f\"  Parameters: {param_count:,} ({param_count/1e6:.1f}M)\")\n",
    "    print(f\"  Parameter memory (FP32): {param_memory_gb:.2f} GB\")\n",
    "    print(f\"  Parameter memory (FP16): {param_memory_gb/2:.2f} GB\")\n",
    "    \n",
    "    # 计算激活内存 (batch_size=1, seq_len=1024)\n",
    "    batch_size = 1\n",
    "    seq_len = 1024\n",
    "    \n",
    "    # 注意力机制的内存需求\n",
    "    attention_matrix_size = batch_size * config.n_head * seq_len * seq_len * 4  # bytes\n",
    "    qkv_size = batch_size * seq_len * config.n_embd * 3 * 4  # bytes\n",
    "    output_size = batch_size * seq_len * config.n_embd * 4  # bytes\n",
    "    \n",
    "    activation_memory_gb = (attention_matrix_size + qkv_size + output_size) / (1024**3)\n",
    "    \n",
    "    print(f\"\\nActivation Memory (batch=1, seq_len={seq_len}):\")\n",
    "    print(f\"  Attention matrices: {attention_matrix_size/(1024**2):.1f} MB\")\n",
    "    print(f\"  QKV tensors: {qkv_size/(1024**2):.1f} MB\")\n",
    "    print(f\"  Output tensors: {output_size/(1024**2):.1f} MB\")\n",
    "    print(f\"  Total activation memory: {activation_memory_gb*1000:.1f} MB\")\n",
    "    \n",
    "    # GPU内存容量 (RTX 3090 = 24GB)\n",
    "    gpu_memory_gb = 24\n",
    "    total_memory_gb = param_memory_gb + activation_memory_gb\n",
    "    \n",
    "    print(f\"\\nGPU Memory Analysis (per RTX 3090):\")\n",
    "    print(f\"  Available memory: {gpu_memory_gb} GB\")\n",
    "    print(f\"  Required memory: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"  Memory utilization: {(total_memory_gb/gpu_memory_gb)*100:.1f}%\")\n",
    "    \n",
    "    if total_memory_gb > gpu_memory_gb:\n",
    "        print(f\"  ⚠️ Model too large for single GPU!\")\n",
    "        print(f\"  💡 Recommendation: Use model parallelism or reduce precision\")\n",
    "    else:\n",
    "        print(f\"  ✅ Model fits in single GPU memory\")\n",
    "    \n",
    "    # 多GPU内存分析\n",
    "    num_gpus = len(jax.devices())\n",
    "    if num_gpus > 1:\n",
    "        print(f\"\\nMulti-GPU Memory Analysis ({num_gpus} GPUs):\")\n",
    "        print(f\"  Total GPU memory: {gpu_memory_gb * num_gpus} GB\")\n",
    "        print(f\"  Memory per GPU (data parallel): {total_memory_gb:.2f} GB\")\n",
    "        print(f\"  Memory per GPU (model parallel): {param_memory_gb/num_gpus + activation_memory_gb:.2f} GB\")\n",
    "        \n",
    "        if param_memory_gb/num_gpus + activation_memory_gb < gpu_memory_gb:\n",
    "            print(f\"  ✅ Model parallelism viable\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Need more aggressive parallelization\")\n",
    "    \n",
    "    return {\n",
    "        'param_memory_gb': param_memory_gb,\n",
    "        'activation_memory_gb': activation_memory_gb,\n",
    "        'total_memory_gb': total_memory_gb,\n",
    "        'gpu_memory_gb': gpu_memory_gb,\n",
    "        'num_gpus': num_gpus,\n",
    "        'memory_utilization': (total_memory_gb/gpu_memory_gb)*100\n",
    "    }\n",
    "\n",
    "# 运行内存分析\n",
    "memory_analysis = analyze_memory_usage(test_config)\n",
    "\n",
    "# 为大模型运行分析\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Large Model Analysis (1.5B parameters)\")\n",
    "large_memory_analysis = analyze_memory_usage(configs['xlarge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16d0f8",
   "metadata": {},
   "source": [
    "## Section 8: Performance Optimization Recommendations\n",
    "\n",
    "基于分析结果提供性能优化建议。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimization_recommendations(memory_analysis, parallel_results, num_gpus):\n",
    "    \"\"\"生成优化建议\"\"\"\n",
    "    print(f\"🎯 Performance Optimization Recommendations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. 内存优化建议\n",
    "    print(f\"\\n1. Memory Optimization:\")\n",
    "    if memory_analysis['memory_utilization'] > 80:\n",
    "        print(f\"   ⚠️ High memory utilization ({memory_analysis['memory_utilization']:.1f}%)\")\n",
    "        recommendations.extend([\n",
    "            \"Use mixed precision (FP16) to reduce memory by 50%\",\n",
    "            \"Implement gradient checkpointing for training\",\n",
    "            \"Use model parallelism to distribute parameters across GPUs\"\n",
    "        ])\n",
    "    else:\n",
    "        print(f\"   ✅ Memory utilization OK ({memory_analysis['memory_utilization']:.1f}%)\")\n",
    "        recommendations.append(\"Consider increasing batch size for better throughput\")\n",
    "    \n",
    "    for rec in recommendations[-3:]:\n",
    "        print(f\"     • {rec}\")\n",
    "    \n",
    "    # 2. 并行化策略建议\n",
    "    print(f\"\\n2. Parallelization Strategy:\")\n",
    "    if num_gpus > 1:\n",
    "        print(f\"   Available GPUs: {num_gpus}\")\n",
    "        if memory_analysis['total_memory_gb'] < memory_analysis['gpu_memory_gb']:\n",
    "            print(f\"   💡 Recommended: Data Parallelism\")\n",
    "            recommendations.extend([\n",
    "                \"Use data parallelism for better scaling\",\n",
    "                \"Implement pmap for efficient batch processing\",\n",
    "                \"Use gradient synchronization across devices\"\n",
    "            ])\n",
    "        else:\n",
    "            print(f\"   💡 Recommended: Model Parallelism\")\n",
    "            recommendations.extend([\n",
    "                \"Shard attention heads across GPUs\",\n",
    "                \"Partition MLP layers across devices\",\n",
    "                \"Use pipeline parallelism for large models\"\n",
    "            ])\n",
    "    else:\n",
    "        print(f\"   Single GPU setup - focus on memory optimization\")\n",
    "        recommendations.append(\"Consider upgrading to multi-GPU setup for large models\")\n",
    "    \n",
    "    for rec in recommendations[-3:]:\n",
    "        print(f\"     • {rec}\")\n",
    "    \n",
    "    # 3. 计算优化建议\n",
    "    print(f\"\\n3. Computation Optimization:\")\n",
    "    compute_recommendations = [\n",
    "        \"Use JAX JIT compilation for all forward passes\",\n",
    "        \"Implement fused attention kernels for better performance\",\n",
    "        \"Use XLA optimizations for tensor operations\",\n",
    "        \"Consider using Triton kernels for custom operations\"\n",
    "    ]\n",
    "    \n",
    "    if parallel_results and parallel_results['throughput'] < 100:\n",
    "        print(f\"   ⚠️ Low throughput ({parallel_results['throughput']:.1f} tokens/s)\")\n",
    "        compute_recommendations.extend([\n",
    "            \"Optimize attention computation with flash attention\",\n",
    "            \"Use larger batch sizes to amortize overhead\",\n",
    "            \"Profile and optimize bottleneck operations\"\n",
    "        ])\n",
    "    \n",
    "    recommendations.extend(compute_recommendations)\n",
    "    for rec in compute_recommendations:\n",
    "        print(f\"     • {rec}\")\n",
    "    \n",
    "    # 4. 系统级优化建议\n",
    "    print(f\"\\n4. System-Level Optimization:\")\n",
    "    system_recommendations = [\n",
    "        \"Pin CUDA contexts to avoid memory fragmentation\",\n",
    "        \"Use NVIDIA's CUDA Multi-Process Service (MPS)\",\n",
    "        \"Optimize data loading pipeline to prevent GPU starvation\",\n",
    "        \"Monitor GPU utilization and memory bandwidth\"\n",
    "    ]\n",
    "    \n",
    "    recommendations.extend(system_recommendations)\n",
    "    for rec in system_recommendations:\n",
    "        print(f\"     • {rec}\")\n",
    "    \n",
    "    # 5. 具体实现建议\n",
    "    print(f\"\\n5. Implementation Priorities:\")\n",
    "    priorities = [\n",
    "        \"HIGH: Fix mesh creation issues for multi-GPU support\",\n",
    "        \"HIGH: Implement proper sharding for attention layers\",\n",
    "        \"MEDIUM: Add mixed precision support (FP16/BF16)\",\n",
    "        \"MEDIUM: Optimize memory allocation patterns\",\n",
    "        \"LOW: Fine-tune XLA compilation flags\"\n",
    "    ]\n",
    "    \n",
    "    for priority in priorities:\n",
    "        print(f\"     • {priority}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# 生成优化建议\n",
    "all_recommendations = generate_optimization_recommendations(\n",
    "    memory_analysis, parallel_results, len(jax.devices())\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 Total recommendations generated: {len(all_recommendations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea76f0a",
   "metadata": {},
   "source": [
    "## Section 9: Next Steps and Action Items\n",
    "\n",
    "基于分析结果的具体行动计划。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_plan():\n",
    "    \"\"\"创建具体的行动计划\"\"\"\n",
    "    print(f\"📋 Action Plan for Multi-GPU Optimization\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    action_items = {\n",
    "        \"Immediate (Next 1-2 days)\": [\n",
    "            {\n",
    "                \"task\": \"Fix mesh creation issues\",\n",
    "                \"description\": \"Debug and resolve the mesh creation problems preventing multi-GPU utilization\",\n",
    "                \"files\": [\"src/utils/gpu_utils.py\", \"run_benchmark.py\"],\n",
    "                \"priority\": \"HIGH\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Implement proper sharding strategies\",\n",
    "                \"description\": \"Add PartitionSpec definitions for different model components\", \n",
    "                \"files\": [\"src/models/gpt_model.py\"],\n",
    "                \"priority\": \"HIGH\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Add comprehensive error handling\",\n",
    "                \"description\": \"Improve error messages and recovery for GPU/mesh failures\",\n",
    "                \"files\": [\"progressive_benchmark.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"Short-term (Next week)\": [\n",
    "            {\n",
    "                \"task\": \"Implement mixed precision support\",\n",
    "                \"description\": \"Add FP16/BF16 support to reduce memory usage by 50%\",\n",
    "                \"files\": [\"src/models/gpt_model.py\", \"configs/benchmark_config.json\"],\n",
    "                \"priority\": \"HIGH\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Optimize attention computation\",\n",
    "                \"description\": \"Implement flash attention or other efficient attention variants\",\n",
    "                \"files\": [\"src/models/attention.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Add performance monitoring\",\n",
    "                \"description\": \"Implement GPU utilization and memory monitoring\",\n",
    "                \"files\": [\"src/utils/monitoring.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"Medium-term (Next 2 weeks)\": [\n",
    "            {\n",
    "                \"task\": \"Pipeline parallelism\",\n",
    "                \"description\": \"Implement pipeline parallelism for very large models\",\n",
    "                \"files\": [\"src/models/pipeline_gpt.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Advanced sharding strategies\", \n",
    "                \"description\": \"Implement tensor parallelism for MLP and attention layers\",\n",
    "                \"files\": [\"src/models/sharded_layers.py\"],\n",
    "                \"priority\": \"LOW\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Benchmarking suite\",\n",
    "                \"description\": \"Create comprehensive benchmarking and profiling tools\",\n",
    "                \"files\": [\"benchmarks/comprehensive_benchmark.py\"],\n",
    "                \"priority\": \"LOW\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for timeframe, items in action_items.items():\n",
    "        print(f\"\\n{timeframe}:\")\n",
    "        for i, item in enumerate(items, 1):\n",
    "            print(f\"  {i}. {item['task']} [{item['priority']}]\")\n",
    "            print(f\"     {item['description']}\")\n",
    "            print(f\"     Files: {', '.join(item['files'])}\")\n",
    "    \n",
    "    return action_items\n",
    "\n",
    "def save_analysis_results():\n",
    "    \"\"\"保存分析结果到文件\"\"\"\n",
    "    results = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"gpu_count\": len(jax.devices()),\n",
    "        \"jax_version\": jax.__version__,\n",
    "        \"memory_analysis\": memory_analysis,\n",
    "        \"parallel_results\": parallel_results,\n",
    "        \"mesh_created\": mesh is not None,\n",
    "        \"recommendations\": all_recommendations\n",
    "    }\n",
    "    \n",
    "    results_file = Path(\"multi_gpu_analysis_results.json\")\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n💾 Analysis results saved to: {results_file}\")\n",
    "    return results_file\n",
    "\n",
    "# 创建行动计划\n",
    "action_plan = create_action_plan()\n",
    "\n",
    "# 保存分析结果\n",
    "results_file = save_analysis_results()\n",
    "\n",
    "print(f\"\\n🎯 Multi-GPU Analysis Complete!\")\n",
    "print(f\"📊 Key findings:\")\n",
    "print(f\"  • GPU count: {len(jax.devices())}\")\n",
    "print(f\"  • Mesh creation: {'✅ Success' if mesh else '❌ Failed'}\")\n",
    "print(f\"  • Memory utilization: {memory_analysis['memory_utilization']:.1f}%\")\n",
    "if parallel_results:\n",
    "    print(f\"  • Parallel throughput: {parallel_results['throughput']:.1f} tokens/s\")\n",
    "print(f\"  • Total recommendations: {len(all_recommendations)}\")\n",
    "print(f\"\\n📋 Next: Review {results_file} and implement high-priority action items\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
