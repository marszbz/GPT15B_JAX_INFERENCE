{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8cc89c",
   "metadata": {},
   "source": [
    "# Multi-GPU JAX Parallelization for GPT-1.5B Inference\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬ä¸“é—¨ç”¨äºŽåˆ†æžå’Œä¼˜åŒ–GPT-1.5B JAXæŽ¨ç†ç³»ç»Ÿçš„å¤šGPUå¹¶è¡ŒåŒ–ç­–ç•¥ã€‚\n",
    "\n",
    "## ç›®æ ‡\n",
    "1. åˆ†æžå½“å‰çš„å¤šå¤´æ³¨æ„åŠ›å®žçŽ°\n",
    "2. å®žçŽ°JAX meshå’Œåˆ†ç‰‡ç­–ç•¥\n",
    "3. ä¼˜åŒ–å¤šGPUå†…å­˜ä½¿ç”¨\n",
    "4. æé«˜æŽ¨ç†åžåé‡\n",
    "5. è§£å†³meshåˆ›å»ºé—®é¢˜\n",
    "\n",
    "## çŽ¯å¢ƒä¿¡æ¯\n",
    "- ç³»ç»Ÿ: Ubuntu 22.04\n",
    "- GPU: 4x RTX 3090\n",
    "- JAXç‰ˆæœ¬: 0.6.1 with CUDA support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9ac47",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "å¯¼å…¥JAXã€Flaxã€NumPyä»¥åŠå…¶ä»–å¿…è¦çš„åº“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "# è®¾ç½®JAXçŽ¯å¢ƒï¼ˆå¿…é¡»åœ¨å¯¼å…¥JAXä¹‹å‰ï¼‰\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'\n",
    "# æ¸…ç†å¯èƒ½å­˜åœ¨çš„XLA_FLAGS\n",
    "if 'XLA_FLAGS' in os.environ:\n",
    "    del os.environ['XLA_FLAGS']\n",
    "\n",
    "# å¯¼å…¥JAXç›¸å…³åŒ…\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, pmap, devices, device_count\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax.experimental import mesh_utils\n",
    "import flax.linen as nn\n",
    "from flax import jax_utils\n",
    "import numpy as np\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Available devices: {len(jax.devices())}\")\n",
    "for i, device in enumerate(jax.devices()):\n",
    "    print(f\"  Device {i}: {device}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰CUDAæ”¯æŒ\n",
    "if jax.devices()[0].platform == 'gpu':\n",
    "    print(\"âœ… CUDA support detected\")\n",
    "else:\n",
    "    print(\"âš ï¸ No CUDA support detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd187f8",
   "metadata": {},
   "source": [
    "## Section 2: Define Configuration and Model Classes\n",
    "\n",
    "å®šä¹‰GPTé…ç½®å’Œå¤šå¤´æ³¨æ„åŠ›ç±»çš„å®žçŽ°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62570c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProgressiveGPTConfig:\n",
    "    \"\"\"æ¸è¿›å¼GPTé…ç½®\"\"\"\n",
    "    vocab_size: int = 50257\n",
    "    n_positions: int = 1024\n",
    "    n_embd: int = 768\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    dropout: float = 0.1\n",
    "    use_bias: bool = True\n",
    "    \n",
    "    def get_param_count(self) -> int:\n",
    "        \"\"\"ä¼°ç®—å‚æ•°é‡\"\"\"\n",
    "        # åµŒå…¥å±‚: vocab_size * n_embd + n_positions * n_embd\n",
    "        embed_params = self.vocab_size * self.n_embd + self.n_positions * self.n_embd\n",
    "        \n",
    "        # æ¯ä¸ªtransformerå±‚çš„å‚æ•°\n",
    "        # æ³¨æ„åŠ›: 4 * n_embd^2 (qkv + output projection)\n",
    "        # MLP: 2 * n_embd * (4 * n_embd) = 8 * n_embd^2\n",
    "        # LayerNorm: 2 * n_embd * 2 = 4 * n_embd\n",
    "        layer_params = (4 * self.n_embd * self.n_embd + \n",
    "                       8 * self.n_embd * self.n_embd + \n",
    "                       4 * self.n_embd)\n",
    "        \n",
    "        # æ€»å‚æ•° = åµŒå…¥ + å±‚æ•° * æ¯å±‚å‚æ•° + æœ€ç»ˆLMå¤´\n",
    "        total_params = embed_params + self.n_layer * layer_params + self.vocab_size * self.n_embd\n",
    "        return total_params\n",
    "\n",
    "\n",
    "class ProgressiveMultiHeadAttention(nn.Module):\n",
    "    \"\"\"æ¸è¿›å¼å¤šå¤´æ³¨æ„åŠ›\"\"\"\n",
    "    config: ProgressiveGPTConfig\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # QKVæŠ•å½±\n",
    "        qkv = nn.Dense(3 * self.config.n_embd, use_bias=self.config.use_bias)(x)\n",
    "        qkv = qkv.reshape(B, T, 3, self.config.n_head, C // self.config.n_head)\n",
    "        qkv = qkv.transpose(2, 0, 3, 1, 4)  # (3, B, nh, T, hs)\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # æ¯ä¸ªçš„å½¢çŠ¶: (B, nh, T, hs)\n",
    "        \n",
    "        # æ³¨æ„åŠ›æƒé‡ - æ­£ç¡®çš„è½¬ç½®ç»´åº¦\n",
    "        # k.shape = (B, nh, T, hs), æˆ‘ä»¬æƒ³è½¬ç½®æœ€åŽä¸¤ä¸ªç»´åº¦ T å’Œ hs\n",
    "        att = (q @ k.transpose(0, 1, 3, 2)) * (1.0 / jnp.sqrt(k.shape[-1]))\n",
    "        \n",
    "        # åº”ç”¨å› æžœæŽ©ç \n",
    "        if mask is not None:\n",
    "            att = jnp.where(mask, att, -jnp.inf)\n",
    "        \n",
    "        att = jax.nn.softmax(att, axis=-1)\n",
    "        \n",
    "        # åº”ç”¨æ³¨æ„åŠ›åˆ°å€¼\n",
    "        y = att @ v  # (B, nh, T, hs)\n",
    "        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)\n",
    "        \n",
    "        # è¾“å‡ºæŠ•å½±\n",
    "        return nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)(y)\n",
    "\n",
    "print(\"âœ… Model classes defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ac036",
   "metadata": {},
   "source": [
    "## Section 3: Multi-GPU Mesh Creation and Analysis\n",
    "\n",
    "åˆ›å»ºå¤šGPU meshå¹¶åˆ†æžå½“å‰çš„å¹¶è¡ŒåŒ–ç­–ç•¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gpu_setup():\n",
    "    \"\"\"åˆ†æžå½“å‰GPUè®¾ç½®\"\"\"\n",
    "    devices = jax.devices()\n",
    "    print(f\"GPU Analysis:\")\n",
    "    print(f\"  Total devices: {len(devices)}\")\n",
    "    print(f\"  Device platform: {devices[0].platform}\")\n",
    "    \n",
    "    # èŽ·å–è®¾å¤‡å†…å­˜ä¿¡æ¯\n",
    "    for i, device in enumerate(devices):\n",
    "        print(f\"  Device {i}: {device}\")\n",
    "        print(f\"    Platform: {device.platform}\")\n",
    "        print(f\"    Device kind: {device.device_kind}\")\n",
    "    \n",
    "    return devices\n",
    "\n",
    "def create_device_mesh(devices):\n",
    "    \"\"\"åˆ›å»ºè®¾å¤‡meshç”¨äºŽå¹¶è¡ŒåŒ–\"\"\"\n",
    "    if len(devices) == 1:\n",
    "        print(\"âš ï¸ Only 1 device available, no parallelization possible\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # å°è¯•åˆ›å»º2D mesh: (data_parallel, model_parallel)\n",
    "        if len(devices) == 4:\n",
    "            # 4ä¸ªGPU: 2x2 meshæˆ–è€…4x1 mesh\n",
    "            mesh_shape = (2, 2)\n",
    "            mesh_devices = mesh_utils.create_device_mesh(mesh_shape)\n",
    "            mesh = Mesh(mesh_devices, axis_names=('data', 'model'))\n",
    "            print(f\"âœ… Created 2x2 mesh with axes ('data', 'model')\")\n",
    "        elif len(devices) == 2:\n",
    "            mesh_shape = (2, 1)\n",
    "            mesh_devices = mesh_utils.create_device_mesh(mesh_shape)\n",
    "            mesh = Mesh(mesh_devices, axis_names=('data', 'model'))\n",
    "            print(f\"âœ… Created 2x1 mesh with axes ('data', 'model')\")\n",
    "        else:\n",
    "            # å¯¹äºŽå…¶ä»–æƒ…å†µï¼Œä½¿ç”¨1D mesh\n",
    "            mesh_devices = np.array(devices).reshape(-1, 1)\n",
    "            mesh = Mesh(mesh_devices, axis_names=('data',))\n",
    "            print(f\"âœ… Created 1D mesh with {len(devices)} devices\")\n",
    "        \n",
    "        print(f\"  Mesh shape: {mesh.shape}\")\n",
    "        print(f\"  Mesh axis names: {mesh.axis_names}\")\n",
    "        return mesh\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create mesh: {e}\")\n",
    "        return None\n",
    "\n",
    "# åˆ†æžGPUè®¾ç½®\n",
    "devices = analyze_gpu_setup()\n",
    "mesh = create_device_mesh(devices)\n",
    "\n",
    "if mesh:\n",
    "    print(f\"\\nðŸŽ¯ Mesh created successfully!\")\n",
    "    print(f\"  Can use data parallelism: {'data' in mesh.axis_names}\")\n",
    "    print(f\"  Can use model parallelism: {'model' in mesh.axis_names}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No mesh created, will use single device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652a79c",
   "metadata": {},
   "source": [
    "## Section 4: Initialize Model Configuration for Testing\n",
    "\n",
    "åˆ›å»ºç”¨äºŽæµ‹è¯•çš„æ¨¡åž‹é…ç½®å®žä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸åŒè§„æ¨¡çš„æµ‹è¯•é…ç½®\n",
    "configs = {\n",
    "    'small': ProgressiveGPTConfig(\n",
    "        n_embd=512, n_layer=8, n_head=8, n_positions=512\n",
    "    ),\n",
    "    'medium': ProgressiveGPTConfig(\n",
    "        n_embd=768, n_layer=12, n_head=12, n_positions=1024\n",
    "    ),\n",
    "    'large': ProgressiveGPTConfig(\n",
    "        n_embd=1024, n_layer=24, n_head=16, n_positions=1024\n",
    "    ),\n",
    "    'xlarge': ProgressiveGPTConfig(\n",
    "        n_embd=1280, n_layer=36, n_head=20, n_positions=1024\n",
    "    )\n",
    "}\n",
    "\n",
    "# æ‰“å°é…ç½®ä¿¡æ¯\n",
    "for name, config in configs.items():\n",
    "    param_count = config.get_param_count()\n",
    "    print(f\"{name.upper()} Config:\")\n",
    "    print(f\"  Embedding dim: {config.n_embd}\")\n",
    "    print(f\"  Layers: {config.n_layer}\")\n",
    "    print(f\"  Heads: {config.n_head}\")\n",
    "    print(f\"  Max positions: {config.n_positions}\")\n",
    "    print(f\"  Estimated params: {param_count:,} ({param_count/1e6:.1f}M)\")\n",
    "    print(f\"  Estimated memory: {param_count * 4 / (1024**3):.2f} GB\")\n",
    "    print()\n",
    "\n",
    "# é€‰æ‹©æµ‹è¯•é…ç½®\n",
    "test_config = configs['medium']\n",
    "print(f\"ðŸŽ¯ Using MEDIUM config for testing\")\n",
    "print(f\"  Parameters: {test_config.get_param_count()/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f923d",
   "metadata": {},
   "source": [
    "## Section 5: Test Attention Mechanism with Dummy Data\n",
    "\n",
    "ä½¿ç”¨è™šæ‹Ÿæ•°æ®æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578eb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(config, batch_size=2, seq_len=32):\n",
    "    \"\"\"åˆ›å»ºæµ‹è¯•æ•°æ®\"\"\"\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # åˆ›å»ºè¾“å…¥æ•°æ® (batch_size, seq_len, n_embd)\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, config.n_embd))\n",
    "    \n",
    "    # åˆ›å»ºå› æžœæŽ©ç \n",
    "    mask = jnp.tril(jnp.ones((seq_len, seq_len)))[None, None, :, :]\n",
    "    mask = mask.astype(jnp.bool_)\n",
    "    \n",
    "    return x, mask\n",
    "\n",
    "def test_attention_basic(config):\n",
    "    \"\"\"åŸºç¡€æ³¨æ„åŠ›æµ‹è¯•\"\"\"\n",
    "    print(f\"ðŸ§ª Testing attention mechanism...\")\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡åž‹å’Œæµ‹è¯•æ•°æ®\n",
    "    attention = ProgressiveMultiHeadAttention(config)\n",
    "    x, mask = create_test_data(config)\n",
    "    \n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Mask shape: {mask.shape}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–å‚æ•°\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    params = attention.init(key, x, mask)\n",
    "    \n",
    "    # è®¡ç®—å‚æ•°é‡\n",
    "    param_count = sum(p.size for p in jax.tree_util.tree_leaves(params))\n",
    "    print(f\"  Attention params: {param_count:,} ({param_count/1e6:.2f}M)\")\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    start_time = time.time()\n",
    "    output = attention.apply(params, x, mask)\n",
    "    jax.block_until_ready(output)\n",
    "    forward_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Forward pass time: {forward_time*1000:.2f}ms\")\n",
    "    \n",
    "    # éªŒè¯è¾“å‡º\n",
    "    assert output.shape == x.shape, f\"Output shape mismatch: {output.shape} vs {x.shape}\"\n",
    "    assert not jnp.any(jnp.isnan(output)), \"Output contains NaN values\"\n",
    "    assert jnp.all(jnp.isfinite(output)), \"Output contains infinite values\"\n",
    "    \n",
    "    print(f\"  âœ… Attention mechanism test passed!\")\n",
    "    \n",
    "    return params, output\n",
    "\n",
    "# è¿è¡ŒåŸºç¡€æµ‹è¯•\n",
    "params, output = test_attention_basic(test_config)\n",
    "print(f\"\\nðŸ“Š Basic attention test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c62a5a",
   "metadata": {},
   "source": [
    "## Section 6: Multi-GPU Parallelization Testing\n",
    "\n",
    "æµ‹è¯•å¤šGPUå¹¶è¡ŒåŒ–ç­–ç•¥å’Œæ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sharded_attention(config, mesh):\n",
    "    \"\"\"åˆ›å»ºåˆ†ç‰‡çš„æ³¨æ„åŠ›æ¨¡åž‹\"\"\"\n",
    "    if mesh is None:\n",
    "        print(\"âš ï¸ No mesh available, using single device\")\n",
    "        return None, None\n",
    "    \n",
    "    attention = ProgressiveMultiHeadAttention(config)\n",
    "    \n",
    "    # å®šä¹‰åˆ†ç‰‡è§„èŒƒ\n",
    "    # è¾“å…¥: (batch, seq_len, embd) -> åœ¨batchç»´åº¦ä¸Šåˆ†ç‰‡\n",
    "    input_sharding = NamedSharding(mesh, PartitionSpec('data', None, None))\n",
    "    \n",
    "    # å‚æ•°åˆ†ç‰‡ç­–ç•¥\n",
    "    # æ³¨æ„åŠ›æƒé‡å¯ä»¥åœ¨headç»´åº¦ä¸Šåˆ†ç‰‡\n",
    "    param_sharding = NamedSharding(mesh, PartitionSpec())\n",
    "    \n",
    "    return attention, input_sharding\n",
    "\n",
    "def benchmark_parallel_attention(config, mesh, num_runs=5):\n",
    "    \"\"\"åŸºå‡†æµ‹è¯•å¹¶è¡Œæ³¨æ„åŠ›\"\"\"\n",
    "    print(f\"ðŸƒ Benchmarking parallel attention...\")\n",
    "    \n",
    "    if mesh is None:\n",
    "        print(\"âš ï¸ No mesh available, skipping parallel benchmark\")\n",
    "        return None\n",
    "    \n",
    "    # åˆ›å»ºåˆ†ç‰‡æ¨¡åž‹\n",
    "    attention, input_sharding = create_sharded_attention(config, mesh)\n",
    "    if attention is None:\n",
    "        return None\n",
    "    \n",
    "    with mesh:\n",
    "        # åˆ›å»ºæ›´å¤§çš„æµ‹è¯•æ•°æ®æ¥ä½“çŽ°å¹¶è¡ŒåŒ–ä¼˜åŠ¿\n",
    "        batch_size = len(jax.devices()) * 4  # æ¯ä¸ªè®¾å¤‡å¤„ç†4ä¸ªbatch\n",
    "        seq_len = 128\n",
    "        \n",
    "        key = jax.random.PRNGKey(42)\n",
    "        x = jax.random.normal(key, (batch_size, seq_len, config.n_embd))\n",
    "        mask = jnp.tril(jnp.ones((seq_len, seq_len)))[None, None, :, :]\n",
    "        mask = mask.astype(jnp.bool_)\n",
    "        \n",
    "        print(f\"  Parallel test data shape: {x.shape}\")\n",
    "        print(f\"  Total elements: {x.size:,}\")\n",
    "        \n",
    "        # åˆ†ç‰‡è¾“å…¥æ•°æ®\n",
    "        x_sharded = jax.device_put(x, input_sharding)\n",
    "        \n",
    "        # åˆå§‹åŒ–å‚æ•°\n",
    "        params = attention.init(key, x, mask)\n",
    "        \n",
    "        # JITç¼–è¯‘çš„å¹¶è¡Œå‰å‘ä¼ æ’­\n",
    "        @jax.jit\n",
    "        def parallel_forward(params, x, mask):\n",
    "            return attention.apply(params, x, mask)\n",
    "        \n",
    "        # é¢„çƒ­\n",
    "        print(f\"  Warming up JIT compilation...\")\n",
    "        for _ in range(3):\n",
    "            output = parallel_forward(params, x_sharded, mask)\n",
    "            jax.block_until_ready(output)\n",
    "        \n",
    "        # åŸºå‡†æµ‹è¯•\n",
    "        print(f\"  Running {num_runs} benchmark iterations...\")\n",
    "        times = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            output = parallel_forward(params, x_sharded, mask)\n",
    "            jax.block_until_ready(output)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            print(f\"    Run {i+1}: {(end_time - start_time)*1000:.2f}ms\")\n",
    "        \n",
    "        results = {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times),\n",
    "            'throughput': (batch_size * seq_len) / np.mean(times),  # tokens/second\n",
    "            'batch_size': batch_size,\n",
    "            'seq_len': seq_len,\n",
    "            'num_devices': len(jax.devices())\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Parallel Attention Benchmark Results:\")\n",
    "        print(f\"  Mean time: {results['mean_time']*1000:.2f}ms Â± {results['std_time']*1000:.2f}ms\")\n",
    "        print(f\"  Throughput: {results['throughput']:.1f} tokens/s\")\n",
    "        print(f\"  Batch size: {results['batch_size']}\")\n",
    "        print(f\"  Devices used: {results['num_devices']}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# è¿è¡Œå¹¶è¡ŒåŸºå‡†æµ‹è¯•\n",
    "if mesh is not None:\n",
    "    parallel_results = benchmark_parallel_attention(test_config, mesh)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping parallel benchmark due to no mesh\")\n",
    "    parallel_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfa584",
   "metadata": {},
   "source": [
    "## Section 7: Memory Usage Analysis\n",
    "\n",
    "åˆ†æžå†…å­˜ä½¿ç”¨æ¨¡å¼å’Œä¼˜åŒ–ç­–ç•¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage(config):\n",
    "    \"\"\"åˆ†æžå†…å­˜ä½¿ç”¨æƒ…å†µ\"\"\"\n",
    "    print(f\"ðŸ’¾ Memory Usage Analysis\")\n",
    "    \n",
    "    # è®¡ç®—ç†è®ºå†…å­˜éœ€æ±‚\n",
    "    param_count = config.get_param_count()\n",
    "    param_memory_gb = param_count * 4 / (1024**3)  # float32\n",
    "    \n",
    "    print(f\"\\nModel Memory Requirements:\")\n",
    "    print(f\"  Parameters: {param_count:,} ({param_count/1e6:.1f}M)\")\n",
    "    print(f\"  Parameter memory (FP32): {param_memory_gb:.2f} GB\")\n",
    "    print(f\"  Parameter memory (FP16): {param_memory_gb/2:.2f} GB\")\n",
    "    \n",
    "    # è®¡ç®—æ¿€æ´»å†…å­˜ (batch_size=1, seq_len=1024)\n",
    "    batch_size = 1\n",
    "    seq_len = 1024\n",
    "    \n",
    "    # æ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜éœ€æ±‚\n",
    "    attention_matrix_size = batch_size * config.n_head * seq_len * seq_len * 4  # bytes\n",
    "    qkv_size = batch_size * seq_len * config.n_embd * 3 * 4  # bytes\n",
    "    output_size = batch_size * seq_len * config.n_embd * 4  # bytes\n",
    "    \n",
    "    activation_memory_gb = (attention_matrix_size + qkv_size + output_size) / (1024**3)\n",
    "    \n",
    "    print(f\"\\nActivation Memory (batch=1, seq_len={seq_len}):\")\n",
    "    print(f\"  Attention matrices: {attention_matrix_size/(1024**2):.1f} MB\")\n",
    "    print(f\"  QKV tensors: {qkv_size/(1024**2):.1f} MB\")\n",
    "    print(f\"  Output tensors: {output_size/(1024**2):.1f} MB\")\n",
    "    print(f\"  Total activation memory: {activation_memory_gb*1000:.1f} MB\")\n",
    "    \n",
    "    # GPUå†…å­˜å®¹é‡ (RTX 3090 = 24GB)\n",
    "    gpu_memory_gb = 24\n",
    "    total_memory_gb = param_memory_gb + activation_memory_gb\n",
    "    \n",
    "    print(f\"\\nGPU Memory Analysis (per RTX 3090):\")\n",
    "    print(f\"  Available memory: {gpu_memory_gb} GB\")\n",
    "    print(f\"  Required memory: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"  Memory utilization: {(total_memory_gb/gpu_memory_gb)*100:.1f}%\")\n",
    "    \n",
    "    if total_memory_gb > gpu_memory_gb:\n",
    "        print(f\"  âš ï¸ Model too large for single GPU!\")\n",
    "        print(f\"  ðŸ’¡ Recommendation: Use model parallelism or reduce precision\")\n",
    "    else:\n",
    "        print(f\"  âœ… Model fits in single GPU memory\")\n",
    "    \n",
    "    # å¤šGPUå†…å­˜åˆ†æž\n",
    "    num_gpus = len(jax.devices())\n",
    "    if num_gpus > 1:\n",
    "        print(f\"\\nMulti-GPU Memory Analysis ({num_gpus} GPUs):\")\n",
    "        print(f\"  Total GPU memory: {gpu_memory_gb * num_gpus} GB\")\n",
    "        print(f\"  Memory per GPU (data parallel): {total_memory_gb:.2f} GB\")\n",
    "        print(f\"  Memory per GPU (model parallel): {param_memory_gb/num_gpus + activation_memory_gb:.2f} GB\")\n",
    "        \n",
    "        if param_memory_gb/num_gpus + activation_memory_gb < gpu_memory_gb:\n",
    "            print(f\"  âœ… Model parallelism viable\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Need more aggressive parallelization\")\n",
    "    \n",
    "    return {\n",
    "        'param_memory_gb': param_memory_gb,\n",
    "        'activation_memory_gb': activation_memory_gb,\n",
    "        'total_memory_gb': total_memory_gb,\n",
    "        'gpu_memory_gb': gpu_memory_gb,\n",
    "        'num_gpus': num_gpus,\n",
    "        'memory_utilization': (total_memory_gb/gpu_memory_gb)*100\n",
    "    }\n",
    "\n",
    "# è¿è¡Œå†…å­˜åˆ†æž\n",
    "memory_analysis = analyze_memory_usage(test_config)\n",
    "\n",
    "# ä¸ºå¤§æ¨¡åž‹è¿è¡Œåˆ†æž\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Large Model Analysis (1.5B parameters)\")\n",
    "large_memory_analysis = analyze_memory_usage(configs['xlarge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16d0f8",
   "metadata": {},
   "source": [
    "## Section 8: Performance Optimization Recommendations\n",
    "\n",
    "åŸºäºŽåˆ†æžç»“æžœæä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimization_recommendations(memory_analysis, parallel_results, num_gpus):\n",
    "    \"\"\"ç”Ÿæˆä¼˜åŒ–å»ºè®®\"\"\"\n",
    "    print(f\"ðŸŽ¯ Performance Optimization Recommendations\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. å†…å­˜ä¼˜åŒ–å»ºè®®\n",
    "    print(f\"\\n1. Memory Optimization:\")\n",
    "    if memory_analysis['memory_utilization'] > 80:\n",
    "        print(f\"   âš ï¸ High memory utilization ({memory_analysis['memory_utilization']:.1f}%)\")\n",
    "        recommendations.extend([\n",
    "            \"Use mixed precision (FP16) to reduce memory by 50%\",\n",
    "            \"Implement gradient checkpointing for training\",\n",
    "            \"Use model parallelism to distribute parameters across GPUs\"\n",
    "        ])\n",
    "    else:\n",
    "        print(f\"   âœ… Memory utilization OK ({memory_analysis['memory_utilization']:.1f}%)\")\n",
    "        recommendations.append(\"Consider increasing batch size for better throughput\")\n",
    "    \n",
    "    for rec in recommendations[-3:]:\n",
    "        print(f\"     â€¢ {rec}\")\n",
    "    \n",
    "    # 2. å¹¶è¡ŒåŒ–ç­–ç•¥å»ºè®®\n",
    "    print(f\"\\n2. Parallelization Strategy:\")\n",
    "    if num_gpus > 1:\n",
    "        print(f\"   Available GPUs: {num_gpus}\")\n",
    "        if memory_analysis['total_memory_gb'] < memory_analysis['gpu_memory_gb']:\n",
    "            print(f\"   ðŸ’¡ Recommended: Data Parallelism\")\n",
    "            recommendations.extend([\n",
    "                \"Use data parallelism for better scaling\",\n",
    "                \"Implement pmap for efficient batch processing\",\n",
    "                \"Use gradient synchronization across devices\"\n",
    "            ])\n",
    "        else:\n",
    "            print(f\"   ðŸ’¡ Recommended: Model Parallelism\")\n",
    "            recommendations.extend([\n",
    "                \"Shard attention heads across GPUs\",\n",
    "                \"Partition MLP layers across devices\",\n",
    "                \"Use pipeline parallelism for large models\"\n",
    "            ])\n",
    "    else:\n",
    "        print(f\"   Single GPU setup - focus on memory optimization\")\n",
    "        recommendations.append(\"Consider upgrading to multi-GPU setup for large models\")\n",
    "    \n",
    "    for rec in recommendations[-3:]:\n",
    "        print(f\"     â€¢ {rec}\")\n",
    "    \n",
    "    # 3. è®¡ç®—ä¼˜åŒ–å»ºè®®\n",
    "    print(f\"\\n3. Computation Optimization:\")\n",
    "    compute_recommendations = [\n",
    "        \"Use JAX JIT compilation for all forward passes\",\n",
    "        \"Implement fused attention kernels for better performance\",\n",
    "        \"Use XLA optimizations for tensor operations\",\n",
    "        \"Consider using Triton kernels for custom operations\"\n",
    "    ]\n",
    "    \n",
    "    if parallel_results and parallel_results['throughput'] < 100:\n",
    "        print(f\"   âš ï¸ Low throughput ({parallel_results['throughput']:.1f} tokens/s)\")\n",
    "        compute_recommendations.extend([\n",
    "            \"Optimize attention computation with flash attention\",\n",
    "            \"Use larger batch sizes to amortize overhead\",\n",
    "            \"Profile and optimize bottleneck operations\"\n",
    "        ])\n",
    "    \n",
    "    recommendations.extend(compute_recommendations)\n",
    "    for rec in compute_recommendations:\n",
    "        print(f\"     â€¢ {rec}\")\n",
    "    \n",
    "    # 4. ç³»ç»Ÿçº§ä¼˜åŒ–å»ºè®®\n",
    "    print(f\"\\n4. System-Level Optimization:\")\n",
    "    system_recommendations = [\n",
    "        \"Pin CUDA contexts to avoid memory fragmentation\",\n",
    "        \"Use NVIDIA's CUDA Multi-Process Service (MPS)\",\n",
    "        \"Optimize data loading pipeline to prevent GPU starvation\",\n",
    "        \"Monitor GPU utilization and memory bandwidth\"\n",
    "    ]\n",
    "    \n",
    "    recommendations.extend(system_recommendations)\n",
    "    for rec in system_recommendations:\n",
    "        print(f\"     â€¢ {rec}\")\n",
    "    \n",
    "    # 5. å…·ä½“å®žçŽ°å»ºè®®\n",
    "    print(f\"\\n5. Implementation Priorities:\")\n",
    "    priorities = [\n",
    "        \"HIGH: Fix mesh creation issues for multi-GPU support\",\n",
    "        \"HIGH: Implement proper sharding for attention layers\",\n",
    "        \"MEDIUM: Add mixed precision support (FP16/BF16)\",\n",
    "        \"MEDIUM: Optimize memory allocation patterns\",\n",
    "        \"LOW: Fine-tune XLA compilation flags\"\n",
    "    ]\n",
    "    \n",
    "    for priority in priorities:\n",
    "        print(f\"     â€¢ {priority}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# ç”Ÿæˆä¼˜åŒ–å»ºè®®\n",
    "all_recommendations = generate_optimization_recommendations(\n",
    "    memory_analysis, parallel_results, len(jax.devices())\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Total recommendations generated: {len(all_recommendations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea76f0a",
   "metadata": {},
   "source": [
    "## Section 9: Next Steps and Action Items\n",
    "\n",
    "åŸºäºŽåˆ†æžç»“æžœçš„å…·ä½“è¡ŒåŠ¨è®¡åˆ’ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_plan():\n",
    "    \"\"\"åˆ›å»ºå…·ä½“çš„è¡ŒåŠ¨è®¡åˆ’\"\"\"\n",
    "    print(f\"ðŸ“‹ Action Plan for Multi-GPU Optimization\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    action_items = {\n",
    "        \"Immediate (Next 1-2 days)\": [\n",
    "            {\n",
    "                \"task\": \"Fix mesh creation issues\",\n",
    "                \"description\": \"Debug and resolve the mesh creation problems preventing multi-GPU utilization\",\n",
    "                \"files\": [\"src/utils/gpu_utils.py\", \"run_benchmark.py\"],\n",
    "                \"priority\": \"HIGH\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Implement proper sharding strategies\",\n",
    "                \"description\": \"Add PartitionSpec definitions for different model components\", \n",
    "                \"files\": [\"src/models/gpt_model.py\"],\n",
    "                \"priority\": \"HIGH\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Add comprehensive error handling\",\n",
    "                \"description\": \"Improve error messages and recovery for GPU/mesh failures\",\n",
    "                \"files\": [\"progressive_benchmark.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"Short-term (Next week)\": [\n",
    "            {\n",
    "                \"task\": \"Implement mixed precision support\",\n",
    "                \"description\": \"Add FP16/BF16 support to reduce memory usage by 50%\",\n",
    "                \"files\": [\"src/models/gpt_model.py\", \"configs/benchmark_config.json\"],\n",
    "                \"priority\": \"HIGH\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Optimize attention computation\",\n",
    "                \"description\": \"Implement flash attention or other efficient attention variants\",\n",
    "                \"files\": [\"src/models/attention.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Add performance monitoring\",\n",
    "                \"description\": \"Implement GPU utilization and memory monitoring\",\n",
    "                \"files\": [\"src/utils/monitoring.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"Medium-term (Next 2 weeks)\": [\n",
    "            {\n",
    "                \"task\": \"Pipeline parallelism\",\n",
    "                \"description\": \"Implement pipeline parallelism for very large models\",\n",
    "                \"files\": [\"src/models/pipeline_gpt.py\"],\n",
    "                \"priority\": \"MEDIUM\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Advanced sharding strategies\", \n",
    "                \"description\": \"Implement tensor parallelism for MLP and attention layers\",\n",
    "                \"files\": [\"src/models/sharded_layers.py\"],\n",
    "                \"priority\": \"LOW\"\n",
    "            },\n",
    "            {\n",
    "                \"task\": \"Benchmarking suite\",\n",
    "                \"description\": \"Create comprehensive benchmarking and profiling tools\",\n",
    "                \"files\": [\"benchmarks/comprehensive_benchmark.py\"],\n",
    "                \"priority\": \"LOW\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for timeframe, items in action_items.items():\n",
    "        print(f\"\\n{timeframe}:\")\n",
    "        for i, item in enumerate(items, 1):\n",
    "            print(f\"  {i}. {item['task']} [{item['priority']}]\")\n",
    "            print(f\"     {item['description']}\")\n",
    "            print(f\"     Files: {', '.join(item['files'])}\")\n",
    "    \n",
    "    return action_items\n",
    "\n",
    "def save_analysis_results():\n",
    "    \"\"\"ä¿å­˜åˆ†æžç»“æžœåˆ°æ–‡ä»¶\"\"\"\n",
    "    results = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"gpu_count\": len(jax.devices()),\n",
    "        \"jax_version\": jax.__version__,\n",
    "        \"memory_analysis\": memory_analysis,\n",
    "        \"parallel_results\": parallel_results,\n",
    "        \"mesh_created\": mesh is not None,\n",
    "        \"recommendations\": all_recommendations\n",
    "    }\n",
    "    \n",
    "    results_file = Path(\"multi_gpu_analysis_results.json\")\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Analysis results saved to: {results_file}\")\n",
    "    return results_file\n",
    "\n",
    "# åˆ›å»ºè¡ŒåŠ¨è®¡åˆ’\n",
    "action_plan = create_action_plan()\n",
    "\n",
    "# ä¿å­˜åˆ†æžç»“æžœ\n",
    "results_file = save_analysis_results()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Multi-GPU Analysis Complete!\")\n",
    "print(f\"ðŸ“Š Key findings:\")\n",
    "print(f\"  â€¢ GPU count: {len(jax.devices())}\")\n",
    "print(f\"  â€¢ Mesh creation: {'âœ… Success' if mesh else 'âŒ Failed'}\")\n",
    "print(f\"  â€¢ Memory utilization: {memory_analysis['memory_utilization']:.1f}%\")\n",
    "if parallel_results:\n",
    "    print(f\"  â€¢ Parallel throughput: {parallel_results['throughput']:.1f} tokens/s\")\n",
    "print(f\"  â€¢ Total recommendations: {len(all_recommendations)}\")\n",
    "print(f\"\\nðŸ“‹ Next: Review {results_file} and implement high-priority action items\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
