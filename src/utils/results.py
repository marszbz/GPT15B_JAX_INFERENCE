"""
ç»“æœä¿å­˜å’ŒæŠ¥å‘Šç”Ÿæˆå·¥å…·
"""

import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional


class ResultManager:
    """ç»“æœç®¡ç†å™¨ - å¤„ç†åŸºå‡†æµ‹è¯•ç»“æœçš„ä¿å­˜å’Œåˆ†æ"""
    
    def __init__(self, output_dir: str = 'results'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results_cache = {}
    
    def save_results(self, results: Dict, prefix: str = 'gpt15b_benchmark') -> tuple:
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†JSONç»“æœ
        json_file = self.output_dir / f"{prefix}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š
        report_file = self.output_dir / f"performance_report_{timestamp}.txt" 
        self._generate_report(results, report_file)
        
        # ç¼“å­˜ç»“æœ
        self.results_cache[timestamp] = results
        
        return json_file, report_file
    
    def _generate_report(self, results: Dict, report_file: Path):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            info = results.get('benchmark_info', {})
            f.write(f"æµ‹è¯•æ—¶é—´: {info.get('timestamp', 'N/A')}\n")
            f.write(f"æ€»æ‰§è¡Œæ—¶é—´: {info.get('total_execution_time', 0):.2f}ç§’\n")
            f.write(f"GPUæ•°é‡: {info.get('gpu_count', 0)}\n")
            f.write(f"JAXç‰ˆæœ¬: {info.get('jax_version', 'N/A')}\n")
            f.write(f"CUDAç‰ˆæœ¬: {info.get('cuda_version', 'N/A')}\n")
            f.write(f"å¹³å°: {info.get('platform', 'N/A')}\n\n")
            
            # æ¨¡å‹é…ç½®
            model_cfg = info.get('model_config', {})
            f.write("æ¨¡å‹é…ç½®:\n")
            for key, value in model_cfg.items():
                f.write(f"  {key}: {value}\n")
            f.write("\n")
            
            # æ€§èƒ½ç»“æœ
            self._write_performance_results(f, results.get('results', {}))
    
    def _write_performance_results(self, f, results_data: Dict):
        """å†™å…¥æ€§èƒ½ç»“æœ"""
        f.write("æ€§èƒ½æµ‹è¯•ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        
        all_throughputs = []
        all_latencies = []
        
        for config_id, result in results_data.items():
            f.write(f"\né…ç½® {config_id}:\n")
            f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {result.get('samples_tested', 0)}\n")
            f.write(f"  å¹³å‡æ¨ç†æ—¶é—´: {result.get('avg_inference_time', 0):.3f}Â±{result.get('std_inference_time', 0):.3f}s\n")
            f.write(f"  å¹³å‡ååé‡: {result.get('avg_throughput', 0):.1f} tokens/s\n")
            f.write(f"  ååé‡èŒƒå›´: {result.get('min_throughput', 0):.1f} - {result.get('max_throughput', 0):.1f} tokens/s\n")
            f.write(f"  å¹³å‡æ¯tokenå»¶è¿Ÿ: {result.get('avg_latency_per_token', 0):.4f}s\n")
            f.write(f"  æ€»ç”Ÿæˆtokenæ•°: {result.get('total_tokens_generated', 0)}\n")
            
            all_throughputs.append(result.get('avg_throughput', 0))
            all_latencies.append(result.get('avg_latency_per_token', 0))
        
        # æ€»ä½“ç»Ÿè®¡
        if all_throughputs:
            f.write(f"\næ€»ä½“æ€§èƒ½ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡ååé‡: {np.mean(all_throughputs):.1f} tokens/s\n")
            f.write(f"  æœ€é«˜ååé‡: {max(all_throughputs):.1f} tokens/s\n")
            f.write(f"  å¹³å‡å»¶è¿Ÿ: {np.mean(all_latencies):.4f}s/token\n")
            f.write(f"  æœ€ä½å»¶è¿Ÿ: {min(all_latencies):.4f}s/token\n")
    
    def load_results(self, timestamp: str) -> Optional[Dict]:
        """åŠ è½½æŒ‡å®šæ—¶é—´æˆ³çš„ç»“æœ"""
        if timestamp in self.results_cache:
            return self.results_cache[timestamp]
        
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        json_file = self.output_dir / f"gpt15b_benchmark_{timestamp}.json"
        if json_file.exists():
            with open(json_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
                self.results_cache[timestamp] = results
                return results
        
        return None
    
    def list_available_results(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç»“æœæ–‡ä»¶"""
        json_files = list(self.output_dir.glob("gpt15b_benchmark_*.json"))
        timestamps = []
        for file in json_files:
            # æå–æ—¶é—´æˆ³
            parts = file.stem.split('_')
            if len(parts) >= 3:
                timestamp = '_'.join(parts[-2:])
                timestamps.append(timestamp)
        return sorted(timestamps)
    
    def compare_results(self, timestamp1: str, timestamp2: str) -> Dict:
        """æ¯”è¾ƒä¸¤æ¬¡æµ‹è¯•ç»“æœ"""
        result1 = self.load_results(timestamp1)
        result2 = self.load_results(timestamp2)
        
        if not result1 or not result2:
            return {}
        
        comparison = {
            'timestamp1': timestamp1,
            'timestamp2': timestamp2,
            'improvements': {},
            'regressions': {}
        }
        
        # æ¯”è¾ƒå„é…ç½®çš„æ€§èƒ½
        for config_id in result1.get('results', {}):
            if config_id in result2.get('results', {}):
                throughput1 = result1['results'][config_id].get('avg_throughput', 0)
                throughput2 = result2['results'][config_id].get('avg_throughput', 0)
                
                improvement = (throughput2 - throughput1) / throughput1 * 100 if throughput1 > 0 else 0
                
                if improvement > 5:  # 5%ä»¥ä¸Šæ”¹è¿›
                    comparison['improvements'][config_id] = improvement
                elif improvement < -5:  # 5%ä»¥ä¸Šé€€åŒ–
                    comparison['regressions'][config_id] = improvement
        
        return comparison


def save_benchmark_results(results: Dict, output_dir: str = 'results') -> tuple:
    """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœï¼ˆå‘åå…¼å®¹çš„å‡½æ•°ï¼‰"""
    manager = ResultManager(output_dir)
    return manager.save_results(results)


def print_performance_summary(results: Dict):
    """æ‰“å°æ€§èƒ½æ‘˜è¦"""
    print(f"\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®æ•°: {len(results.get('results', {}))}")
    print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {results.get('benchmark_info', {}).get('total_execution_time', 0):.2f}ç§’")
    
    # æ˜¾ç¤ºæ€§èƒ½äº®ç‚¹
    results_data = results.get('results', {})
    if results_data:
        all_throughputs = [r.get('avg_throughput', 0) for r in results_data.values()]
        all_latencies = [r.get('avg_latency_per_token', 0) for r in results_data.values()]
        
        if all_throughputs:
            print(f"\nğŸ† æ€§èƒ½äº®ç‚¹:")
            print(f"   æœ€é«˜ååé‡: {max(all_throughputs):.1f} tokens/s")
            print(f"   æœ€ä½å»¶è¿Ÿ: {min(all_latencies):.4f}s/token")
            print(f"   å¹³å‡ååé‡: {np.mean(all_throughputs):.1f} tokens/s")
