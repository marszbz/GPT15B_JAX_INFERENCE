"""
ç»“æœä¿å­˜å’ŒæŠ¥å‘Šç”Ÿæˆå·¥å…·
"""

import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, Any


def save_benchmark_results(results: Dict, output_dir: str = 'results') -> tuple:
    """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†JSONç»“æœ
    json_file = output_path / f"gpt15b_benchmark_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š
    report_file = output_path / f"performance_report_{timestamp}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        # åŸºæœ¬ä¿¡æ¯
        info = results['benchmark_info']
        f.write(f"æµ‹è¯•æ—¶é—´: {info['timestamp']}\n")
        f.write(f"æ€»æ‰§è¡Œæ—¶é—´: {info['total_execution_time']:.2f}ç§’\n")
        f.write(f"GPUæ•°é‡: {info['gpu_count']}\n")
        f.write(f"JAXç‰ˆæœ¬: {info['jax_version']}\n")
        f.write(f"CUDAç‰ˆæœ¬: {info['cuda_version']}\n")
        f.write(f"å¹³å°: {info['platform']}\n\n")
        
        # æ¨¡å‹é…ç½®
        model_cfg = info['model_config']
        f.write("æ¨¡å‹é…ç½®:\n")
        f.write(f"  å±‚æ•°: {model_cfg['n_layer']}\n")
        f.write(f"  æ³¨æ„åŠ›å¤´æ•°: {model_cfg['n_head']}\n")
        f.write(f"  åµŒå…¥ç»´åº¦: {model_cfg['n_embd']}\n")
        f.write(f"  è¯æ±‡è¡¨å¤§å°: {model_cfg['vocab_size']}\n\n")
        
        # æ€§èƒ½ç»“æœ
        f.write("æ€§èƒ½æµ‹è¯•ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        
        all_throughputs = []
        all_latencies = []
        
        for config_id, result in results['results'].items():
            f.write(f"\né…ç½® {config_id}:\n")
            f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {result['samples_tested']}\n")
            f.write(f"  å¹³å‡æ¨ç†æ—¶é—´: {result['avg_inference_time']:.3f}Â±{result['std_inference_time']:.3f}s\n")
            f.write(f"  å¹³å‡ååé‡: {result['avg_throughput']:.1f} tokens/s\n")
            f.write(f"  ååé‡èŒƒå›´: {result['min_throughput']:.1f} - {result['max_throughput']:.1f} tokens/s\n")
            f.write(f"  å¹³å‡æ¯tokenå»¶è¿Ÿ: {result['avg_latency_per_token']:.4f}s\n")
            f.write(f"  æ€»ç”Ÿæˆtokenæ•°: {result['total_tokens_generated']}\n")
            
            all_throughputs.append(result['avg_throughput'])
            all_latencies.append(result['avg_latency_per_token'])
        
        # æ€»ä½“ç»Ÿè®¡
        if all_throughputs:
            f.write(f"\næ€»ä½“æ€§èƒ½ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡ååé‡: {np.mean(all_throughputs):.1f} tokens/s\n")
            f.write(f"  æœ€é«˜ååé‡: {max(all_throughputs):.1f} tokens/s\n")
            f.write(f"  å¹³å‡å»¶è¿Ÿ: {np.mean(all_latencies):.4f}s/token\n")
            f.write(f"  æœ€ä½å»¶è¿Ÿ: {min(all_latencies):.4f}s/token\n")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   è¯¦ç»†ç»“æœ: {json_file}")
    print(f"   æ€§èƒ½æŠ¥å‘Š: {report_file}")
    
    return json_file, report_file


def print_performance_summary(results: Dict):
    """æ‰“å°æ€§èƒ½æ‘˜è¦"""
    print(f"\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®æ•°: {len(results['results'])}")
    print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {results['benchmark_info']['total_execution_time']:.2f}ç§’")
    
    # æ˜¾ç¤ºæ€§èƒ½äº®ç‚¹
    if results['results']:
        all_throughputs = [r['avg_throughput'] for r in results['results'].values()]
        all_latencies = [r['avg_latency_per_token'] for r in results['results'].values()]
        
        print(f"\nğŸ† æ€§èƒ½äº®ç‚¹:")
        print(f"   æœ€é«˜ååé‡: {max(all_throughputs):.1f} tokens/s")
        print(f"   æœ€ä½å»¶è¿Ÿ: {min(all_latencies):.4f}s/token")
        print(f"   å¹³å‡ååé‡: {np.mean(all_throughputs):.1f} tokens/s")
