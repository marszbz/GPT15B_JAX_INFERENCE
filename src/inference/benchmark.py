"""
æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—
"""

import time
import jax
import jax.numpy as jnp
import numpy as np
from typing import Dict, List, Any
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.models.gpt_model import GraphPartitionedGPT
from src.data.dataset_loader import SimpleTokenizer


class InferenceBenchmark:
    """æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, model: GraphPartitionedGPT):
        self.model = model
        self.tokenizer = SimpleTokenizer()
        
    def benchmark_single_sample(self, sample: Dict) -> Dict:
        """æµ‹è¯•å•ä¸ªæ ·æœ¬çš„æ¨ç†æ€§èƒ½"""
        prompt = sample['prompt']
        generation_length = sample.get('generation_length', 32)
        sample_id = sample.get('id', 'unknown')
        
        # ç¼–ç è¾“å…¥
        prompt_tokens = self.tokenizer.encode(prompt, max_length=512)
        input_ids = jnp.array([prompt_tokens])  # æ·»åŠ batchç»´åº¦
        
        # é¢„çƒ­ï¼ˆç¡®ä¿JITç¼–è¯‘å®Œæˆï¼‰
        _ = self.model.generate_text(input_ids, max_new_tokens=8)
        
        # æ­£å¼æ¨ç†è®¡æ—¶
        start_time = time.time()
        output_ids = self.model.generate_text(input_ids, max_new_tokens=generation_length)
        
        # ç¡®ä¿è®¡ç®—å®Œæˆ
        jax.block_until_ready(output_ids)
        end_time = time.time()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        inference_time = end_time - start_time
        input_length = len([t for t in prompt_tokens if t != 0])  # å»é™¤padding
        total_tokens = output_ids.shape[1]
        generated_tokens = total_tokens - input_length
        
        throughput = generated_tokens / inference_time if inference_time > 0 else 0
        
        return {
            'sample_id': sample_id,
            'input_length': input_length,
            'generated_tokens': generated_tokens,
            'total_tokens': total_tokens,
            'inference_time': inference_time,
            'throughput_tokens_per_sec': throughput,
            'latency_per_token': inference_time / generated_tokens if generated_tokens > 0 else 0
        }
    
    def benchmark_config(self, dataset: List[Dict], config_id: str, max_samples: int = 10) -> Dict:
        """æµ‹è¯•ç‰¹å®šé…ç½®çš„æ€§èƒ½"""
        if not dataset:
            return {}
        
        print(f"\nğŸ§ª æµ‹è¯•é…ç½® {config_id}")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
        print(f"   æµ‹è¯•æ ·æœ¬æ•°: {min(max_samples, len(dataset))}")
        
        results = []
        test_samples = min(max_samples, len(dataset))
        
        for i in range(test_samples):
            sample = dataset[i]
            
            try:
                result = self.benchmark_single_sample(sample)
                results.append(result)
                
                # æ¯3ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 3 == 0:
                    recent_results = results[-3:]
                    avg_time = np.mean([r['inference_time'] for r in recent_results])
                    avg_throughput = np.mean([r['throughput_tokens_per_sec'] for r in recent_results])
                    print(f"   è¿›åº¦ {i+1}/{test_samples}: å¹³å‡å»¶è¿Ÿ {avg_time:.3f}s, "
                          f"å¹³å‡ååé‡ {avg_throughput:.1f} tokens/s")
                
            except Exception as e:
                print(f"âš ï¸ æ ·æœ¬ {i} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if not results:
            return {}
        
        inference_times = [r['inference_time'] for r in results]
        throughputs = [r['throughput_tokens_per_sec'] for r in results]
        latencies = [r['latency_per_token'] for r in results]
        
        summary = {
            'config_id': config_id,
            'samples_tested': len(results),
            'avg_inference_time': np.mean(inference_times),
            'std_inference_time': np.std(inference_times),
            'avg_throughput': np.mean(throughputs),
            'max_throughput': max(throughputs),
            'min_throughput': min(throughputs),
            'avg_latency_per_token': np.mean(latencies),
            'total_tokens_generated': sum(r['generated_tokens'] for r in results),
            'total_time': sum(inference_times),
            'detailed_results': results
        }
        
        print(f"âœ… é…ç½® {config_id} æµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {summary['avg_inference_time']:.3f}Â±{summary['std_inference_time']:.3f}s")
        print(f"   å¹³å‡ååé‡: {summary['avg_throughput']:.1f} tokens/s")
        print(f"   å¹³å‡æ¯tokenå»¶è¿Ÿ: {summary['avg_latency_per_token']:.4f}s")
        
        return summary
    
    def run_full_benchmark(self, datasets: Dict[str, List[Dict]], max_samples_per_config: int = 10) -> Dict:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print("\nğŸš€ å¼€å§‹GPT-1.5B JAXæ¨ç†æ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        print(f"ğŸ“Š æ•°æ®é›†é…ç½®æ•°: {len(datasets)}")
        print(f"ğŸ”§ GPUæ•°é‡: {len(jax.devices())}")
        print(f"ğŸ“¦ JAXç‰ˆæœ¬: {jax.__version__}")
        print(f"ğŸ—ï¸ æ¨¡å‹è§„æ¨¡: {self.model.config.n_layer}å±‚, {self.model.config.n_head}å¤´, {self.model.config.n_embd}ç»´")
        
        all_results = {}
        total_start_time = time.time()
        
        # é€ä¸ªæµ‹è¯•é…ç½®
        for config_id, dataset in datasets.items():
            config_result = self.benchmark_config(dataset, config_id, max_samples_per_config)
            if config_result:
                all_results[config_id] = config_result
        
        total_time = time.time() - total_start_time
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        benchmark_summary = {
            'benchmark_info': {
                'total_execution_time': total_time,
                'configs_tested': len(all_results),
                'gpu_count': len(jax.devices()),
                'jax_version': jax.__version__,
                'model_config': {
                    'n_layer': self.model.config.n_layer,
                    'n_head': self.model.config.n_head,
                    'n_embd': self.model.config.n_embd,
                    'vocab_size': self.model.config.vocab_size
                },
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'platform': 'Windows',
                'cuda_version': '11.8'
            },
            'results': all_results
        }
        
        return benchmark_summary
