"""
GPT-1.5Bæ¨¡å‹å®šä¹‰ - ä½¿ç”¨JAX/Flaxå®ç°
æ”¯æŒå›¾åˆ†å‰²å’Œå¤šGPUå¹¶è¡Œæ¨ç†
"""

import jax
import jax.numpy as jnp
import flax.linen as nn
from typing import Dict, Any, Optional
from dataclasses import dataclass
from functools import partial


@dataclass
class GPTConfig:
    """GPT-1.5Bæ¨¡å‹é…ç½®"""
    vocab_size: int = 50257
    n_positions: int = 2048
    n_embd: int = 1600
    n_layer: int = 48
    n_head: int = 25
    dropout: float = 0.1
    use_bias: bool = True


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, x, training: bool = False):
        B, T, C = x.shape
        head_dim = C // self.config.n_head
        
        # QKVæŠ•å½±
        qkv = nn.Dense(3 * C, use_bias=self.config.use_bias)(x)
        q, k, v = jnp.split(qkv, 3, axis=-1)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        q = q.reshape(B, T, self.config.n_head, head_dim).transpose(0, 2, 1, 3)
        k = k.reshape(B, T, self.config.n_head, head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(B, T, self.config.n_head, head_dim).transpose(0, 2, 1, 3)
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scale = 1.0 / jnp.sqrt(head_dim)
        attn_weights = jnp.matmul(q, k.transpose(0, 1, 3, 2)) * scale
        
        # å› æœæ©ç 
        mask = jnp.tril(jnp.ones((T, T)))
        attn_weights = jnp.where(mask, attn_weights, -1e10)
        attn_weights = jax.nn.softmax(attn_weights, axis=-1)
        
        # Dropout
        if training:
            attn_weights = nn.Dropout(rate=self.config.dropout)(
                attn_weights, deterministic=not training
            )
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        out = jnp.matmul(attn_weights, v)
        out = out.transpose(0, 2, 1, 3).reshape(B, T, C)
        
        # è¾“å‡ºæŠ•å½±
        out = nn.Dense(C, use_bias=self.config.use_bias)(out)
        if training:
            out = nn.Dropout(rate=self.config.dropout)(out, deterministic=not training)
        
        return out


class MLP(nn.Module):
    """å‰é¦ˆç¥ç»ç½‘ç»œ"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, x, training: bool = False):
        # ç¬¬ä¸€å±‚ï¼šå‡ç»´åˆ°4*n_embd
        x = nn.Dense(4 * self.config.n_embd, use_bias=self.config.use_bias)(x)
        x = jax.nn.gelu(x)
        
        # ç¬¬äºŒå±‚ï¼šé™ç»´å›n_embd
        x = nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)(x)
        
        if training:
            x = nn.Dropout(rate=self.config.dropout)(x, deterministic=not training)
        
        return x


class TransformerBlock(nn.Module):
    """Transformerè§£ç å™¨å—"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, x, training: bool = False):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_out = MultiHeadAttention(self.config)(x, training)
        x = nn.LayerNorm()(x + attn_out)
        
        # MLP + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        mlp_out = MLP(self.config)(x, training)
        x = nn.LayerNorm()(x + mlp_out)
        
        return x


class GPTModel(nn.Module):
    """GPT-1.5Bä¸»æ¨¡å‹"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, input_ids, training: bool = False):
        B, T = input_ids.shape
        
        # TokenåµŒå…¥
        token_embed = nn.Embed(self.config.vocab_size, self.config.n_embd)(input_ids)
        
        # ä½ç½®åµŒå…¥
        pos_embed = nn.Embed(self.config.n_positions, self.config.n_embd)(
            jnp.arange(T)[None, :]
        )
        
        # ç»„åˆåµŒå…¥
        x = token_embed + pos_embed
        
        if training:
            x = nn.Dropout(rate=self.config.dropout)(x, deterministic=not training)
        
        # 48ä¸ªTransformerå—
        for _ in range(self.config.n_layer):
            x = TransformerBlock(self.config)(x, training)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        x = nn.LayerNorm()(x)
        
        # è¯­è¨€æ¨¡å‹å¤´
        logits = nn.Dense(self.config.vocab_size, use_bias=False)(x)
        
        return logits


class GraphPartitionedGPT:
    """å›¾åˆ†å‰²GPTæ¨ç†å¼•æ“"""
    
    def __init__(self, config: GPTConfig):
        self.config = config
        self.devices = jax.devices()
        self.num_devices = len(self.devices)
        
        print(f"ğŸ”§ åˆå§‹åŒ–å›¾åˆ†å‰²GPTæ¨ç†å¼•æ“")
        print(f"   GPUæ•°é‡: {self.num_devices}")
        for i, device in enumerate(self.devices):
            print(f"   GPU {i}: {device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = GPTModel(config)
        self._init_model_parameters()
        
        # è®¾ç½®å¤šGPUåˆ†ç‰‡
        if self.num_devices > 1:
            self._setup_graph_partitioning()
        else:
            print("â„¹ï¸ å•GPUæ¨¡å¼")
            self.sharded_params = self.params
    
    def _init_model_parameters(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        print("ğŸ”„ åˆå§‹åŒ–GPT-1.5Bæ¨¡å‹å‚æ•°...")
        
        key = jax.random.PRNGKey(42)
        dummy_input = jnp.ones((1, 128), dtype=jnp.int32)
        
        # åˆå§‹åŒ–å‚æ•°
        self.params = self.model.init(key, dummy_input, training=False)
        
        # è®¡ç®—å‚æ•°é‡
        param_count = sum(x.size for x in jax.tree_util.tree_leaves(self.params))
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {param_count:,} ({param_count/1e9:.2f}B)")
    
    def _setup_graph_partitioning(self):
        """è®¾ç½®å›¾åˆ†å‰²å’Œå‚æ•°åˆ†ç‰‡"""
        print("ğŸ•¸ï¸ è®¾ç½®å›¾åˆ†å‰²å’Œå¤šGPUå¹¶è¡Œ...")
        
        # åˆ›å»ºè®¾å¤‡ç½‘æ ¼
        self.mesh = jax.make_mesh((self.num_devices,), ('model',))
        
        # å®šä¹‰åˆ†ç‰‡ç­–ç•¥
        def get_partition_spec(param):
            """ä¸ºä¸åŒå‚æ•°å®šä¹‰åˆ†ç‰‡ç­–ç•¥"""
            if param.ndim >= 2:
                # å¤§çš„æƒé‡çŸ©é˜µæŒ‰ç¬¬ä¸€ä¸ªç»´åº¦åˆ†ç‰‡
                if param.shape[0] >= 512:
                    return jax.sharding.PartitionSpec('model', None)
                elif param.shape[1] >= 512:
                    return jax.sharding.PartitionSpec(None, 'model')
                else:
                    return jax.sharding.PartitionSpec()
            else:
                # 1Då‚æ•°ä¸åˆ†ç‰‡
                return jax.sharding.PartitionSpec()
        
        # åº”ç”¨åˆ†ç‰‡è§„èŒƒ
        self.param_spec = jax.tree_util.tree_map(get_partition_spec, self.params)
        
        # åˆ›å»ºåˆ†ç‰‡å¹¶åˆ†å¸ƒå‚æ•°
        sharding = jax.sharding.NamedSharding(self.mesh, self.param_spec)
        self.sharded_params = jax.device_put(self.params, sharding)
        
        print(f"âœ… å›¾åˆ†å‰²å®Œæˆï¼Œå‚æ•°å·²åˆ†å¸ƒåˆ° {self.num_devices} ä¸ªGPU")
    
    @partial(jax.jit, static_argnums=(0,))
    def forward_pass(self, params, input_ids):
        """JITç¼–è¯‘çš„å‰å‘ä¼ æ’­"""
        return self.model.apply(params, input_ids, training=False)
    
    @partial(jax.jit, static_argnums=(0,))
    def generate_next_token(self, params, input_ids):
        """ç”Ÿæˆä¸‹ä¸€ä¸ªtokenï¼ˆJITç¼–è¯‘ï¼‰"""
        logits = self.forward_pass(params, input_ids)
        # è´ªå©ªè§£ç ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
        next_token = jnp.argmax(logits[:, -1, :], axis=-1, keepdims=True)
        return next_token
    
    def generate_text(self, input_ids: jnp.ndarray, max_new_tokens: int = 32) -> jnp.ndarray:
        """è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ"""
        current_ids = input_ids
        params = self.sharded_params
        
        for step in range(max_new_tokens):
            # é˜²æ­¢åºåˆ—è¿‡é•¿
            if current_ids.shape[1] >= self.config.n_positions:
                # æˆªæ–­ä¿ç•™æœ€æ–°çš„éƒ¨åˆ†
                current_ids = current_ids[:, -(self.config.n_positions-1):]
            
            # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
            next_token = self.generate_next_token(params, current_ids)
            
            # æ‹¼æ¥æ–°token
            current_ids = jnp.concatenate([current_ids, next_token], axis=1)
        
        return current_ids
