"""
æ•°æ®é›†åŠ è½½å™¨ - å¤„ç†æ‚¨çš„JSONLæ ¼å¼æ•°æ®é›†
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional


class SimpleTokenizer:
    """ç®€åŒ–çš„æ–‡æœ¬åˆ†è¯å™¨"""
    
    def __init__(self, vocab_size: int = 50257):
        self.vocab_size = vocab_size
        self.pad_token_id = 0
    
    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå­—ç¬¦çš„ç¼–ç 
        tokens = []
        for char in text.lower():
            token_id = min(ord(char), self.vocab_size - 1)
            tokens.append(token_id)
        
        # å¤„ç†é•¿åº¦é™åˆ¶
        if max_length:
            if len(tokens) > max_length:
                tokens = tokens[:max_length]
            else:
                # å¡«å……åˆ°æŒ‡å®šé•¿åº¦
                tokens.extend([self.pad_token_id] * (max_length - len(tokens)))
        
        return tokens
    
    def decode(self, tokens: List[int]) -> str:
        """è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬"""
        chars = []
        for token_id in tokens:
            if token_id != self.pad_token_id and token_id > 0:
                chars.append(chr(min(token_id, 127)))  # é™åˆ¶åœ¨ASCIIèŒƒå›´å†…
        return ''.join(chars)


class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨ - å¤„ç†JSONLæ ¼å¼æ•°æ®"""
    
    def __init__(self, dataset_dir: str):
        self.dataset_dir = Path(dataset_dir)
        self.datasets = {}
        self._load_all_datasets()
    
    def _load_all_datasets(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†é…ç½®"""
        print("ğŸ“ åŠ è½½æ•°æ®é›†...")
        
        if not self.dataset_dir.exists():
            print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {self.dataset_dir}")
            return
        
        # æŸ¥æ‰¾æ‰€æœ‰é…ç½®æ–‡ä»¶
        config_files = list(self.dataset_dir.glob("benchmark_dataset_config_*.jsonl"))
        print(f"ğŸ” æ‰¾åˆ° {len(config_files)} ä¸ªé…ç½®æ–‡ä»¶")
        
        for config_file in config_files:
            config_id = config_file.stem.split('_')[-1]
            samples = []
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            if config_file.stat().st_size == 0:
                print(f"âš ï¸ é…ç½® {config_id} æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # è¯»å–JSONLæ–‡ä»¶
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                sample = json.loads(line)
                                samples.append(sample)
                            except json.JSONDecodeError as e:
                                print(f"âš ï¸ é…ç½® {config_id} ç¬¬ {line_num} è¡ŒJSONè§£æé”™è¯¯: {e}")
                                continue
                
                if samples:
                    self.datasets[config_id] = samples
                    print(f"ğŸ“Š é…ç½® {config_id}: {len(samples)} ä¸ªæ ·æœ¬")
                    
                    # æ˜¾ç¤ºç¤ºä¾‹
                    if samples:
                        sample = samples[0]
                        print(f"   ç¤ºä¾‹: prompt_length={sample.get('prompt_length', 'N/A')}, "
                              f"generation_length={sample.get('generation_length', 'N/A')}")
                else:
                    print(f"âš ï¸ é…ç½® {config_id} æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
                    
            except Exception as e:
                print(f"âŒ åŠ è½½é…ç½® {config_id} å¤±è´¥: {e}")
    
    def get_valid_datasets(self) -> Dict[str, List[Dict]]:
        """è·å–æ‰€æœ‰æœ‰æ•ˆæ•°æ®é›†"""
        return {k: v for k, v in self.datasets.items() if v}
    
    def get_dataset_stats(self) -> Dict[str, Dict]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for config_id, samples in self.datasets.items():
            if not samples:
                continue
                
            prompt_lengths = [s.get('prompt_length', 0) for s in samples]
            gen_lengths = [s.get('generation_length', 0) for s in samples]
            
            stats[config_id] = {
                'sample_count': len(samples),
                'avg_prompt_length': np.mean(prompt_lengths) if prompt_lengths else 0,
                'avg_generation_length': np.mean(gen_lengths) if gen_lengths else 0,
                'prompt_length_range': (min(prompt_lengths), max(prompt_lengths)) if prompt_lengths else (0, 0),
                'generation_length_range': (min(gen_lengths), max(gen_lengths)) if gen_lengths else (0, 0),
                'source_types': list(set(s.get('source_type', 'unknown') for s in samples))
            }
        
        return stats
    
    def get_dataset_by_config(self, config_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šé…ç½®çš„æ•°æ®é›†"""
        return self.datasets.get(config_id, [])
    
    def print_dataset_summary(self):
        """æ‰“å°æ•°æ®é›†æ‘˜è¦"""
        stats = self.get_dataset_stats()
        print("\nğŸ“Š æ•°æ®é›†æ‘˜è¦:")
        print("-" * 50)
        
        for config_id, stat in stats.items():
            print(f"é…ç½® {config_id}:")
            print(f"  æ ·æœ¬æ•°: {stat['sample_count']}")
            print(f"  å¹³å‡prompté•¿åº¦: {stat['avg_prompt_length']:.1f}")
            print(f"  å¹³å‡ç”Ÿæˆé•¿åº¦: {stat['avg_generation_length']:.1f}")
            print(f"  Prompté•¿åº¦èŒƒå›´: {stat['prompt_length_range']}")
            print(f"  ç”Ÿæˆé•¿åº¦èŒƒå›´: {stat['generation_length_range']}")
            print(f"  æ•°æ®æºç±»å‹: {', '.join(stat['source_types'])}")
            print()
