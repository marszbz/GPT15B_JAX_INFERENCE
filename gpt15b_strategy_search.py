#!/usr/bin/env python3
"""
GPT-1.5B JAXç­–ç•¥æœç´¢ç³»ç»Ÿ
è‡ªåŠ¨æœç´¢æœ€ä¼˜çš„åˆ†ç‰‡å’Œå¹¶è¡ŒåŒ–ç­–ç•¥
"""

import os
import sys
import time
import json
import itertools
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Set
from dataclasses import dataclass, asdict
from functools import partial
import numpy as np

# è®¾ç½®JAXç¯å¢ƒ
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

try:
    import jax
    import jax.numpy as jnp
    from jax import random, devices, make_jaxpr
    from jax.sharding import Mesh, PartitionSpec, NamedSharding
    from jax.experimental import mesh_utils
    import flax.linen as nn
    print(f"âœ… JAX {jax.__version__} ç­–ç•¥æœç´¢æ¨¡å¼")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class HardwareInfo:
    """ç¡¬ä»¶ä¿¡æ¯"""
    gpu_name: str
    memory_gb: float
    compute_capability: str
    device_count: int
    config_key: str

# å¯¼å…¥ç­–ç•¥æœç´¢é…ç½®
try:
    from strategy_search_config import (
        SEARCH_SPACE, CONSTRAINTS, OPTIMIZATION_OBJECTIVES,
        SEARCH_ALGORITHMS, BENCHMARK_CONFIG, MODEL_VARIANTS,
        HARDWARE_CONFIGS, ADVANCED_OPTIONS, OUTPUT_CONFIG
    )
    print("âœ… ç­–ç•¥æœç´¢é…ç½®åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ ç­–ç•¥æœç´¢é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
    # ä½¿ç”¨é»˜è®¤é…ç½®çš„fallbackä»£ç 
    HARDWARE_CONFIGS = {}
    CONSTRAINTS = {"max_memory_per_gpu": 20.0}

@dataclass
class GPTConfig:
    """GPT-1.5Bé…ç½®"""
    vocab_size: int = 50257
    n_positions: int = 2048
    n_embd: int = 1600
    n_layer: int = 48
    n_head: int = 25
    dropout: float = 0.0
    use_bias: bool = True
    
    def get_param_count(self) -> int:
        """ä¼°ç®—å‚æ•°æ•°é‡"""
        embed_params = self.vocab_size * self.n_embd + self.n_positions * self.n_embd
        layer_params = (
            self.n_embd * 3 * self.n_embd +  # QKV
            self.n_embd * self.n_embd +      # attention output
            self.n_embd * 4 * self.n_embd +  # MLP up
            4 * self.n_embd * self.n_embd +  # MLP down
            4 * self.n_embd                    # layer norms
        )
        total_params = embed_params + self.n_layer * layer_params
        return total_params

@dataclass
class ShardingStrategy:
    """åˆ†ç‰‡ç­–ç•¥é…ç½®"""
    name: str
    mesh_shape: Tuple[int, ...]
    mesh_axes: Tuple[str, ...]
    
    # æ¨¡å‹åˆ†ç‰‡ç­–ç•¥
    embedding_spec: PartitionSpec
    attention_qkv_spec: PartitionSpec
    attention_out_spec: PartitionSpec
    mlp_up_spec: PartitionSpec
    mlp_down_spec: PartitionSpec
    lm_head_spec: PartitionSpec
    
    # æ•°æ®åˆ†ç‰‡ç­–ç•¥
    input_spec: PartitionSpec
    
    # é¢„æœŸæ€§èƒ½ç‰¹å¾
    memory_efficiency: float = 0.0
    compute_efficiency: float = 0.0
    communication_overhead: float = 0.0

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    throughput_tokens_per_sec: float
    latency_ms: float
    memory_usage_gb: float
    gpu_utilization: float
    communication_time_ms: float
    compilation_time_ms: float
    
    # ç»¼åˆè¯„åˆ†
    overall_score: float = 0.0

class StrategySearchSpace:
    """ç­–ç•¥æœç´¢ç©ºé—´å®šä¹‰"""
    
    def __init__(self, num_devices: int):
        self.num_devices = num_devices
        self.possible_mesh_shapes = self._generate_mesh_shapes()
        self.possible_partition_specs = self._generate_partition_specs()
    
    def _generate_mesh_shapes(self) -> List[Tuple[int, ...]]:
        """ç”Ÿæˆå¯èƒ½çš„meshå½¢çŠ¶"""
        shapes = []
        
        # 1D mesh
        shapes.append((self.num_devices,))
        
        # 2D mesh
        for i in range(1, self.num_devices + 1):
            if self.num_devices % i == 0:
                j = self.num_devices // i
                if i <= j:  # é¿å…é‡å¤
                    shapes.append((i, j))
        
        # 3D mesh (å¦‚æœè®¾å¤‡æ•°è¶³å¤Ÿ)
        if self.num_devices >= 8:
            for i in range(1, int(self.num_devices**0.33) + 2):
                for j in range(i, int((self.num_devices/i)**0.5) + 2):
                    if self.num_devices % (i * j) == 0:
                        k = self.num_devices // (i * j)
                        if j <= k:
                            shapes.append((i, j, k))
        
        return shapes
    
    def _generate_partition_specs(self) -> Dict[str, List[PartitionSpec]]:
        """ç”Ÿæˆå¯èƒ½çš„åˆ†ç‰‡è§„èŒƒ"""
        specs = {
            'no_shard': [PartitionSpec()],
            'data_only': [PartitionSpec('data', None)],
            'model_only': [PartitionSpec(None, 'model')],
            'data_model': [PartitionSpec('data', 'model')],
            'model_data': [PartitionSpec('model', 'data')],
        }
        
        # 3Dåˆ†ç‰‡ï¼ˆå¦‚æœæœ‰ç¬¬ä¸‰ä¸ªè½´ï¼‰
        if self.num_devices >= 8:
            specs.update({
                'pipeline_data': [PartitionSpec('pipeline', 'data', None)],
                'pipeline_model': [PartitionSpec('pipeline', None, 'model')],
                'full_3d': [PartitionSpec('pipeline', 'data', 'model')]
            })
        
        return specs
    
    def generate_all_strategies(self) -> List[ShardingStrategy]:
        """ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç­–ç•¥ç»„åˆ"""
        strategies = []
        
        for mesh_shape in self.possible_mesh_shapes:
            # ç¡®å®šè½´åç§°
            if len(mesh_shape) == 1:
                mesh_axes = ('data',)
                available_specs = ['no_shard', 'data_only']
            elif len(mesh_shape) == 2:
                mesh_axes = ('data', 'model')
                available_specs = ['no_shard', 'data_only', 'model_only', 'data_model']
            else:  # 3D
                mesh_axes = ('pipeline', 'data', 'model')
                available_specs = list(self.possible_partition_specs.keys())
            
            # ä¸ºæ¯ä¸ªç»„ä»¶é€‰æ‹©åˆ†ç‰‡ç­–ç•¥
            embedding_specs = self._filter_specs_for_component('embedding', available_specs)
            attention_specs = self._filter_specs_for_component('attention', available_specs)
            mlp_specs = self._filter_specs_for_component('mlp', available_specs)
            
            # ç”Ÿæˆç­–ç•¥ç»„åˆï¼ˆé™åˆ¶æ•°é‡é¿å…çˆ†ç‚¸ï¼‰
            counter = 0
            for emb_spec_name in embedding_specs:
                for att_spec_name in attention_specs:
                    for mlp_spec_name in mlp_specs:
                        if counter >= 20:  # æ¯ä¸ªmeshå½¢çŠ¶æœ€å¤š20ä¸ªç­–ç•¥
                            break
                        
                        emb_spec = self.possible_partition_specs[emb_spec_name][0]
                        att_spec = self.possible_partition_specs[att_spec_name][0]
                        mlp_spec = self.possible_partition_specs[mlp_spec_name][0]
                        
                        strategy = ShardingStrategy(
                            name=f"mesh{mesh_shape}_{emb_spec_name}_{att_spec_name}_{mlp_spec_name}",
                            mesh_shape=mesh_shape,
                            mesh_axes=mesh_axes,
                            embedding_spec=emb_spec,
                            attention_qkv_spec=att_spec,
                            attention_out_spec=att_spec,
                            mlp_up_spec=mlp_spec,
                            mlp_down_spec=mlp_spec,
                            lm_head_spec=emb_spec,  # é€šå¸¸ä¸embeddingç›¸åŒ
                            input_spec=PartitionSpec(mesh_axes[0] if mesh_axes else None, None)
                        )
                        
                        strategies.append(strategy)
                        counter += 1
                
                if counter >= 20:
                    break
        
        return strategies
    
    def _filter_specs_for_component(self, component: str, available_specs: List[str]) -> List[str]:
        """ä¸ºç‰¹å®šç»„ä»¶è¿‡æ»¤åˆé€‚çš„åˆ†ç‰‡è§„èŒƒ"""
        if component == 'embedding':
            # åµŒå…¥å±‚é€‚åˆè¯æ±‡è¡¨åˆ†ç‰‡
            return [spec for spec in available_specs if 'model' in spec or spec == 'no_shard']
        elif component == 'attention':
            # æ³¨æ„åŠ›å±‚é€‚åˆå¤´åˆ†ç‰‡
            return available_specs
        elif component == 'mlp':
            # MLPå±‚é€‚åˆéšè—å±‚åˆ†ç‰‡
            return available_specs
        else:
            return available_specs

class SimpleGPTModel(nn.Module):
    """ç®€åŒ–çš„GPTæ¨¡å‹ç”¨äºç­–ç•¥æœç´¢"""
    config: GPTConfig
    
    def setup(self):
        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)
        self.wpe = nn.Embed(self.config.n_positions, self.config.n_embd)
        
        # åªä½¿ç”¨å‡ å±‚è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        self.layers = [self._create_layer() for _ in range(4)]
        self.ln_f = nn.LayerNorm()
        self.lm_head = nn.Dense(self.config.vocab_size, use_bias=False)
    
    def _create_layer(self):
        """åˆ›å»ºå•ä¸ªTransformerå±‚"""
        return {
            'ln_1': nn.LayerNorm(),
            'attn_qkv': nn.Dense(3 * self.config.n_embd, use_bias=self.config.use_bias),
            'attn_out': nn.Dense(self.config.n_embd, use_bias=self.config.use_bias),
            'ln_2': nn.LayerNorm(),
            'mlp_up': nn.Dense(4 * self.config.n_embd, use_bias=self.config.use_bias),
            'mlp_down': nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)
        }
    
    def __call__(self, input_ids):
        B, T = input_ids.shape
        
        # Token + Position embedding
        token_emb = self.wte(input_ids)
        pos = jnp.arange(0, T)[None, :]
        pos_emb = self.wpe(pos)
        x = token_emb + pos_emb
        
        # Transformer layers (ç®€åŒ–å®ç°)
        for layer in self.layers:
            # Self-attention
            qkv = layer['attn_qkv'](layer['ln_1'](x))
            # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
            att_out = layer['attn_out'](qkv[:, :, :self.config.n_embd])
            x = x + att_out
            
            # MLP
            mlp_hidden = layer['mlp_up'](layer['ln_2'](x))
            mlp_out = layer['mlp_down'](jax.nn.gelu(mlp_hidden))
            x = x + mlp_out
        
        # Final output
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        return logits

class StrategyBenchmarker:
    """ç­–ç•¥åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self, config: GPTConfig):
        self.config = config
        self.devices = jax.devices()
        self.model = SimpleGPTModel(config)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        key = jax.random.PRNGKey(42)
        self.test_input = jax.random.randint(key, (4, 128), 0, config.vocab_size)
        
        print(f"ğŸ”§ ç­–ç•¥åŸºå‡†æµ‹è¯•å™¨åˆå§‹åŒ–")
        print(f"   å¯ç”¨è®¾å¤‡: {len(self.devices)}")
        print(f"   æµ‹è¯•è¾“å…¥: {self.test_input.shape}")
    
    def benchmark_strategy(self, strategy: ShardingStrategy) -> Optional[PerformanceMetrics]:
        """åŸºå‡†æµ‹è¯•å•ä¸ªç­–ç•¥"""
        try:
            print(f"\nâš¡ æµ‹è¯•ç­–ç•¥: {strategy.name}")
            print(f"   Meshå½¢çŠ¶: {strategy.mesh_shape}")
            print(f"   Meshè½´: {strategy.mesh_axes}")
            
            # åˆ›å»ºmesh
            if len(strategy.mesh_shape) == 1:
                devices_array = np.array(self.devices[:strategy.mesh_shape[0]])
            elif len(strategy.mesh_shape) == 2:
                total_devices = strategy.mesh_shape[0] * strategy.mesh_shape[1]
                devices_array = np.array(self.devices[:total_devices]).reshape(strategy.mesh_shape)
            else:  # 3D
                total_devices = np.prod(strategy.mesh_shape)
                devices_array = np.array(self.devices[:total_devices]).reshape(strategy.mesh_shape)
            
            mesh = Mesh(devices_array, axis_names=strategy.mesh_axes)
            
            with mesh:
                # åˆ›å»ºåˆ†ç‰‡æ•°æ®
                input_sharding = NamedSharding(mesh, strategy.input_spec)
                input_sharded = jax.device_put(self.test_input, input_sharding)
                
                # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
                key = jax.random.PRNGKey(42)
                params = self.model.init(key, self.test_input)
                
                # åº”ç”¨åˆ†ç‰‡ç­–ç•¥åˆ°å‚æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰
                params_sharded = self._apply_sharding_to_params(params, strategy, mesh)
                
                # JITç¼–è¯‘
                @jax.jit
                def sharded_inference(params, input_ids):
                    return self.model.apply(params, input_ids)
                
                # ç¼–è¯‘æ—¶é—´æµ‹é‡
                compilation_start = time.time()
                
                # é¢„çƒ­ç¼–è¯‘
                _ = sharded_inference(params_sharded, input_sharded)
                jax.block_until_ready(_)
                
                compilation_time = (time.time() - compilation_start) * 1000
                
                # æ€§èƒ½æµ‹è¯•
                times = []
                for _ in range(5):
                    start_time = time.time()
                    result = sharded_inference(params_sharded, input_sharded)
                    jax.block_until_ready(result)
                    end_time = time.time()
                    times.append(end_time - start_time)
                
                avg_time = np.mean(times)
                tokens_processed = self.test_input.size
                throughput = tokens_processed / avg_time
                
                # å†…å­˜ä½¿ç”¨ä¼°ç®—ï¼ˆç®€åŒ–ï¼‰
                param_memory = self._estimate_memory_usage(params_sharded, strategy)
                
                metrics = PerformanceMetrics(
                    throughput_tokens_per_sec=throughput,
                    latency_ms=avg_time * 1000,
                    memory_usage_gb=param_memory,
                    gpu_utilization=85.0,  # ç®€åŒ–å‡è®¾
                    communication_time_ms=avg_time * 1000 * 0.1,  # å‡è®¾10%é€šä¿¡å¼€é”€
                    compilation_time_ms=compilation_time
                )
                
                # è®¡ç®—ç»¼åˆè¯„åˆ†
                metrics.overall_score = self._calculate_overall_score(metrics, strategy)
                
                print(f"   âœ… æ€§èƒ½: {throughput:.1f} tokens/s, {avg_time*1000:.2f}ms")
                
                return metrics
                
        except Exception as e:
            print(f"   âŒ ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def _apply_sharding_to_params(self, params, strategy: ShardingStrategy, mesh: Mesh):
        """å°†åˆ†ç‰‡ç­–ç•¥åº”ç”¨åˆ°å‚æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ ¹æ®å‚æ•°åç§°å’Œå½¢çŠ¶åº”ç”¨ä¸åŒçš„åˆ†ç‰‡ç­–ç•¥
        return params
    
    def _estimate_memory_usage(self, params, strategy: ShardingStrategy) -> float:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆGBï¼‰"""
        # ç®€åŒ–çš„å†…å­˜ä¼°ç®—
        param_count = sum(p.size for p in jax.tree_util.tree_leaves(params))
        memory_gb = param_count * 4 / (1024**3)  # float32
        
        # æ ¹æ®åˆ†ç‰‡ç­–ç•¥è°ƒæ•´
        devices_used = np.prod(strategy.mesh_shape)
        if devices_used > 1:
            memory_gb = memory_gb / devices_used * 1.2  # è€ƒè™‘é€šä¿¡ç¼“å†²
        
        return memory_gb
    
    def _calculate_overall_score(self, metrics: PerformanceMetrics, strategy: ShardingStrategy) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        # æƒé‡è®¾ç½®
        throughput_weight = 0.4
        latency_weight = 0.3
        memory_weight = 0.2
        compilation_weight = 0.1
        
        # å½’ä¸€åŒ–æŒ‡æ ‡ï¼ˆå‡è®¾åŸºå‡†å€¼ï¼‰
        throughput_score = min(metrics.throughput_tokens_per_sec / 1000.0, 1.0)
        latency_score = max(0, 1.0 - metrics.latency_ms / 200.0)
        memory_score = max(0, 1.0 - metrics.memory_usage_gb / 10.0)
        compilation_score = max(0, 1.0 - metrics.compilation_time_ms / 5000.0)
        
        overall_score = (
            throughput_weight * throughput_score +
            latency_weight * latency_score +
            memory_weight * memory_score +
            compilation_weight * compilation_score
        )
        
        return overall_score

class StrategySearcher:
    """ç­–ç•¥æœç´¢å™¨"""
    
    def __init__(self, config: GPTConfig, hardware_info: HardwareInfo = None):
        self.config = config
        self.devices = jax.devices()
        
        # ç¡¬ä»¶æ£€æµ‹å’Œçº¦æŸåº”ç”¨
        if hardware_info is None:
            self.hardware_info = detect_hardware()
        else:
            self.hardware_info = hardware_info
        
        self.constraints = apply_hardware_constraints(self.hardware_info)
        
        print(f"ğŸ¯ ç­–ç•¥æœç´¢é…ç½®:")
        print(f"   ç¡¬ä»¶: {self.hardware_info.gpu_name} x{self.hardware_info.device_count}")
        print(f"   å†…å­˜é™åˆ¶: {self.constraints.get('max_memory_per_gpu', 'N/A')}GB")
        print(f"   æœ€å¤§æ‰¹æ¬¡: {self.constraints.get('max_batch_size', 'N/A')}")
        
        self.search_space = StrategySearchSpace(len(self.devices))
        self.benchmarker = StrategyBenchmarker(config)
        self.results: List[Tuple[ShardingStrategy, PerformanceMetrics]] = []
    
    def _filter_strategies_by_hardware(self, strategies: List[ShardingStrategy]) -> List[ShardingStrategy]:
        """æ ¹æ®ç¡¬ä»¶é™åˆ¶è¿‡æ»¤ç­–ç•¥"""
        filtered = []
        
        for strategy in strategies:
            # æ£€æŸ¥å†…å­˜çº¦æŸ
            if hasattr(strategy, 'estimated_memory_per_gpu'):
                if strategy.estimated_memory_per_gpu > self.constraints.get('max_memory_per_gpu', float('inf')):
                    continue
            
            # å¯¹äºRTX 3080ï¼Œä¼˜å…ˆé€‰æ‹©æ¨¡å‹å¹¶è¡Œç­–ç•¥
            if self.hardware_info.config_key.startswith('rtx3080'):
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ¨¡å‹å¹¶è¡Œ
                if not any('model' in str(spec) for spec in [
                    strategy.embedding_spec, strategy.attention_qkv_spec,
                    strategy.mlp_up_spec, strategy.mlp_down_spec                ]):
                    continue  # è·³è¿‡çº¯æ•°æ®å¹¶è¡Œç­–ç•¥
            
            filtered.append(strategy)
        
        print(f"ğŸ“‹ ç¡¬ä»¶è¿‡æ»¤: {len(strategies)} â†’ {len(filtered)} ç­–ç•¥")
        return filtered
    
    def exhaustive_search(self) -> List[Tuple[ShardingStrategy, PerformanceMetrics]]:
        """ç©·ä¸¾æœç´¢æ‰€æœ‰ç­–ç•¥"""
        print(f"\nğŸ” å¼€å§‹ç©·ä¸¾ç­–ç•¥æœç´¢")
        print("=" * 60)
        
        strategies = self.search_space.generate_all_strategies()
        print(f"ğŸ“Š ç”Ÿæˆç­–ç•¥æ•°é‡: {len(strategies)}")
        
        # æ ¹æ®ç¡¬ä»¶è¿‡æ»¤ç­–ç•¥
        strategies = self._filter_strategies_by_hardware(strategies)
        
        successful_results = []
        
        for i, strategy in enumerate(strategies):
            print(f"\n[{i+1}/{len(strategies)}] æµ‹è¯•ç­–ç•¥: {strategy.name}")
            print(f"   Mesh: {strategy.mesh_shape} {strategy.mesh_axes}")
            
            # é’ˆå¯¹RTX 3080çš„ç‰¹æ®Šå¤„ç†
            if self.hardware_info.config_key.startswith('rtx3080'):
                print(f"   ğŸ¯ RTX 3080ä¼˜åŒ–æ¨¡å¼")
                # å¯ä»¥æ·»åŠ ç‰¹æ®Šçš„å†…å­˜æ£€æŸ¥æˆ–ä¼˜åŒ–
            
            metrics = self.benchmarker.benchmark_strategy(strategy)
            if metrics:
                successful_results.append((strategy, metrics))
                print(f"   ç»¼åˆè¯„åˆ†: {metrics.overall_score:.3f}")
            
            # æ¯æµ‹è¯•5ä¸ªç­–ç•¥æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 5 == 0:
                print(f"\nğŸ“ˆ è¿›åº¦: {i+1}/{len(strategies)} ({(i+1)/len(strategies)*100:.1f}%)")
                if successful_results:
                    best_so_far = max(successful_results, key=lambda x: x[1].overall_score)
                    print(f"   å½“å‰æœ€ä½³: {best_so_far[0].name} (è¯„åˆ†: {best_so_far[1].overall_score:.3f})")
        
        self.results = successful_results
        return successful_results
    
    def smart_search(self) -> List[Tuple[ShardingStrategy, PerformanceMetrics]]:
        """æ™ºèƒ½ç­–ç•¥æœç´¢ - æ ¹æ®ç¡¬ä»¶ç‰¹æ€§é€‰æ‹©æœ€ä¼˜ç­–ç•¥"""
        print(f"\nğŸ§  æ™ºèƒ½ç­–ç•¥æœç´¢")
        print("=" * 60)
        print(f"ğŸ¯ ç›®æ ‡ç¡¬ä»¶: {self.hardware_info.gpu_name} x{self.hardware_info.device_count}")
        print(f"ğŸ’¾ å†…å­˜é™åˆ¶: {self.constraints.get('max_memory_per_gpu')}GB")
        
        candidate_strategies = []
        
        # 1. åŸºç¡€ç­–ç•¥
        print(f"\nğŸ“‹ Step 1: ç”ŸæˆåŸºç¡€ç­–ç•¥")
        basic_strategies = self.search_space.generate_all_strategies()
        candidate_strategies.extend(basic_strategies[:10])  # é™åˆ¶æ•°é‡
        print(f"   é€‰æ‹©å‰10ä¸ªåŸºç¡€ç­–ç•¥")
        
        # 2. ç¡¬ä»¶ç‰¹å®šç­–ç•¥
        if self.hardware_info.config_key.startswith('rtx3080'):
            print(f"\nğŸ¯ Step 2: ç”ŸæˆRTX 3080ä¸“ç”¨ç­–ç•¥")
            rtx3080_strategies = self.generate_rtx3080_optimized_strategies()
            candidate_strategies.extend(rtx3080_strategies)
        else:
            print(f"\nğŸ”§ Step 2: ç”Ÿæˆé€šç”¨ä¼˜åŒ–ç­–ç•¥")
            # å¯ä»¥æ·»åŠ å…¶ä»–GPUçš„ç‰¹å®šç­–ç•¥
        
        # 3. æ ¹æ®é…ç½®æ–‡ä»¶æ·»åŠ é¢„å®šä¹‰ç­–ç•¥
        print(f"\nğŸ“– Step 3: æ·»åŠ é…ç½®æ–‡ä»¶ç­–ç•¥")
        config_strategies = self._load_predefined_strategies()
        candidate_strategies.extend(config_strategies)
        
        # 4. ç¡¬ä»¶è¿‡æ»¤
        print(f"\nğŸ” Step 4: ç¡¬ä»¶å…¼å®¹æ€§è¿‡æ»¤")
        filtered_strategies = self._filter_strategies_by_hardware(candidate_strategies)
        
        # 5. æ™ºèƒ½æ’åº
        print(f"\nğŸ¯ Step 5: ç­–ç•¥ä¼˜å…ˆçº§æ’åº")
        sorted_strategies = self._sort_strategies_by_priority(filtered_strategies)
        
        # 6. æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        print(f"\nâš¡ Step 6: æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
        results = []
        max_test_strategies = min(10, len(sorted_strategies))  # æœ€å¤šæµ‹è¯•10ä¸ªç­–ç•¥
        
        for i, strategy in enumerate(sorted_strategies[:max_test_strategies]):
            print(f"\n[{i+1}/{max_test_strategies}] ğŸ§ª æµ‹è¯•ç­–ç•¥: {strategy.name}")
            try:
                metrics = self.benchmarker.benchmark_strategy(strategy)
                if metrics is not None:
                    results.append((strategy, metrics))
                    print(f"   âœ… æˆåŠŸ: {metrics.throughput_tokens_per_sec:.1f} tokens/s")
                else:
                    print(f"   âŒ å¤±è´¥: ç­–ç•¥ä¸å…¼å®¹")
            except Exception as e:
                print(f"   âŒ é”™è¯¯: {str(e)[:50]}...")
        
        # 7. ç»“æœåˆ†æ
        if results:
            best_strategy, best_metrics = max(results, key=lambda x: x[1].overall_score)
            print(f"\nğŸ† æœ€ä¼˜ç­–ç•¥: {best_strategy.name}")
            print(f"   ååé‡: {best_metrics.throughput_tokens_per_sec:.1f} tokens/s")
            print(f"   å»¶è¿Ÿ: {best_metrics.latency_ms:.1f}ms")
            print(f"   å†…å­˜ä½¿ç”¨: {best_metrics.memory_usage_gb:.1f}GB")
            print(f"   GPUåˆ©ç”¨ç‡: {best_metrics.gpu_utilization:.1%}")
        
        return results
    
    def _load_predefined_strategies(self) -> List[ShardingStrategy]:
        """ä»é…ç½®æ–‡ä»¶åŠ è½½é¢„å®šä¹‰ç­–ç•¥"""
        strategies = []
        
        try:
            from strategy_search_config import SEARCH_SPACE
            templates = SEARCH_SPACE.get("sharding_templates", {})
            
            for template_name, template_config in templates.items():
                if template_name == "rtx3080_optimized" and not self.hardware_info.config_key.startswith('rtx3080'):
                    continue  # è·³è¿‡ä¸åŒ¹é…çš„ç¡¬ä»¶ç‰¹å®šç­–ç•¥
                
                # å°†æ¨¡æ¿è½¬æ¢ä¸ºShardingStrategyå¯¹è±¡
                strategy = self._template_to_strategy(template_name, template_config)
                if strategy:
                    strategies.append(strategy)
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ç­–ç•¥åŠ è½½å¤±è´¥: {e}")
        
        return strategies
    
    def _template_to_strategy(self, name: str, template: Dict[str, str]) -> Optional[ShardingStrategy]:
        """å°†æ¨¡æ¿é…ç½®è½¬æ¢ä¸ºç­–ç•¥å¯¹è±¡"""
        try:
            # è§£æPartitionSpecå­—ç¬¦ä¸²
            def parse_spec(spec_str: str) -> PartitionSpec:
                # ç®€åŒ–çš„è§£æï¼Œå®é™…å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
                if "('data', None)" in spec_str:
                    return PartitionSpec('data', None)
                elif "(None, 'model')" in spec_str:
                    return PartitionSpec(None, 'model')
                elif "('data', 'model')" in spec_str:
                    return PartitionSpec('data', 'model')
                elif "(None, None, 'model')" in spec_str:
                    return PartitionSpec(None, None, 'model')
                else:
                    return PartitionSpec()
            
            # æ ¹æ®è®¾å¤‡æ•°é‡é€‰æ‹©meshå½¢çŠ¶
            if len(self.devices) >= 4:
                mesh_shape = (2, 2)
                mesh_axes = ("data", "model")
            elif len(self.devices) >= 2:
                mesh_shape = (2,)
                mesh_axes = ("data",)
            else:
                mesh_shape = (1,)
                mesh_axes = ("data",)
            
            strategy = ShardingStrategy(
                name=f"Config_{name}",
                mesh_shape=mesh_shape,
                mesh_axes=mesh_axes,
                embedding_spec=parse_spec(template.get("embedding", "PartitionSpec()")),
                attention_qkv_spec=parse_spec(template.get("attention_qkv", "PartitionSpec()")),
                attention_out_spec=parse_spec(template.get("attention_out", "PartitionSpec()")),
                mlp_up_spec=parse_spec(template.get("mlp_up", "PartitionSpec()")),
                mlp_down_spec=parse_spec(template.get("mlp_down", "PartitionSpec()")),
                lm_head_spec=parse_spec(template.get("lm_head", "PartitionSpec()")),
                input_spec=parse_spec(template.get("input", "PartitionSpec()"))
            )
            return strategy
        except Exception as e:
            print(f"âš ï¸ æ¨¡æ¿è§£æå¤±è´¥ {name}: {e}")
            return None
    
    def _sort_strategies_by_priority(self, strategies: List[ShardingStrategy]) -> List[ShardingStrategy]:
        """æ ¹æ®ä¼˜å…ˆçº§æ’åºç­–ç•¥"""
        def priority_score(strategy: ShardingStrategy) -> float:
            score = 0.0
            
            # RTX 3080åå¥½æ¨¡å‹å¹¶è¡Œ
            if self.hardware_info.config_key.startswith('rtx3080'):
                if 'model' in str(strategy.embedding_spec):
                    score += 10
                if 'model' in str(strategy.mlp_up_spec):
                    score += 10
                if strategy.name.startswith('RTX3080'):
                    score += 20
            
            # é«˜å†…å­˜æ•ˆç‡åŠ åˆ†
            score += strategy.memory_efficiency * 30
              # ä½é€šä¿¡å¼€é”€åŠ åˆ†
            score += (1 - strategy.communication_overhead) * 20
            
            return score
        
        return sorted(strategies, key=priority_score, reverse=True)
    
    def generate_rtx3080_optimized_strategies(self) -> List[ShardingStrategy]:
        """ç”ŸæˆRTX 3080ä¼˜åŒ–ç­–ç•¥"""
        strategies = []
        
        # RTX 3080ä¸“ç”¨ç­–ç•¥ - å¼ºè°ƒæ¨¡å‹å¹¶è¡Œä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        if len(self.devices) >= 4:
            # 2x2 mesh ç”¨äºæ¨¡å‹å¹¶è¡Œ
            strategy = ShardingStrategy(
                name="RTX3080_ModelParallel_2x2",
                mesh_shape=(2, 2),
                mesh_axes=('data', 'model'),
                embedding_spec=PartitionSpec(None, 'model'),
                attention_qkv_spec=PartitionSpec(None, 'model'),
                attention_out_spec=PartitionSpec('model', None),
                mlp_up_spec=PartitionSpec(None, 'model'),
                mlp_down_spec=PartitionSpec('model', None),
                lm_head_spec=PartitionSpec(None, 'model'),
                input_spec=PartitionSpec('data', None),
                memory_efficiency=0.8,
                compute_efficiency=0.7,
                communication_overhead=0.3
            )
            strategies.append(strategy)
            
            # æ··åˆç­–ç•¥
            strategy = ShardingStrategy(
                name="RTX3080_Hybrid_2x2",
                mesh_shape=(2, 2),
                mesh_axes=('data', 'model'),
                embedding_spec=PartitionSpec('data', 'model'),
                attention_qkv_spec=PartitionSpec(None, 'model'),
                attention_out_spec=PartitionSpec('model', None),
                mlp_up_spec=PartitionSpec('data', 'model'),
                mlp_down_spec=PartitionSpec('model', 'data'),
                lm_head_spec=PartitionSpec('data', 'model'),
                input_spec=PartitionSpec('data', None),
                memory_efficiency=0.9,
                compute_efficiency=0.8,
                communication_overhead=0.4
            )
            strategies.append(strategy)
        
        return strategies
    
    def analyze_results(self) -> Dict[str, Any]:
        """åˆ†ææœç´¢ç»“æœ"""
        print(f"\nğŸ“Š ç­–ç•¥æœç´¢ç»“æœåˆ†æ")
        print("=" * 60)
        
        if not self.results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„ç­–ç•¥ç»“æœ")
            return {}
        
        # æŒ‰è¯„åˆ†æ’åº
        sorted_results = sorted(self.results, key=lambda x: x[1].overall_score, reverse=True)
        
        print(f"âœ… æˆåŠŸæµ‹è¯•ç­–ç•¥æ•°é‡: {len(self.results)}")
        print(f"\nğŸ† Top 5 ç­–ç•¥:")
        
        for i, (strategy, metrics) in enumerate(sorted_results[:5]):
            print(f"\n{i+1}. {strategy.name}")
            print(f"   ç»¼åˆè¯„åˆ†: {metrics.overall_score:.3f}")
            print(f"   ååé‡: {metrics.throughput_tokens_per_sec:.1f} tokens/s")
            print(f"   å»¶è¿Ÿ: {metrics.latency_ms:.2f}ms")
            print(f"   å†…å­˜: {metrics.memory_usage_gb:.2f}GB")
            print(f"   Mesh: {strategy.mesh_shape} {strategy.mesh_axes}")
        
        # æœ€ä½³ç­–ç•¥è¯¦ç»†åˆ†æ
        best_strategy, best_metrics = sorted_results[0]
        
        print(f"\nğŸ¯ æœ€ä½³ç­–ç•¥è¯¦ç»†åˆ†æ: {best_strategy.name}")
        print("-" * 40)
        print(f"Meshé…ç½®:")
        print(f"   å½¢çŠ¶: {best_strategy.mesh_shape}")
        print(f"   è½´å: {best_strategy.mesh_axes}")
        print(f"åˆ†ç‰‡é…ç½®:")
        print(f"   è¾“å…¥: {best_strategy.input_spec}")
        print(f"   åµŒå…¥: {best_strategy.embedding_spec}")
        print(f"   æ³¨æ„åŠ›QKV: {best_strategy.attention_qkv_spec}")
        print(f"   æ³¨æ„åŠ›è¾“å‡º: {best_strategy.attention_out_spec}")
        print(f"   MLPä¸Š: {best_strategy.mlp_up_spec}")
        print(f"   MLPä¸‹: {best_strategy.mlp_down_spec}")
        print(f"   LMå¤´: {best_strategy.lm_head_spec}")
        
        # æ€§èƒ½åˆ†æ
        print(f"\næ€§èƒ½æŒ‡æ ‡:")
        print(f"   ååé‡: {best_metrics.throughput_tokens_per_sec:.1f} tokens/s")
        print(f"   å»¶è¿Ÿ: {best_metrics.latency_ms:.2f}ms")
        print(f"   å†…å­˜ä½¿ç”¨: {best_metrics.memory_usage_gb:.2f}GB")
        print(f"   GPUåˆ©ç”¨ç‡: {best_metrics.gpu_utilization:.1f}%")
        print(f"   é€šä¿¡æ—¶é—´: {best_metrics.communication_time_ms:.2f}ms")
        print(f"   ç¼–è¯‘æ—¶é—´: {best_metrics.compilation_time_ms:.2f}ms")
        
        return {
            'best_strategy': asdict(best_strategy),
            'best_metrics': asdict(best_metrics),
            'all_results': [(asdict(s), asdict(m)) for s, m in sorted_results]
        }
    
    def export_results(self) -> Path:
        """å¯¼å‡ºæœç´¢ç»“æœ"""
        results_data = self.analyze_results()
        
        # æ·»åŠ å…ƒæ•°æ®
        results_data['metadata'] = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'model_config': asdict(self.config),
            'device_count': len(self.devices),
            'total_strategies_tested': len(self.results)
        }
        
        # ä¿å­˜ç»“æœ
        results_file = Path("gpt15b_strategy_search_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜: {results_file}")
        
        # åˆ›å»ºç®€åŒ–æŠ¥å‘Š
        self._create_summary_report(results_data)
        
        return results_file
    
    def _create_summary_report(self, results_data: Dict[str, Any]):
        """åˆ›å»ºç®€åŒ–æŠ¥å‘Š"""
        report_file = Path("gpt15b_strategy_search_summary.txt")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("GPT-1.5B ç­–ç•¥æœç´¢æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            if 'best_strategy' in results_data:
                best = results_data['best_strategy']
                metrics = results_data['best_metrics']
                
                f.write(f"æœ€ä½³ç­–ç•¥: {best['name']}\n")
                f.write(f"ç»¼åˆè¯„åˆ†: {metrics['overall_score']:.3f}\n")
                f.write(f"ååé‡: {metrics['throughput_tokens_per_sec']:.1f} tokens/s\n")
                f.write(f"å»¶è¿Ÿ: {metrics['latency_ms']:.2f}ms\n")
                f.write(f"å†…å­˜: {metrics['memory_usage_gb']:.2f}GB\n")
                f.write(f"Meshå½¢çŠ¶: {best['mesh_shape']}\n")
                f.write(f"Meshè½´: {best['mesh_axes']}\n\n")
                
                f.write("æ¨èé…ç½®:\n")
                f.write(f"mesh = Mesh(devices.reshape{best['mesh_shape']}, {best['mesh_axes']})\n")
                f.write(f"input_spec = {best['input_spec']}\n")
                f.write(f"embedding_spec = {best['embedding_spec']}\n")
                f.write(f"attention_spec = {best['attention_qkv_spec']}\n")
            
            f.write(f"\næ€»æµ‹è¯•ç­–ç•¥æ•°: {results_data['metadata']['total_strategies_tested']}\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {results_data['metadata']['timestamp']}\n")
        
        print(f"ğŸ“„ ç®€åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

@dataclass
class HardwareInfo:
    """ç¡¬ä»¶ä¿¡æ¯"""
    gpu_name: str
    memory_gb: float
    compute_capability: str
    device_count: int
    config_key: str

def detect_hardware() -> HardwareInfo:
    """æ£€æµ‹å½“å‰ç¡¬ä»¶é…ç½®"""
    devices = jax.devices()
    device_count = len(devices)
    
    if device_count == 0:
        raise RuntimeError("æœªæ£€æµ‹åˆ°å¯ç”¨GPUè®¾å¤‡")
    
    # è·å–ç¬¬ä¸€ä¸ªè®¾å¤‡ä¿¡æ¯ï¼ˆå‡è®¾æ‰€æœ‰è®¾å¤‡ç›¸åŒï¼‰
    device = devices[0]
    device_kind = str(device.device_kind).lower()
    
    # å°è¯•æ£€æµ‹GPUå‹å·ï¼ˆåŸºäºè®¾å¤‡å±æ€§æ¨æ–­ï¼‰
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ£€æµ‹ï¼Œå®é™…ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ–¹æ³•
    if "3080" in device_kind or hasattr(device, 'memory_size'):
        # å°è¯•è·å–å†…å­˜å¤§å°
        try:
            # JAX 0.6.1å¯èƒ½ä¸ç›´æ¥æä¾›å†…å­˜ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
            gpu_name = "RTX 3080"
            memory_gb = 10.0  # é»˜è®¤å‡è®¾10GBç‰ˆæœ¬
            compute_capability = "8.6"
            config_key = "rtx3080_quad"
        except:
            gpu_name = "RTX 3080"
            memory_gb = 10.0
            compute_capability = "8.6"
            config_key = "rtx3080_quad"
    elif "3090" in device_kind:
        gpu_name = "RTX 3090"
        memory_gb = 24.0
        compute_capability = "8.6"
        config_key = "rtx3090_quad"
    elif "4090" in device_kind:
        gpu_name = "RTX 4090"
        memory_gb = 24.0
        compute_capability = "8.9"
        config_key = "rtx4090_quad"
    else:
        # é»˜è®¤é…ç½®
        gpu_name = "æœªçŸ¥GPU"
        memory_gb = 8.0  # ä¿å®ˆä¼°è®¡
        compute_capability = "8.0"
        config_key = "rtx3080_quad"  # ä½¿ç”¨è¾ƒä¸¥æ ¼çš„é™åˆ¶
    
    print(f"ğŸ” æ£€æµ‹åˆ°ç¡¬ä»¶é…ç½®:")
    print(f"   GPUå‹å·: {gpu_name}")
    print(f"   è®¾å¤‡æ•°é‡: {device_count}")
    print(f"   æ˜¾å­˜å¤§å°: {memory_gb}GB (ä¼°è®¡)")
    print(f"   è®¡ç®—èƒ½åŠ›: {compute_capability}")
    print(f"   é…ç½®é”®: {config_key}")
    
    return HardwareInfo(
        gpu_name=gpu_name,
        memory_gb=memory_gb,
        compute_capability=compute_capability,
        device_count=device_count,
        config_key=config_key
    )

def apply_hardware_constraints(hardware: HardwareInfo) -> Dict[str, Any]:
    """æ ¹æ®ç¡¬ä»¶åº”ç”¨çº¦æŸæ¡ä»¶"""
    # è·å–åŸºç¡€çº¦æŸ
    base_constraints = CONSTRAINTS.copy()
      # è·å–GPUç‰¹å®šçº¦æŸ
    gpu_constraints = base_constraints.get("gpu_specific_constraints", {})
    specific_constraints = gpu_constraints.get(hardware.config_key, {})
    
    # åº”ç”¨ç‰¹å®šçº¦æŸ
    if specific_constraints:
        print(f"ğŸ¯ åº”ç”¨{hardware.gpu_name}ç‰¹å®šçº¦æŸ:")
        for key, value in specific_constraints.items():
            print(f"   {key}: {value}")
            base_constraints[key] = value
    
    # æ ¹æ®å®é™…è®¾å¤‡æ•°é‡è°ƒæ•´
    if hardware.device_count != 4:
        print(f"âš ï¸ è®¾å¤‡æ•°é‡({hardware.device_count})ä¸é¢„æœŸ(4)ä¸ç¬¦ï¼Œè°ƒæ•´çº¦æŸ")
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è®¾å¤‡æ•°é‡ç›¸å…³çš„è°ƒæ•´é€»è¾‘
    
    return base_constraints


def main():
    """ä¸»å‡½æ•° - GPT-1.5Bç­–ç•¥æœç´¢ç³»ç»Ÿ"""
    print("ğŸ” GPT-1.5B JAXåˆ†å¸ƒå¼æ¨ç†ç­–ç•¥æœç´¢ç³»ç»Ÿ")
    print("="*80)
    
    try:
        # 1. æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ
        print("\nğŸ–¥ï¸ æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒ...")
        hardware_info = detect_hardware()  # ä½¿ç”¨å·²å®šä¹‰çš„å‡½æ•°
        
        print(f"æ£€æµ‹åˆ°ç¡¬ä»¶é…ç½®:")
        print(f"   GPUå‹å·: {hardware_info.gpu_name}")
        print(f"   å†…å­˜: {hardware_info.memory_gb}GB")
        print(f"   è®¾å¤‡æ•°é‡: {hardware_info.device_count}")
        print(f"   è®¡ç®—èƒ½åŠ›: {hardware_info.compute_capability}")
        
        # 2. åˆå§‹åŒ–ç­–ç•¥æœç´¢å™¨
        print("\nğŸš€ åˆå§‹åŒ–ç­–ç•¥æœç´¢å™¨...")
        config = GPTConfig()  # GPT-1.5Bé…ç½®
        searcher = StrategySearcher(config, hardware_info)
        
        # ä½¿ç”¨æ™ºèƒ½æœç´¢
        best_strategies = searcher.smart_search()
        
        if not best_strategies:
            print("âŒ æœªæ‰¾åˆ°é€‚åˆçš„ç­–ç•¥")
            return
        
        # åˆ†æå’Œä¿å­˜ç»“æœ
        results_file = searcher.export_results()
        print(f"\nğŸ‰ ç­–ç•¥æœç´¢å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
    except Exception as e:
        print(f"âŒ ç­–ç•¥æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()


def run_rtx3080_optimized_search():
    """è¿è¡ŒRTX 3080ä¼˜åŒ–çš„ç­–ç•¥æœç´¢"""
    print("ğŸ¯ RTX 3080ä¸“ç”¨ç­–ç•¥æœç´¢")
    print("=" * 80)
    
    try:
        # æ£€æµ‹ç¡¬ä»¶
        hardware_info = detect_hardware()
        
        if not hardware_info.config_key.startswith('rtx3080'):
            print(f"âš ï¸ å½“å‰ç¡¬ä»¶({hardware_info.gpu_name})ä¸æ˜¯RTX 3080")            
            print(f"   æ˜¯å¦ç»§ç»­ä½¿ç”¨RTX 3080ä¼˜åŒ–ç­–ç•¥? (y/n)")
            response = input().lower()
            if response != 'y':
                return
        
        # åˆ›å»ºGPTé…ç½® - é’ˆå¯¹RTX 3080å†…å­˜é™åˆ¶çš„é…ç½®
        config = GPTConfig(
            vocab_size=50257,
            n_positions=1024,  # å‡å°åºåˆ—é•¿åº¦ä»¥é€‚åº”å†…å­˜
            n_embd=1600,
            n_layer=24,  # å‡åŠå±‚æ•°ä»¥é€‚åº”å†…å­˜
            n_head=25,
            dropout=0.0,
            use_bias=True
        )
        
        print(f"ğŸ”§ RTX 3080ä¼˜åŒ–é…ç½®:")
        print(f"   å‚æ•°é‡: {estimate_model_params(config):,}")
        print(f"   åºåˆ—é•¿åº¦: {config.n_positions}")
        print(f"   å±‚æ•°: {config.n_layer}")
        
        # åˆ›å»ºæœç´¢å™¨
        searcher = StrategySearcher(config, hardware_info)
        
        # æ‰§è¡Œæ™ºèƒ½æœç´¢
        print(f"\nğŸš€ å¼€å§‹RTX 3080ä¼˜åŒ–æœç´¢...")
        results = searcher.smart_search()
        
        if results:
            # ä¿å­˜RTX 3080ä¸“ç”¨ç»“æœ
            rtx3080_results = {
                'hardware': asdict(hardware_info),
                'config': asdict(config),
                'results': [
                    {
                        'strategy': asdict(strategy),
                        'metrics': asdict(metrics)
                    }
                    for strategy, metrics in results
                ]
            }
            
            results_file = Path("rtx3080_strategy_search_results.json")
            with open(results_file, 'w') as f:
                json.dump(rtx3080_results, f, indent=2)
            
            print(f"\nğŸ’¾ RTX 3080ç»“æœå·²ä¿å­˜: {results_file}")
            
            # ç”ŸæˆRTX 3080ä¸“ç”¨ä»£ç æ¨¡æ¿
            generate_rtx3080_code_template(results[0][0] if results else None)
        
    except Exception as e:
        print(f"âŒ RTX 3080æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def estimate_model_params(config: GPTConfig) -> int:
    """ä¼°ç®—æ¨¡å‹å‚æ•°é‡"""
    embed_params = config.vocab_size * config.n_embd + config.n_positions * config.n_embd
    layer_params = (
        config.n_embd * 3 * config.n_embd +  # QKV
        config.n_embd * config.n_embd +      # attention output
        config.n_embd * 4 * config.n_embd +  # MLP up
        4 * config.n_embd * config.n_embd +  # MLP down
        4 * config.n_embd                    # layer norms
    )
    total_params = embed_params + config.n_layer * layer_params
    return total_params

def generate_rtx3080_code_template(best_strategy: Optional[ShardingStrategy]):
    """ç”ŸæˆRTX 3080ä¸“ç”¨çš„ä»£ç æ¨¡æ¿"""
    if not best_strategy:
        return
    
    template = f'''"""
RTX 3080ä¼˜åŒ–çš„GPTæ¨ç†ä»£ç æ¨¡æ¿
åŸºäºç­–ç•¥æœç´¢ç»“æœ: {best_strategy.name}
"""

import jax
import jax.numpy as jnp
from jax.sharding import Mesh, PartitionSpec, NamedSharding
from jax.experimental import mesh_utils
import flax.linen as nn
import numpy as np

# RTX 3080ä¼˜åŒ–é…ç½®
RTX3080_CONFIG = {{
    "mesh_shape": {best_strategy.mesh_shape},
    "mesh_axes": {best_strategy.mesh_axes},
    "memory_limit": "8GB",  # RTX 3080 10GBç•™2GBç»™ç³»ç»Ÿ
    "max_batch_size": 16,   # å—å†…å­˜é™åˆ¶
    "max_sequence_length": 1024
}}

def create_rtx3080_mesh():
    """åˆ›å»ºRTX 3080ä¼˜åŒ–çš„è®¾å¤‡ç½‘æ ¼"""
    devices = jax.devices()
    if len(devices) < {len(best_strategy.mesh_shape)}:
        raise ValueError(f"éœ€è¦è‡³å°‘{{len(best_strategy.mesh_shape)}}ä¸ªGPU")
    
    devices_array = np.array(devices[:{np.prod(best_strategy.mesh_shape)}]).reshape{best_strategy.mesh_shape}
    return Mesh(devices_array, axis_names={best_strategy.mesh_axes})

def create_rtx3080_shardings(mesh):
    """åˆ›å»ºRTX 3080ä¼˜åŒ–çš„åˆ†ç‰‡ç­–ç•¥"""
    return {{
        'embedding': NamedSharding(mesh, {best_strategy.embedding_spec}),
        'attention_qkv': NamedSharding(mesh, {best_strategy.attention_qkv_spec}),
        'attention_out': NamedSharding(mesh, {best_strategy.attention_out_spec}),
        'mlp_up': NamedSharding(mesh, {best_strategy.mlp_up_spec}),
        'mlp_down': NamedSharding(mesh, {best_strategy.mlp_down_spec}),
        'lm_head': NamedSharding(mesh, {best_strategy.lm_head_spec}),
        'input': NamedSharding(mesh, {best_strategy.input_spec})
    }}

# ä½¿ç”¨ç¤ºä¾‹:
# mesh = create_rtx3080_mesh()
# shardings = create_rtx3080_shardings(mesh)
# ç„¶åä½¿ç”¨è¿™äº›åˆ†ç‰‡ç­–ç•¥è¿›è¡Œæ¨ç†
'''
    
    template_file = Path("rtx3080_inference_template.py")
    with open(template_file, 'w') as f:
        f.write(template)
    
    print(f"ğŸ“„ RTX 3080ä»£ç æ¨¡æ¿å·²ç”Ÿæˆ: {template_file}")
