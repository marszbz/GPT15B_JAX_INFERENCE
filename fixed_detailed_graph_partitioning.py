#!/usr/bin/env python3
"""
å›¾åˆ†å‰²ç­–ç•¥è¯¦ç»†é…ç½®å’Œå®ç°
å±•ç¤ºå¦‚ä½•åœ¨JAXä¸­è¿›è¡Œç²¾ç¡®çš„å›¾åˆ†å‰²å’Œå‚æ•°åˆ†ç‰‡
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from functools import partial

# è®¾ç½®JAXç¯å¢ƒ
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
# æ¸…ç†å¯èƒ½å­˜åœ¨çš„æœ‰é—®é¢˜çš„XLA_FLAGS
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

try:
    import jax
    import jax.numpy as jnp
    from jax import random, devices
    from jax.sharding import Mesh, PartitionSpec, NamedSharding
    from jax.experimental import mesh_utils
    import flax.linen as nn
    import numpy as np
    print(f"âœ… JAX {jax.__version__} å›¾åˆ†å‰²æ¨¡å¼åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class GraphPartitionConfig:
    """å›¾åˆ†å‰²é…ç½®"""
    num_devices: int = 4
    mesh_shape: Tuple[int, int] = (2, 2)
    data_axis: str = 'data'
    model_axis: str = 'model'
    parameter_threshold: int = 512  # å¤§äºæ­¤é˜ˆå€¼çš„å‚æ•°è¿›è¡Œåˆ†ç‰‡
    
    # åˆ†ç‰‡ç­–ç•¥é…ç½®
    embedding_sharding: str = 'vocab'      # vocabç»´åº¦åˆ†ç‰‡
    attention_sharding: str = 'heads'      # æ³¨æ„åŠ›å¤´åˆ†ç‰‡
    mlp_sharding: str = 'hidden'           # éšè—å±‚åˆ†ç‰‡
    output_sharding: str = 'vocab'         # è¾“å‡ºå±‚åˆ†ç‰‡

class DetailedGraphPartitioner:
    """è¯¦ç»†çš„å›¾åˆ†å‰²å®ç°"""
    
    def __init__(self, config: GraphPartitionConfig):
        self.config = config
        self.devices = jax.devices()
        if len(self.devices) > config.num_devices:
            self.devices = self.devices[:config.num_devices]
        self.mesh = None
        self.sharding_specs = {}
        
    def create_device_mesh(self):
        """åˆ›å»ºè®¾å¤‡ç½‘æ ¼"""
        print(f"\nğŸ”§ åˆ›å»ºè®¾å¤‡ç½‘æ ¼")
        print("-" * 30)
        
        print(f"æ£€æµ‹åˆ°è®¾å¤‡æ•°é‡: {len(self.devices)}")
        for i, device in enumerate(self.devices):
            print(f"   è®¾å¤‡ {i}: {device}")
        
        try:
            if len(self.devices) >= 4:
                # ä½¿ç”¨å®é™…å¯ç”¨çš„è®¾å¤‡æ•°é‡
                actual_devices = self.devices[:4]
                print(f"ä½¿ç”¨è®¾å¤‡æ•°é‡: {len(actual_devices)}")
                
                # æ‰‹åŠ¨åˆ›å»º2x2è®¾å¤‡æ•°ç»„
                mesh_devices = np.array(actual_devices).reshape(2, 2)
                self.mesh = Mesh(mesh_devices, axis_names=(self.config.data_axis, self.config.model_axis))
                print(f"âœ… åˆ›å»º (2,2) mesh")
                
            elif len(self.devices) == 2:
                # 2x1ç½‘æ ¼ç”¨äº2ä¸ªGPU
                mesh_devices = np.array(self.devices).reshape(2, 1)
                self.mesh = Mesh(mesh_devices, axis_names=(self.config.data_axis, self.config.model_axis))
                print(f"âœ… åˆ›å»º (2,1) mesh")
                
            elif len(self.devices) == 1:
                # å•è®¾å¤‡æ¨¡å¼
                mesh_devices = np.array(self.devices).reshape(1, 1)
                self.mesh = Mesh(mesh_devices, axis_names=('data',))
                print(f"âœ… åˆ›å»º (1,1) mesh (å•è®¾å¤‡æ¨¡å¼)")
                # æ›´æ–°é…ç½®ä»¥é€‚åº”å•è®¾å¤‡
                self.config.model_axis = None
                
            else:
                print(f"âš ï¸ è®¾å¤‡æ•°é‡({len(self.devices)})ä¸æ”¯æŒï¼Œæ— æ³•åˆ›å»ºmesh")
                return False
                
            print(f"   ç½‘æ ¼å½¢çŠ¶: {self.mesh.shape}")
            print(f"   è½´åç§°: {self.mesh.axis_names}")
            
            # æ‰“å°è®¾å¤‡åˆ†é…è¯¦æƒ…
            print(f"   è®¾å¤‡åˆ†å¸ƒ:")
            if len(self.mesh.shape) == 2:
                for i in range(self.mesh.shape[0]):
                    for j in range(self.mesh.shape[1]):
                        device = self.mesh.devices[i, j]
                        print(f"     ä½ç½®[{i},{j}]: {device}")
            else:
                for i, device in enumerate(self.mesh.devices.flat):
                    print(f"     ä½ç½®[{i}]: {device}")
                
            return True
            
        except Exception as e:
            print(f"âŒ ç½‘æ ¼åˆ›å»ºå¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return False
    
    def define_sharding_strategies(self):
        """å®šä¹‰è¯¦ç»†çš„åˆ†ç‰‡ç­–ç•¥"""
        print(f"\nğŸ“‹ å®šä¹‰åˆ†ç‰‡ç­–ç•¥")
        print("-" * 30)
        
        if not self.mesh:
            print("âš ï¸ æœªåˆ›å»ºmeshï¼Œæ— æ³•å®šä¹‰åˆ†ç‰‡ç­–ç•¥")
            return
        
        # ä¸ºä¸åŒç»„ä»¶å®šä¹‰åˆ†ç‰‡è§„åˆ™
        strategies = {
            # åµŒå…¥å±‚åˆ†ç‰‡ç­–ç•¥
            'embedding': {
                'weight': PartitionSpec(self.config.model_axis, None),  # è¯æ±‡è¡¨ç»´åº¦åˆ†ç‰‡
                'description': 'è¯æ±‡è¡¨åœ¨modelè½´åˆ†ç‰‡ï¼Œé™ä½å•è®¾å¤‡å†…å­˜å‹åŠ›'
            },
            
            # æ³¨æ„åŠ›å±‚åˆ†ç‰‡ç­–ç•¥
            'attention': {
                'qkv_weight': PartitionSpec(None, self.config.model_axis),  # æ³¨æ„åŠ›å¤´åˆ†ç‰‡
                'qkv_bias': PartitionSpec(self.config.model_axis),
                'output_weight': PartitionSpec(self.config.model_axis, None),
                'output_bias': PartitionSpec(None),  # å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡
                'description': 'æ³¨æ„åŠ›å¤´åœ¨modelè½´åˆ†ç‰‡ï¼Œå®ç°å¤´å¹¶è¡Œ'
            },
            
            # MLPå±‚åˆ†ç‰‡ç­–ç•¥
            'mlp': {
                'dense1_weight': PartitionSpec(None, self.config.model_axis),  # éšè—å±‚åˆ†ç‰‡
                'dense1_bias': PartitionSpec(self.config.model_axis),
                'dense2_weight': PartitionSpec(self.config.model_axis, None),
                'dense2_bias': PartitionSpec(None),
                'description': 'MLPéšè—å±‚åœ¨modelè½´åˆ†ç‰‡ï¼Œå¹³è¡¡è®¡ç®—è´Ÿè½½'
            },
            
            # LayerNormåˆ†ç‰‡ç­–ç•¥
            'layernorm': {
                'scale': PartitionSpec(None),  # å¤åˆ¶
                'bias': PartitionSpec(None),   # å¤åˆ¶
                'description': 'LayerNormå‚æ•°å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡'
            },
            
            # è¾“å‡ºå±‚åˆ†ç‰‡ç­–ç•¥
            'output': {
                'weight': PartitionSpec(self.config.model_axis, None),  # è¯æ±‡è¡¨åˆ†ç‰‡
                'description': 'è¾“å‡ºå±‚è¯æ±‡è¡¨ç»´åº¦åˆ†ç‰‡'
            },
            
            # æ•°æ®åˆ†ç‰‡ç­–ç•¥
            'data': {
                'input_ids': PartitionSpec(self.config.data_axis, None),     # batchåˆ†ç‰‡
                'attention_mask': PartitionSpec(self.config.data_axis, None, None),
                'logits': PartitionSpec(self.config.data_axis, None, self.config.model_axis),
                'description': 'è¾“å…¥æ•°æ®åœ¨dataè½´åˆ†ç‰‡ï¼Œå®ç°æ•°æ®å¹¶è¡Œ'
            }
        }
        
        # å¤„ç†å•è®¾å¤‡æƒ…å†µ
        if self.config.model_axis is None:
            for component, specs in strategies.items():
                for param_name, spec in specs.items():
                    if param_name != 'description' and isinstance(spec, PartitionSpec):
                        # å°†modelè½´åˆ†ç‰‡æ”¹ä¸ºæ— åˆ†ç‰‡
                        new_spec_args = []
                        for axis in spec:
                            if axis == 'model':
                                new_spec_args.append(None)
                            else:
                                new_spec_args.append(axis)
                        strategies[component][param_name] = PartitionSpec(*new_spec_args)
        
        self.sharding_specs = strategies
        
        # æ‰“å°åˆ†ç‰‡ç­–ç•¥
        for component, specs in strategies.items():
            print(f"\nğŸ” {component.upper()} åˆ†ç‰‡ç­–ç•¥:")
            print(f"   æè¿°: {specs['description']}")
            for param_name, spec in specs.items():
                if param_name != 'description':
                    print(f"   {param_name}: {spec}")
    
    def demonstrate_xla_optimizations(self):
        """æ¼”ç¤ºXLAç¼–è¯‘å™¨ä¼˜åŒ–"""
        print(f"\nâš¡ XLAç¼–è¯‘å™¨ä¼˜åŒ–æ¼”ç¤º")
        print("-" * 30)
        
        if not self.mesh:
            print("âš ï¸ æœªåˆ›å»ºmeshï¼Œæ— æ³•æ¼”ç¤ºXLAä¼˜åŒ–")
            return
        
        # åˆ›å»ºç®€å•çš„è®¡ç®—å›¾
        def simple_computation(x, w):
            """ç®€å•çš„çŸ©é˜µä¹˜æ³• + æ¿€æ´»å‡½æ•°"""
            return jax.nn.gelu(jnp.dot(x, w))
        
        # JITç¼–è¯‘
        jit_computation = jax.jit(simple_computation)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        key = jax.random.PRNGKey(42)
        x = jax.random.normal(key, (32, 1600))  # batch_size x hidden_dim
        w = jax.random.normal(key, (1600, 4800))  # hidden_dim x 3*hidden_dim
        
        print(f"ğŸ“Š è®¡ç®—å›¾åˆ†æ:")
        print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"   æƒé‡å½¢çŠ¶: {w.shape}")
        print(f"   è®¡ç®—ç±»å‹: çŸ©é˜µä¹˜æ³• + GELUæ¿€æ´»")
        
        # åˆ†ç‰‡æ•°æ®
        with self.mesh:
            if self.config.model_axis:
                x_sharding = NamedSharding(self.mesh, PartitionSpec(self.config.data_axis, None))
                w_sharding = NamedSharding(self.mesh, PartitionSpec(None, self.config.model_axis))
            else:
                x_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
                w_sharding = NamedSharding(self.mesh, PartitionSpec(None))
            
            x_sharded = jax.device_put(x, x_sharding)
            w_sharded = jax.device_put(w, w_sharding)
            
            print(f"\nğŸ”§ XLAä¼˜åŒ–è¿‡ç¨‹:")
            print(f"   1. å›¾æ„å»º: è§£æPythonä»£ç ä¸ºXLA HLO")
            print(f"   2. ä¼˜åŒ–å™¨: åº”ç”¨èåˆã€é‡æ’åˆ—ç­‰ä¼˜åŒ–")
            print(f"   3. å¹¶è¡ŒåŒ–: æ ¹æ®åˆ†ç‰‡ç­–ç•¥åˆ†å¸ƒè®¡ç®—")
            print(f"   4. ä»£ç ç”Ÿæˆ: ç”Ÿæˆé«˜æ•ˆçš„GPU kernel")
            
            # é¢„çƒ­JITç¼–è¯‘
            print(f"\nğŸš€ JITç¼–è¯‘é¢„çƒ­...")
            for i in range(3):
                result = jit_computation(x_sharded, w_sharded)
                jax.block_until_ready(result)
                print(f"   é¢„çƒ­ {i+1}/3 å®Œæˆ")
            
            # æ€§èƒ½æµ‹è¯•
            print(f"\nğŸ“ˆ æ€§èƒ½æµ‹è¯•:")
            times = []
            for i in range(5):
                start_time = time.time()
                result = jit_computation(x_sharded, w_sharded)
                jax.block_until_ready(result)
                end_time = time.time()
                times.append(end_time - start_time)
                print(f"   è¿è¡Œ {i+1}: {(end_time - start_time)*1000:.2f}ms")
            
            avg_time = np.mean(times)
            throughput = (32 * 1600 * 4800) / avg_time / 1e9  # GFLOPS
            
            print(f"\nğŸ¯ XLAä¼˜åŒ–æ•ˆæœ:")
            print(f"   å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time*1000:.2f}ms")
            print(f"   è®¡ç®—ååé‡: {throughput:.2f} GFLOPS")
            print(f"   è¾“å‡ºå½¢çŠ¶: {result.shape}")
            print(f"   å†…å­˜æ•ˆç‡: å‚æ•°è‡ªåŠ¨åˆ†ç‰‡åˆ°å¤šGPU")
            
        return {
            'avg_time_ms': avg_time * 1000,
            'throughput_gflops': throughput,
            'output_shape': result.shape
        }
    
    def analyze_parameter_distribution(self, model_config=None):
        """åˆ†æå‚æ•°åˆ†å¸ƒ"""
        print(f"\nğŸ“Š å‚æ•°åˆ†å¸ƒåˆ†æ")
        print("-" * 30)
        
        # æ¨¡æ‹ŸGPT-1.5Bå‚æ•°
        vocab_size = 50257
        n_embd = 1600
        n_layer = 48
        n_head = 25
        
        # åˆ†ææ¯ä¸ªè®¾å¤‡çš„å‚æ•°åˆ†å¸ƒ
        device_params = {f'device_{i}': 0 for i in range(len(self.devices))}
        total_params = 0
        
        print(f"ğŸ“ˆ å‚æ•°åˆ†å¸ƒç»Ÿè®¡:")
        
        # è®¡ç®—åµŒå…¥å±‚å‚æ•°
        embed_params = vocab_size * n_embd + 2048 * n_embd
        if len(self.devices) > 1 and self.config.model_axis:
            sharded_embed = embed_params // len(self.devices)  # è¯æ±‡è¡¨åˆ†ç‰‡
        else:
            sharded_embed = embed_params  # å•è®¾å¤‡æˆ–æ— åˆ†ç‰‡
            
        for i in range(len(self.devices)):
            if len(self.devices) > 1 and self.config.model_axis:
                device_params[f'device_{i}'] += sharded_embed
            else:
                device_params[f'device_0'] += embed_params
                break
        total_params += embed_params
        print(f"   åµŒå…¥å±‚: {embed_params:,} å‚æ•° â†’ æ¯è®¾å¤‡: {sharded_embed:,}")
        
        # è®¡ç®—Transformerå±‚å‚æ•°
        layer_param_count = (
            n_embd * 3 * n_embd +  # QKV
            n_embd * n_embd +      # attention output
            n_embd * 4 * n_embd +  # MLP dense1
            4 * n_embd * n_embd +  # MLP dense2
            4 * n_embd             # biases + layernorms
        )
        
        transformer_params = n_layer * layer_param_count
        if len(self.devices) > 1 and self.config.model_axis:
            sharded_transformer = transformer_params // len(self.devices)
        else:
            sharded_transformer = transformer_params
            
        for i in range(len(self.devices)):
            if len(self.devices) > 1 and self.config.model_axis:
                device_params[f'device_{i}'] += sharded_transformer
            else:
                device_params[f'device_0'] += transformer_params
                break
        total_params += transformer_params
        print(f"   Transformer: {transformer_params:,} å‚æ•° â†’ æ¯è®¾å¤‡: {sharded_transformer:,}")
        
        # è®¡ç®—è¾“å‡ºå±‚å‚æ•°
        output_params = n_embd * vocab_size
        if len(self.devices) > 1 and self.config.model_axis:
            sharded_output = output_params // len(self.devices)
        else:
            sharded_output = output_params
            
        for i in range(len(self.devices)):
            if len(self.devices) > 1 and self.config.model_axis:
                device_params[f'device_{i}'] += sharded_output
            else:
                device_params[f'device_0'] += output_params
                break
        total_params += output_params
        print(f"   è¾“å‡ºå±‚: {output_params:,} å‚æ•° â†’ æ¯è®¾å¤‡: {sharded_output:,}")
        
        print(f"\nğŸ“Š è®¾å¤‡è´Ÿè½½å¹³è¡¡:")
        for device, count in device_params.items():
            if count > 0:  # åªæ˜¾ç¤ºæœ‰å‚æ•°çš„è®¾å¤‡
                percentage = (count / total_params) * 100
                memory_gb = count * 4 / (1024**3)  # float32
                print(f"   {device}: {count:,} å‚æ•° ({percentage:.1f}%) â‰ˆ {memory_gb:.2f}GB")
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e9:.2f}B)")
        
        active_devices = sum(1 for count in device_params.values() if count > 0)
        if active_devices > 1:
            print(f"   å¹³å‡æ¯è®¾å¤‡: {total_params//active_devices:,}")
            min_params = min(count for count in device_params.values() if count > 0)
            max_params = max(count for count in device_params.values() if count > 0)
            print(f"   è´Ÿè½½å‡è¡¡åº¦: {(min_params/max_params)*100:.1f}%")
        else:
            print(f"   å•è®¾å¤‡æ¨¡å¼: æ‰€æœ‰å‚æ•°åœ¨ä¸€ä¸ªè®¾å¤‡ä¸Š")
        
        return {
            'total_params': total_params,
            'device_distribution': device_params,
            'memory_per_device_gb': max(device_params.values()) * 4 / (1024**3)
        }
    
    def demonstrate_sharding_execution(self):
        """æ¼”ç¤ºåˆ†ç‰‡æ‰§è¡Œè¿‡ç¨‹"""
        print(f"\nğŸš€ åˆ†ç‰‡æ‰§è¡Œæ¼”ç¤º")
        print("-" * 30)
        
        if not self.mesh:
            print("âš ï¸ æœªåˆ›å»ºmeshï¼Œæ— æ³•æ¼”ç¤ºåˆ†ç‰‡")
            return None
            
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ¨¡å‹å‚æ•°
        key = jax.random.PRNGKey(42)
        
        # æ¨¡æ‹Ÿå‚æ•°å­—å…¸
        params = {
            'embedding': {
                'weight': jax.random.normal(key, (50257, 1600))  # è¯æ±‡è¡¨ x åµŒå…¥ç»´åº¦
            },
            'attention': {
                'qkv_weight': jax.random.normal(key, (1600, 4800)),  # 3 * n_embd
                'output_weight': jax.random.normal(key, (1600, 1600))
            },
            'mlp': {
                'dense1_weight': jax.random.normal(key, (1600, 6400)),  # 4 * n_embd
                'dense2_weight': jax.random.normal(key, (6400, 1600))
            }
        }
        
        print(f"ğŸ“¦ åŸå§‹å‚æ•°å½¢çŠ¶:")
        for component, comp_params in params.items():
            for param_name, param in comp_params.items():
                print(f"   {component}.{param_name}: {param.shape}")
        
        # åº”ç”¨åˆ†ç‰‡
        with self.mesh:
            sharded_params = {}
            
            for component, comp_params in params.items():
                sharded_params[component] = {}
                
                for param_name, param in comp_params.items():
                    # æ ¹æ®ç»„ä»¶ç±»å‹é€‰æ‹©åˆ†ç‰‡ç­–ç•¥
                    if component == 'embedding' and 'weight' in param_name:
                        if self.config.model_axis:
                            spec = PartitionSpec(self.config.model_axis, None)  # è¯æ±‡è¡¨åˆ†ç‰‡
                        else:
                            spec = PartitionSpec()  # å•è®¾å¤‡ä¸åˆ†ç‰‡
                    elif component == 'attention':
                        if 'qkv' in param_name:
                            if self.config.model_axis:
                                spec = PartitionSpec(None, self.config.model_axis)  # æ³¨æ„åŠ›å¤´åˆ†ç‰‡
                            else:
                                spec = PartitionSpec()
                        else:
                            if self.config.model_axis:
                                spec = PartitionSpec(self.config.model_axis, None)
                            else:
                                spec = PartitionSpec()
                    elif component == 'mlp':
                        if 'dense1' in param_name:
                            if self.config.model_axis:
                                spec = PartitionSpec(None, self.config.model_axis)  # éšè—å±‚åˆ†ç‰‡
                            else:
                                spec = PartitionSpec()
                        else:
                            if self.config.model_axis:
                                spec = PartitionSpec(self.config.model_axis, None)
                            else:
                                spec = PartitionSpec()
                    else:
                        spec = PartitionSpec()  # ä¸åˆ†ç‰‡
                    
                    # åˆ›å»ºåˆ†ç‰‡
                    sharding = NamedSharding(self.mesh, spec)
                    sharded_param = jax.device_put(param, sharding)
                    sharded_params[component][param_name] = sharded_param
                    
                    print(f"   âœ… {component}.{param_name}: {spec} â†’ å·²åˆ†ç‰‡")
        
        print(f"\nğŸ¯ åˆ†ç‰‡æ‰§è¡Œå®Œæˆ!")
        return sharded_params

def main():
    """ä¸»å‡½æ•° - è¯¦ç»†å›¾åˆ†å‰²åˆ†æ"""
    print(f"ğŸ” è¯¦ç»†å›¾åˆ†å‰²ç­–ç•¥åˆ†æ")
    print("=" * 50)
    
    # åˆ›å»ºé…ç½®
    available_devices = len(jax.devices())
    config = GraphPartitionConfig(
        num_devices=available_devices,
        mesh_shape=(2, 2) if available_devices >= 4 else (2, 1)
    )
    
    partitioner = DetailedGraphPartitioner(config)
    
    try:
        # 1. åˆ›å»ºè®¾å¤‡ç½‘æ ¼
        mesh_success = partitioner.create_device_mesh()
        
        if mesh_success:
            # 2. å®šä¹‰åˆ†ç‰‡ç­–ç•¥
            partitioner.define_sharding_strategies()
            
            # 3. æ¼”ç¤ºXLAä¼˜åŒ–
            xla_results = partitioner.demonstrate_xla_optimizations()
            
            # 4. åˆ†æå‚æ•°åˆ†å¸ƒ
            param_analysis = partitioner.analyze_parameter_distribution()
            
            # 5. æ¼”ç¤ºåˆ†ç‰‡æ‰§è¡Œ
            sharded_params = partitioner.demonstrate_sharding_execution()
            
            # 6. ä¿å­˜ç»“æœ
            results = {
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'config': {
                    'num_devices': config.num_devices,
                    'mesh_shape': config.mesh_shape,
                    'data_axis': config.data_axis,
                    'model_axis': config.model_axis
                },
                'mesh_info': {
                    'shape': list(partitioner.mesh.shape),
                    'axis_names': list(partitioner.mesh.axis_names),
                    'device_count': len(partitioner.devices)
                },
                'xla_optimization': xla_results,
                'parameter_analysis': param_analysis,
                'sharding_successful': sharded_params is not None
            }
            
            results_file = Path("detailed_graph_partition_analysis.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜: {results_file}")
            
            # 7. æ€»ç»“
            print(f"\nğŸ¯ å›¾åˆ†å‰²åˆ†ææ€»ç»“")
            print("=" * 40)
            print(f"ğŸ“Š å…³é”®æŒ‡æ ‡:")
            print(f"   è®¾å¤‡æ•°é‡: {len(partitioner.devices)}")
            print(f"   ç½‘æ ¼é…ç½®: {partitioner.mesh.shape}")
            print(f"   æ€»å‚æ•°é‡: {param_analysis['total_params']/1e9:.2f}B")
            print(f"   æ¯è®¾å¤‡å†…å­˜: {param_analysis['memory_per_device_gb']:.2f}GB")
            
            if xla_results:
                print(f"   XLAååé‡: {xla_results['throughput_gflops']:.1f} GFLOPS")
                print(f"   å¹³å‡å»¶è¿Ÿ: {xla_results['avg_time_ms']:.2f}ms")
            
            print(f"\nğŸ’¡ XLAç¼–è¯‘å™¨ä¼˜åŒ–ç‰¹æ€§:")
            print(f"   1. è‡ªåŠ¨å›¾èåˆ: å°†å°æ“ä½œåˆå¹¶ä¸ºå¤§kernel")
            print(f"   2. å†…å­˜ä¼˜åŒ–: å‡å°‘ä¸­é—´ç»“æœçš„å†…å­˜å ç”¨")
            print(f"   3. å¹¶è¡Œä¼˜åŒ–: æ ¹æ®meshè‡ªåŠ¨åˆ†å¸ƒè®¡ç®—")
            print(f"   4. å¿«é€Ÿæ•°å­¦: ä½¿ç”¨è¿‘ä¼¼ä½†æ›´å¿«çš„æ•°å­¦å‡½æ•°")
            
            print(f"\nğŸ”§ å›¾åˆ†å‰²æ ¸å¿ƒæŠ€æœ¯:")
            print(f"   1. å‚æ•°åˆ†ç‰‡: å¤§æƒé‡çŸ©é˜µåˆ†å¸ƒåˆ°å¤šGPU")
            print(f"   2. æ•°æ®å¹¶è¡Œ: batchç»´åº¦åˆ†ç‰‡åˆ°ä¸åŒè®¾å¤‡")
            print(f"   3. æ¨¡å‹å¹¶è¡Œ: æ³¨æ„åŠ›å¤´/MLPåˆ†ç‰‡")
            print(f"   4. æ··åˆå¹¶è¡Œ: æ•°æ®+æ¨¡å‹å¹¶è¡Œç»“åˆ")
            
        else:
            print(f"âŒ æ— æ³•åˆ›å»ºè®¾å¤‡ç½‘æ ¼ï¼Œè¯·æ£€æŸ¥GPUè®¾å¤‡æ•°é‡")
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
