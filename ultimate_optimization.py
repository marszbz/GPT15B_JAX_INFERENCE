#!/usr/bin/env python3
"""
ç»ˆæä¼˜åŒ–GPT-1.5B JAXæ¨ç†ç³»ç»Ÿ
åŸºäºæˆåŠŸçš„å¤§å‹æ¨¡å‹æµ‹è¯•ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from functools import partial

# è®¾ç½®JAXç¯å¢ƒï¼ˆå¿…é¡»åœ¨å¯¼å…¥JAXä¹‹å‰ï¼‰
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.9'  # å¢åŠ å†…å­˜ä½¿ç”¨
os.environ['JAX_ENABLE_X64'] = 'false'  # ä¿æŒFP32ç²¾åº¦
# ä¼˜åŒ–XLAç¼–è¯‘
os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'

# å¯¼å…¥JAXç›¸å…³åŒ…
try:
    import jax
    import jax.numpy as jnp
    from jax import random, pmap, devices, device_count
    from jax.sharding import Mesh, PartitionSpec, NamedSharding
    from jax.experimental import mesh_utils
    import flax.linen as nn
    from flax import jax_utils
    import numpy as np
    print(f"âœ… JAX {jax.__version__} åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class UltimateGPTConfig:
    """ç»ˆæä¼˜åŒ–GPTé…ç½®"""
    vocab_size: int = 50257
    n_positions: int = 2048  # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦
    n_embd: int = 1600
    n_layer: int = 48
    n_head: int = 25
    dropout: float = 0.0  # æ¨ç†æ—¶å…³é—­dropout
    use_bias: bool = True
    
    # æ–°å¢ä¼˜åŒ–å‚æ•°
    use_flash_attention: bool = True
    use_mixed_precision: bool = False  # æš‚æ—¶ä¿æŒFP32
    batch_size_multiplier: int = 2  # æ‰¹æ¬¡å¤§å°å€æ•°
    
    def get_param_count(self) -> int:
        """è®¡ç®—å‚æ•°é‡"""
        embed_params = self.vocab_size * self.n_embd + self.n_positions * self.n_embd
        layer_params = (4 * self.n_embd * self.n_embd + 8 * self.n_embd * self.n_embd + 4 * self.n_embd)
        total_params = embed_params + self.n_layer * layer_params + self.vocab_size * self.n_embd
        return total_params

class OptimizedMultiHeadAttention(nn.Module):
    """ä¼˜åŒ–çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    config: UltimateGPTConfig
    
    def setup(self):
        self.c_attn = nn.Dense(3 * self.config.n_embd, use_bias=self.config.use_bias)
        self.c_proj = nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)
        self.n_head = self.config.n_head
        self.n_embd = self.config.n_embd
        self.head_dim = self.n_embd // self.n_head
        
    def __call__(self, x, mask=None, training=False):
        B, T, C = x.shape
        
        # å•æ¬¡QKVæŠ•å½±ï¼Œç„¶ååˆ†å‰²
        qkv = self.c_attn(x)
        qkv = qkv.reshape(B, T, 3, self.n_head, self.head_dim)
        q, k, v = jnp.split(qkv, 3, axis=2)
        
        # é‡æ’ç»´åº¦: (B, T, 1, nh, hd) -> (B, nh, T, hd)
        q = q.squeeze(2).transpose(0, 2, 1, 3)
        k = k.squeeze(2).transpose(0, 2, 1, 3)
        v = v.squeeze(2).transpose(0, 2, 1, 3)
        
        # ä¼˜åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        if self.config.use_flash_attention:
            # ç®€åŒ–çš„flash attentioné£æ ¼è®¡ç®—
            scale = 1.0 / jnp.sqrt(self.head_dim)
            
            # åˆ†å—è®¡ç®—æ³¨æ„åŠ›ä»¥èŠ‚çœå†…å­˜
            chunk_size = min(256, T)
            outputs = []
            
            for i in range(0, T, chunk_size):
                end_i = min(i + chunk_size, T)
                q_chunk = q[:, :, i:end_i, :]
                
                # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
                scores = jnp.einsum('bnid,bnjd->bnij', q_chunk, k) * scale
                
                # åº”ç”¨å› æœæ©ç 
                if mask is not None:
                    mask_chunk = mask[:, :, i:end_i, :]
                    scores = jnp.where(mask_chunk, scores, -jnp.inf)
                
                # æ³¨æ„åŠ›æƒé‡å’Œè¾“å‡º
                attn_weights = jax.nn.softmax(scores, axis=-1)
                output_chunk = jnp.einsum('bnij,bnjd->bnid', attn_weights, v)
                outputs.append(output_chunk)
            
            y = jnp.concatenate(outputs, axis=2)
        else:
            # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
            att = (q @ k.transpose(0, 1, 3, 2)) * (1.0 / jnp.sqrt(self.head_dim))
            if mask is not None:
                att = jnp.where(mask, att, -jnp.inf)
            att = jax.nn.softmax(att, axis=-1)
            y = att @ v
        
        # é‡æ–°æ•´ç†è¾“å‡º
        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)
        return self.c_proj(y)

class OptimizedMLP(nn.Module):
    """ä¼˜åŒ–çš„MLP"""
    config: UltimateGPTConfig
    
    def setup(self):
        self.c_fc = nn.Dense(4 * self.config.n_embd, use_bias=self.config.use_bias)
        self.c_proj = nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)
    
    def __call__(self, x, training=False):
        # ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°
        x = self.c_fc(x)
        x = jax.nn.gelu(x, approximate=True)  # è¿‘ä¼¼GELUæ›´å¿«
        x = self.c_proj(x)
        return x

class OptimizedTransformerBlock(nn.Module):
    """ä¼˜åŒ–çš„Transformerå—"""
    config: UltimateGPTConfig
    
    def setup(self):
        self.ln_1 = nn.LayerNorm()
        self.attn = OptimizedMultiHeadAttention(self.config)
        self.ln_2 = nn.LayerNorm()
        self.mlp = OptimizedMLP(self.config)
    
    def __call__(self, x, mask=None, training=False):
        # Pre-normæ¶æ„
        x = x + self.attn(self.ln_1(x), mask, training)
        x = x + self.mlp(self.ln_2(x), training)
        return x

class UltimateGPT(nn.Module):
    """ç»ˆæä¼˜åŒ–GPTæ¨¡å‹"""
    config: UltimateGPTConfig
    
    def setup(self):
        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)
        self.wpe = nn.Embed(self.config.n_positions, self.config.n_embd)
        self.h = [OptimizedTransformerBlock(self.config) for _ in range(self.config.n_layer)]
        self.ln_f = nn.LayerNorm()
        
        # é¢„è®¡ç®—ä½ç½®ç¼–ç 
        self.pos_cache = jnp.arange(self.config.n_positions)
    
    def __call__(self, input_ids, training=False):
        B, T = input_ids.shape
        
        # Tokenå’Œä½ç½®åµŒå…¥
        token_embed = self.wte(input_ids)
        pos_ids = self.pos_cache[:T][None, :]
        pos_embed = self.wpe(pos_ids)
        
        x = token_embed + pos_embed
        
        # é¢„è®¡ç®—å› æœæ©ç 
        mask = jnp.tril(jnp.ones((T, T)))[None, None, :, :]
        mask = mask.astype(jnp.bool_)
        
        # Transformerå±‚
        for block in self.h:
            x = block(x, mask, training)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        x = self.ln_f(x)
        
        # è¯­è¨€æ¨¡å‹å¤´ï¼ˆé‡ç”¨è¯åµŒå…¥æƒé‡ï¼‰
        logits = x @ self.wte.embedding.T
        
        return logits

class UltimateInferenceEngine:
    """ç»ˆææ¨ç†å¼•æ“"""
    
    def __init__(self, config: UltimateGPTConfig):
        self.config = config
        self.devices = jax.devices()
        
        print(f"ğŸš€ åˆå§‹åŒ–ç»ˆæGPTæ¨ç†å¼•æ“")
        print(f"   è®¾å¤‡æ•°é‡: {len(self.devices)}")
        print(f"   é…ç½®: {config.n_layer}å±‚, {config.n_embd}ç»´åº¦, {config.n_head}å¤´")
        
        # åˆ›å»ºè®¾å¤‡mesh
        self.mesh = self._create_mesh()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = UltimateGPT(config)
        self._init_parameters()
        
        # ç¼–è¯‘ä¼˜åŒ–å‡½æ•°
        self._compile_functions()
    
    def _create_mesh(self):
        """åˆ›å»ºä¼˜åŒ–çš„è®¾å¤‡mesh"""
        num_devices = len(self.devices)
        if num_devices >= 4:
            # 2x2 mesh for optimal parallelization
            mesh_devices = mesh_utils.create_device_mesh((2, 2))
            mesh = Mesh(mesh_devices, axis_names=('data', 'model'))
            print(f"   Mesh: 2x2 ('data', 'model')")
        elif num_devices == 2:
            mesh_devices = mesh_utils.create_device_mesh((2, 1))
            mesh = Mesh(mesh_devices, axis_names=('data', 'model'))
            print(f"   Mesh: 2x1 ('data', 'model')")
        else:
            mesh = None
            print(f"   Mesh: å•è®¾å¤‡æ¨¡å¼")
        
        return mesh
    
    def _init_parameters(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        print("ğŸ”„ åˆå§‹åŒ–æ¨¡å‹å‚æ•°...")
        
        key = jax.random.PRNGKey(42)
        dummy_input = jnp.ones((1, 64), dtype=jnp.int32)
        
        start_time = time.time()
        self.params = self.model.init(key, dummy_input, training=False)
        init_time = time.time() - start_time
        
        param_count = sum(x.size for x in jax.tree_util.tree_leaves(self.params))
        param_memory = param_count * 4 / (1024**3)
        
        print(f"   å‚æ•°é‡: {param_count:,} ({param_count/1e6:.1f}M)")
        print(f"   å†…å­˜ä½¿ç”¨: {param_memory:.2f} GB")
        print(f"   åˆå§‹åŒ–æ—¶é—´: {init_time:.2f}s")
    
    def _compile_functions(self):
        """ç¼–è¯‘ä¼˜åŒ–å‡½æ•°"""
        print("âš¡ ç¼–è¯‘ä¼˜åŒ–å‡½æ•°...")
        
        if self.mesh:
            # åˆ†ç‰‡æ¨ç†å‡½æ•°
            input_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
            
            @partial(jax.jit, in_shardings=(None, input_sharding), out_shardings=input_sharding)
            def sharded_forward(params, input_ids):
                return self.model.apply(params, input_ids, training=False)
            
            self.forward_fn = sharded_forward
            print("   åˆ†ç‰‡æ¨ç†å‡½æ•°å·²ç¼–è¯‘")
        else:
            # å•è®¾å¤‡æ¨ç†å‡½æ•°
            @jax.jit
            def single_forward(params, input_ids):
                return self.model.apply(params, input_ids, training=False)
            
            self.forward_fn = single_forward
            print("   å•è®¾å¤‡æ¨ç†å‡½æ•°å·²ç¼–è¯‘")
    
    def benchmark_ultimate_performance(self, batch_size=None, seq_len=512, num_runs=10):
        """ç»ˆææ€§èƒ½åŸºå‡†æµ‹è¯•"""
        if batch_size is None:
            batch_size = len(self.devices) * self.config.batch_size_multiplier
        
        print(f"\nğŸ† ç»ˆææ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   åºåˆ—é•¿åº¦: {seq_len}")
        print(f"   æ€»tokens: {batch_size * seq_len:,}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        key = jax.random.PRNGKey(42)
        input_ids = jax.random.randint(key, (batch_size, seq_len), 0, self.config.vocab_size)
        
        if self.mesh:
            with self.mesh:
                input_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
                input_ids = jax.device_put(input_ids, input_sharding)
        
        # é¢„çƒ­
        print("ğŸ”¥ é¢„çƒ­ç¼–è¯‘...")
        for i in range(3):
            logits = self.forward_fn(self.params, input_ids)
            jax.block_until_ready(logits)
            print(f"   é¢„çƒ­ {i+1}/3 å®Œæˆ")
        
        # åŸºå‡†æµ‹è¯•
        print(f"ğŸš€ è¿è¡Œ {num_runs} æ¬¡åŸºå‡†æµ‹è¯•...")
        times = []
        
        for i in range(num_runs):
            start_time = time.time()
            logits = self.forward_fn(self.params, input_ids)
            jax.block_until_ready(logits)
            end_time = time.time()
            
            time_taken = end_time - start_time
            throughput = (batch_size * seq_len) / time_taken
            times.append(time_taken)
            
            print(f"   Run {i+1}: {time_taken*1000:.2f}ms, {throughput:.1f} tokens/s")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_time = np.mean(times)
        std_time = np.std(times)
        mean_throughput = (batch_size * seq_len) / mean_time
        peak_throughput = (batch_size * seq_len) / min(times)
        
        results = {
            'config': {
                'n_embd': self.config.n_embd,
                'n_layer': self.config.n_layer,
                'n_head': self.config.n_head,
                'params': self.config.get_param_count()
            },
            'performance': {
                'batch_size': batch_size,
                'seq_len': seq_len,
                'total_tokens': batch_size * seq_len,
                'mean_time': mean_time,
                'std_time': std_time,
                'mean_throughput': mean_throughput,
                'peak_throughput': peak_throughput,
                'devices': len(self.devices)
            },
            'times': times
        }
        
        print(f"\nğŸ¯ ç»ˆææ€§èƒ½ç»“æœ:")
        print(f"   å¹³å‡æ—¶é—´: {mean_time*1000:.2f}ms Â± {std_time*1000:.2f}ms")
        print(f"   å¹³å‡ååé‡: {mean_throughput:.1f} tokens/s")
        print(f"   å³°å€¼ååé‡: {peak_throughput:.1f} tokens/s")
        print(f"   å‚æ•°é‡: {self.config.get_param_count()/1e6:.1f}M")
        print(f"   è®¾å¤‡åˆ©ç”¨: {len(self.devices)}x RTX 3090")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç»ˆæä¼˜åŒ–GPT-1.5B JAXæ¨ç†ç³»ç»Ÿ")
    print("=" * 60)
    print(f"ğŸ“¦ JAXç‰ˆæœ¬: {jax.__version__}")
    print(f"ğŸ–¥ï¸ è®¾å¤‡æ•°é‡: {len(jax.devices())}")
    
    # åˆ›å»ºç»ˆæé…ç½®
    config = UltimateGPTConfig()
    print(f"\nğŸ“‹ ç»ˆæé…ç½®:")
    print(f"   å‚æ•°é‡: {config.get_param_count()/1e6:.1f}M")
    print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {config.n_positions}")
    print(f"   Flashæ³¨æ„åŠ›: {config.use_flash_attention}")
    
    try:
        # åˆ›å»ºæ¨ç†å¼•æ“
        engine = UltimateInferenceEngine(config)
        
        # è¿è¡Œå¤šç§æ‰¹æ¬¡å¤§å°çš„æµ‹è¯•
        batch_sizes = [4, 8, 16, 32]
        all_results = []
        
        for batch_size in batch_sizes:
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            try:
                results = engine.benchmark_ultimate_performance(
                    batch_size=batch_size, 
                    seq_len=512, 
                    num_runs=5
                )
                all_results.append(results)
                
                # å¦‚æœå†…å­˜ä¸è¶³ï¼Œåœæ­¢å¢åŠ æ‰¹æ¬¡å¤§å°
                if results['performance']['mean_time'] > 30:  # è¶…è¿‡30ç§’å°±åœæ­¢
                    print("âš ï¸ æ—¶é—´è¿‡é•¿ï¼Œåœæ­¢å¢åŠ æ‰¹æ¬¡å¤§å°")
                    break
                    
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡å¤§å° {batch_size} æµ‹è¯•å¤±è´¥: {e}")
                break
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        if all_results:
            results_file = Path("ultimate_performance_results.json")
            with open(results_file, 'w') as f:
                json.dump(all_results, f, indent=2, default=str)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            
            # æ‰¾å‡ºæœ€ä½³æ€§èƒ½
            best_result = max(all_results, key=lambda x: x['performance']['peak_throughput'])
            best_throughput = best_result['performance']['peak_throughput']
            best_batch = best_result['performance']['batch_size']
            
            print(f"\nğŸ† æœ€ä½³æ€§èƒ½:")
            print(f"   å³°å€¼ååé‡: {best_throughput:.1f} tokens/s")
            print(f"   æœ€ä½³æ‰¹æ¬¡å¤§å°: {best_batch}")
            print(f"   æ¨¡å‹è§„æ¨¡: {config.get_param_count()/1e6:.1f}Må‚æ•°")
            print(f"   å¤šGPUåŠ é€Ÿ: {len(jax.devices())}x RTX 3090")
            
        print(f"\nâœ… ç»ˆæä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
