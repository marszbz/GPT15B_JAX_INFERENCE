#!/usr/bin/env python3
"""
GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•ä¸»ç¨‹åº
Ubuntu + 4*RTX3080 + CUDA11.8 + JAX0.6.1
ä½¿ç”¨å›¾åˆ†å‰²æ–¹æ³•è¿›è¡Œå¤šGPUå¹¶è¡Œæ¨ç†
"""

import os
import sys
import argparse
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# è®¾ç½®JAXç¯å¢ƒ
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
# æ¸…ç†å¯èƒ½å­˜åœ¨çš„XLA_FLAGS
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

import jax
import numpy as np
from src.models.gpt_model import GPTConfig, GraphPartitionedGPT
from src.data.dataset_loader import DatasetLoader
from src.inference.benchmark import InferenceBenchmark
from src.utils.gpu_utils import setup_jax_environment, check_gpu_setup, print_gpu_status
from src.utils.results import save_benchmark_results, print_performance_summary


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description='GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•')
    parser.add_argument('--dataset-dir', default='datasets', help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', default='results', help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--config', type=str, help='æµ‹è¯•ç‰¹å®šé…ç½®ID (ä¾‹å¦‚: 0,1,3)')
    parser.add_argument('--max-samples', type=int, default=10, help='æ¯ä¸ªé…ç½®çš„æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--show-gpu-info', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†GPUä¿¡æ¯')
    
    args = parser.parse_args()
    
    print("ğŸ¯ GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    print(f"ğŸ’» å¹³å°: Windows")
    print(f"ğŸ Python: 3.10")
    print(f"âš¡ CUDA: 11.8")
    
    # è®¾ç½®JAXç¯å¢ƒ
    setup_jax_environment()
    
    # æ£€æŸ¥GPUç¯å¢ƒ
    if not check_gpu_setup():
        print("âŒ GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    if args.show_gpu_info:
        print_gpu_status()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print(f"\nğŸ—ï¸ åˆå§‹åŒ–GPT-1.5Bæ¨¡å‹...")
    config = GPTConfig()
    model = GraphPartitionedGPT(config)
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®é›†ä»: {args.dataset_dir}")
    dataset_loader = DatasetLoader(args.dataset_dir)
    datasets = dataset_loader.get_valid_datasets()
    
    if not datasets:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®é›†ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶")
        return
    
    # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡
    dataset_loader.print_dataset_summary()
    
    # è¿‡æ»¤ç‰¹å®šé…ç½®
    if args.config:
        config_ids = [c.strip() for c in args.config.split(',')]
        filtered_datasets = {k: v for k, v in datasets.items() if k in config_ids}
        if filtered_datasets:
            datasets = filtered_datasets
            print(f"\nğŸ¯ åªæµ‹è¯•æŒ‡å®šé…ç½®: {list(datasets.keys())}")
        else:
            print(f"âŒ æŒ‡å®šçš„é…ç½® {config_ids} ä¸å­˜åœ¨")
            return
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = InferenceBenchmark(model)
    results = benchmark.run_full_benchmark(datasets, args.max_samples)
    
    # ä¿å­˜ç»“æœ
    json_file, report_file = save_benchmark_results(results, args.output_dir)
    
    # æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦
    print_performance_summary(results)
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   {report_file.name}")
    print(f"   {json_file.name}")


if __name__ == "__main__":
    main()
