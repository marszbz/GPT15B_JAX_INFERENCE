#!/usr/bin/env python3
"""
JAXåˆ†å¸ƒå¼æ¨ç†ç­–ç•¥å®Œæ•´æ•™ç¨‹
è¯¦ç»†è§£é‡ŠJAXåœ¨å¤šGPUç¯å¢ƒä¸‹çš„åˆ†å¸ƒå¼æ¨ç†æœºåˆ¶
"""

import os
import sys
import time
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass

# è®¾ç½®JAXç¯å¢ƒ
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

try:
    import jax
    import jax.numpy as jnp
    from jax import random, devices
    from jax.sharding import Mesh, PartitionSpec, NamedSharding
    from jax.experimental import mesh_utils
    import flax.linen as nn
    print("âœ… JAX {} åˆ†å¸ƒå¼æ¨ç†æ•™ç¨‹".format(jax.__version__))
except ImportError as e:
    print("âŒ JAXå¯¼å…¥å¤±è´¥: {}".format(e))
    sys.exit(1)

class JAXDistributedInferenceTutorial:
    """JAXåˆ†å¸ƒå¼æ¨ç†ç­–ç•¥æ•™ç¨‹"""
    
    def __init__(self):
        self.devices = jax.devices()
        self.mesh = None
        
    def explain_distributed_concepts(self):
        """è§£é‡Šåˆ†å¸ƒå¼æ¨ç†çš„åŸºæœ¬æ¦‚å¿µ"""
        print("ğŸ“ JAXåˆ†å¸ƒå¼æ¨ç†åŸºç¡€æ¦‚å¿µ")
        print("="*80)
        
        print("ğŸ’¡ ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼æ¨ç†ï¼Ÿ")
        print("   åˆ†å¸ƒå¼æ¨ç†æ˜¯å°†å¤§å‹æ¨¡å‹çš„æ¨ç†è®¡ç®—åˆ†æ•£åˆ°å¤šä¸ªGPUä¸Š")
        print("   ç›®æ ‡ï¼šçªç ´å•GPUå†…å­˜é™åˆ¶ï¼Œæé«˜æ¨ç†é€Ÿåº¦")
        
        print("\nğŸ” JAXåˆ†å¸ƒå¼æ¨ç†çš„æ ¸å¿ƒç»„ä»¶ï¼š")
        print("   1. è®¾å¤‡ç½‘æ ¼(Device Mesh): å°†GPUç»„ç»‡ä¸ºé€»è¾‘ç½‘æ ¼")
        print("   2. åˆ†ç‰‡ç­–ç•¥(Sharding): å†³å®šæ•°æ®å¦‚ä½•åˆ†å¸ƒåˆ°å„GPU")
        print("   3. è‡ªåŠ¨å¹¶è¡ŒåŒ–: JAXè‡ªåŠ¨å¤„ç†è®¡ç®—å’Œé€šä¿¡")
        print("   4. JITç¼–è¯‘: é™æ€å›¾ç¼–è¯‘ä¼˜åŒ–æ€§èƒ½")
        
        print("\nğŸ“Š åˆ†å¸ƒå¼æ¨ç†çš„ä¼˜åŠ¿ï¼š")
        print("   âœ… å†…å­˜æ‰©å±•: å¯ä»¥è¿è¡Œè¶…å¤§æ¨¡å‹")
        print("   âœ… é€Ÿåº¦æå‡: å¹¶è¡Œè®¡ç®—åŠ é€Ÿæ¨ç†")
        print("   âœ… è‡ªåŠ¨åŒ–: JAXå¤„ç†å¤æ‚çš„åˆ†å¸ƒå¼ç»†èŠ‚")
        print("   âœ… é€æ˜æ€§: ä»£ç å‡ ä¹æ— éœ€ä¿®æ”¹")
    
    def demonstrate_device_mesh_creation(self):
        """æ¼”ç¤ºè®¾å¤‡ç½‘æ ¼åˆ›å»º"""
        print("\nğŸ•¸ï¸ è®¾å¤‡ç½‘æ ¼(Device Mesh)è¯¦è§£")
        print("="*80)
        
        print("ğŸ“± å½“å‰GPUé…ç½®ï¼š")
        for i, device in enumerate(self.devices):
            print("   GPU {}: {}".format(i, device))
        
        print("\nğŸ”§ è®¾å¤‡ç½‘æ ¼çš„ä½œç”¨ï¼š")
        print("   â€¢ å°†ç‰©ç†GPUç»„ç»‡ä¸ºé€»è¾‘ç½‘æ ¼")
        print("   â€¢ å®šä¹‰æ•°æ®å’Œæ¨¡å‹çš„åˆ†å¸ƒè½´")
        print("   â€¢ æ”¯æŒå¤šç»´å¹¶è¡Œç­–ç•¥")
        
        if len(self.devices) >= 4:
            # åˆ›å»º2x2ç½‘æ ¼
            devices_array = np.array(self.devices[:4]).reshape(2, 2)
            self.mesh = Mesh(devices_array, axis_names=('data', 'model'))
            
            print("\nâœ… åˆ›å»º2x2è®¾å¤‡ç½‘æ ¼ï¼š")
            print("   ç½‘æ ¼å½¢çŠ¶: {}".format(dict(self.mesh.shape)))
            print("   è½´åç§°: {}".format(self.mesh.axis_names))
            
            print("\nğŸ“ è®¾å¤‡åˆ†å¸ƒå›¾ï¼š")
            print("   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print("   â”‚  GPU0   â”‚  GPU1   â”‚ â† dataè½´=0")
            print("   â”‚ (data=0)â”‚ (data=0)â”‚")
            print("   â”‚(model=0)â”‚(model=1)â”‚")
            print("   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
            print("   â”‚  GPU2   â”‚  GPU3   â”‚ â† dataè½´=1")
            print("   â”‚ (data=1)â”‚ (data=1)â”‚")
            print("   â”‚(model=0)â”‚(model=1)â”‚")
            print("   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            print("    model=0   model=1")
            
        elif len(self.devices) == 2:
            devices_array = np.array(self.devices).reshape(2, 1)
            self.mesh = Mesh(devices_array, axis_names=('data', 'model'))
            print("\nâœ… åˆ›å»º2x1è®¾å¤‡ç½‘æ ¼")
            
        else:
            print("\nâš ï¸ GPUæ•°é‡ä¸è¶³ï¼Œæ— æ³•æ¼”ç¤ºå¤šGPUç½‘æ ¼")
            return False
            
        return True
    
    def explain_sharding_strategies(self):
        """è§£é‡Šåˆ†ç‰‡ç­–ç•¥"""
        print("\nğŸ”€ åˆ†ç‰‡ç­–ç•¥(Sharding Strategies)è¯¦è§£")
        print("="*80)
        
        print("ğŸ¯ ä»€ä¹ˆæ˜¯åˆ†ç‰‡ï¼Ÿ")
        print("   åˆ†ç‰‡æ˜¯å°†å¤§å¼ é‡åˆ‡åˆ†åˆ°å¤šä¸ªè®¾å¤‡ä¸Šçš„ç­–ç•¥")
        print("   ç›®æ ‡ï¼šå¹³è¡¡å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡")
        
        print("\nğŸ“‹ ä¸»è¦åˆ†ç‰‡ç±»å‹ï¼š")
        
        print("\n1ï¸âƒ£ æ•°æ®å¹¶è¡Œ(Data Parallelism):")
        print("   â€¢ æ¨¡å‹å‚æ•°å¤åˆ¶åˆ°æ¯ä¸ªGPU")
        print("   â€¢ è¾“å…¥æ•°æ®æŒ‰batchç»´åº¦åˆ†ç‰‡")
        print("   â€¢ é€‚ç”¨ï¼šæ¨¡å‹è¾ƒå°ï¼Œæ•°æ®é‡å¤§")
        print("   â€¢ ç¤ºä¾‹ï¼šPartitionSpec('data', None)")
        
        print("\n2ï¸âƒ£ æ¨¡å‹å¹¶è¡Œ(Model Parallelism):")
        print("   â€¢ æ¨¡å‹å‚æ•°æŒ‰ç»´åº¦åˆ†ç‰‡")
        print("   â€¢ è¾“å…¥æ•°æ®ä¿æŒå®Œæ•´æˆ–éƒ¨åˆ†åˆ†ç‰‡")
        print("   â€¢ é€‚ç”¨ï¼šæ¨¡å‹å·¨å¤§ï¼Œå†…å­˜ä¸è¶³")
        print("   â€¢ ç¤ºä¾‹ï¼šPartitionSpec(None, 'model')")
        
        print("\n3ï¸âƒ£ æ··åˆå¹¶è¡Œ(Hybrid Parallelism):")
        print("   â€¢ ç»“åˆæ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œ")
        print("   â€¢ çµæ´»çš„åˆ†ç‰‡ç»„åˆ")
        print("   â€¢ é€‚ç”¨ï¼šå¤§æ¨¡å‹+å¤§æ•°æ®")
        print("   â€¢ ç¤ºä¾‹ï¼šPartitionSpec('data', 'model')")
        
        print("\n4ï¸âƒ£ æµæ°´çº¿å¹¶è¡Œ(Pipeline Parallelism):")
        print("   â€¢ æ¨¡å‹æŒ‰å±‚åˆ†ç‰‡åˆ°ä¸åŒGPU")
        print("   â€¢ å±‚é—´ä¸²è¡Œæ‰§è¡Œï¼Œå±‚å†…å¹¶è¡Œ")
        print("   â€¢ é€‚ç”¨ï¼šè¶…æ·±æ¨¡å‹")
        
        if self.mesh:
            self._demonstrate_sharding_examples()
    
    def _demonstrate_sharding_examples(self):
        """æ¼”ç¤ºå…·ä½“çš„åˆ†ç‰‡ç¤ºä¾‹"""
        print("\nğŸ¬ åˆ†ç‰‡ç­–ç•¥å®é™…æ¼”ç¤º")
        print("-" * 40)
        
        # åˆ›å»ºç¤ºä¾‹å¼ é‡
        key = jax.random.PRNGKey(42)
        batch_size, seq_len, hidden_dim = 8, 128, 1024
        
        x = jax.random.normal(key, (batch_size, seq_len, hidden_dim))
        print("åŸå§‹å¼ é‡å½¢çŠ¶: {}".format(x.shape))
        
        with self.mesh:
            print("\nğŸ“Š ä¸åŒåˆ†ç‰‡ç­–ç•¥å¯¹æ¯”ï¼š")
            
            # ç­–ç•¥1ï¼šæ•°æ®å¹¶è¡Œ
            data_parallel_spec = PartitionSpec('data', None, None)
            data_sharding = NamedSharding(self.mesh, data_parallel_spec)
            x_data_parallel = jax.device_put(x, data_sharding)
            
            print("\n1ï¸âƒ£ æ•°æ®å¹¶è¡Œåˆ†ç‰‡ï¼š")
            print("   åˆ†ç‰‡è§„èŒƒ: {}".format(data_parallel_spec))
            print("   å«ä¹‰: batchç»´åº¦åˆ†ç‰‡ï¼Œå…¶ä»–ç»´åº¦å®Œæ•´")
            print("   æ¯ä¸ªGPU: ({}, {}, {})".format(
                batch_size//2, seq_len, hidden_dim))
            
            # ç­–ç•¥2ï¼šåºåˆ—å¹¶è¡Œ
            seq_parallel_spec = PartitionSpec(None, 'model', None)
            seq_sharding = NamedSharding(self.mesh, seq_parallel_spec)
            x_seq_parallel = jax.device_put(x, seq_sharding)
            
            print("\n2ï¸âƒ£ åºåˆ—å¹¶è¡Œåˆ†ç‰‡ï¼š")
            print("   åˆ†ç‰‡è§„èŒƒ: {}".format(seq_parallel_spec))
            print("   å«ä¹‰: åºåˆ—é•¿åº¦ç»´åº¦åˆ†ç‰‡")
            print("   æ¯ä¸ªGPU: ({}, {}, {})".format(
                batch_size, seq_len//2, hidden_dim))
            
            # ç­–ç•¥3ï¼šéšè—å±‚å¹¶è¡Œ
            hidden_parallel_spec = PartitionSpec(None, None, 'model')
            hidden_sharding = NamedSharding(self.mesh, hidden_parallel_spec)
            x_hidden_parallel = jax.device_put(x, hidden_sharding)
            
            print("\n3ï¸âƒ£ éšè—å±‚å¹¶è¡Œåˆ†ç‰‡ï¼š")
            print("   åˆ†ç‰‡è§„èŒƒ: {}".format(hidden_parallel_spec))
            print("   å«ä¹‰: éšè—ç»´åº¦åˆ†ç‰‡")
            print("   æ¯ä¸ªGPU: ({}, {}, {})".format(
                batch_size, seq_len, hidden_dim//2))
            
            # ç­–ç•¥4ï¼šæ··åˆå¹¶è¡Œ
            hybrid_spec = PartitionSpec('data', 'model', None)
            hybrid_sharding = NamedSharding(self.mesh, hybrid_spec)
            x_hybrid = jax.device_put(x, hybrid_sharding)
            
            print("\n4ï¸âƒ£ æ··åˆå¹¶è¡Œåˆ†ç‰‡ï¼š")
            print("   åˆ†ç‰‡è§„èŒƒ: {}".format(hybrid_spec))
            print("   å«ä¹‰: batchå’Œåºåˆ—ç»´åº¦éƒ½åˆ†ç‰‡")
            print("   æ¯ä¸ªGPU: ({}, {}, {})".format(
                batch_size//2, seq_len//2, hidden_dim))
    
    def explain_inference_patterns(self):
        """è§£é‡Šæ¨ç†æ¨¡å¼"""
        print("\nğŸš€ åˆ†å¸ƒå¼æ¨ç†æ¨¡å¼è¯¦è§£")
        print("="*80)
        
        print("ğŸ” æ¨ç†è¿‡ç¨‹ä¸­çš„å…³é”®æ­¥éª¤ï¼š")
        
        print("\n1ï¸âƒ£ è¾“å…¥é¢„å¤„ç†ï¼š")
        print("   â€¢ å°†è¾“å…¥æ•°æ®åˆ†ç‰‡åˆ°å„GPU")
        print("   â€¢ æ ¹æ®åˆ†ç‰‡ç­–ç•¥åˆ†å¸ƒtoken")
        print("   â€¢ å¤„ç†attention mask")
        
        print("\n2ï¸âƒ£ å‰å‘ä¼ æ’­ï¼š")
        print("   â€¢ æ¯ä¸ªGPUå¹¶è¡Œè®¡ç®—å±€éƒ¨ç»“æœ")
        print("   â€¢ æ ¹æ®éœ€è¦æ’å…¥é€šä¿¡æ“ä½œ")
        print("   â€¢ è‡ªåŠ¨å¤„ç†ç»´åº¦åŒ¹é…")
        
        print("\n3ï¸âƒ£ é€šä¿¡åŒæ­¥ï¼š")
        print("   â€¢ All-Reduce: å½’çº¦æ±‚å’Œ/å¹³å‡")
        print("   â€¢ All-Gather: æ”¶é›†æ‰€æœ‰åˆ†ç‰‡")
        print("   â€¢ Reduce-Scatter: å½’çº¦åé‡åˆ†å¸ƒ")
        
        print("\n4ï¸âƒ£ è¾“å‡ºèšåˆï¼š")
        print("   â€¢ åˆå¹¶å„GPUçš„è¾“å‡º")
        print("   â€¢ ç”Ÿæˆæœ€ç»ˆé¢„æµ‹ç»“æœ")
        print("   â€¢ å¤„ç†æ¦‚ç‡åˆ†å¸ƒ")
        
        if self.mesh:
            self._demonstrate_inference_flow()
    
    def _demonstrate_inference_flow(self):
        """æ¼”ç¤ºæ¨ç†æµç¨‹"""
        print("\nğŸ¬ å®é™…æ¨ç†æµç¨‹æ¼”ç¤º")
        print("-" * 40)
        
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„æ¨ç†å‡½æ•°
        def simple_inference(x, w1, w2):
            """ç®€åŒ–çš„æ¨ç†å‡½æ•°ï¼šLinear -> ReLU -> Linear"""
            h = jnp.dot(x, w1)  # ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢
            h = jax.nn.relu(h)  # æ¿€æ´»å‡½æ•°
            return jnp.dot(h, w2)  # ç¬¬äºŒå±‚çº¿æ€§å˜æ¢
        
        # JITç¼–è¯‘
        jit_inference = jax.jit(simple_inference)
        
        key = jax.random.PRNGKey(42)
        batch_size, input_dim, hidden_dim, output_dim = 8, 512, 1024, 256
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = jax.random.normal(key, (batch_size, input_dim))
        w1 = jax.random.normal(key, (input_dim, hidden_dim))
        w2 = jax.random.normal(key, (hidden_dim, output_dim))
        
        with self.mesh:
            print("ğŸ“ æ¨ç†å‡½æ•°ï¼š")
            print("   def inference(x, w1, w2):")
            print("       h = x @ w1    # è¾“å…¥å˜æ¢")
            print("       h = relu(h)   # æ¿€æ´»")
            print("       return h @ w2 # è¾“å‡ºå˜æ¢")
            
            # å®šä¹‰åˆ†ç‰‡ç­–ç•¥
            x_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
            w1_sharding = NamedSharding(self.mesh, PartitionSpec(None, 'model'))
            w2_sharding = NamedSharding(self.mesh, PartitionSpec('model', None))
            
            # åº”ç”¨åˆ†ç‰‡
            x_sharded = jax.device_put(x, x_sharding)
            w1_sharded = jax.device_put(w1, w1_sharding)
            w2_sharded = jax.device_put(w2, w2_sharding)
            
            print("\nğŸ”€ åˆ†ç‰‡é…ç½®ï¼š")
            print("   x:  {} â†’ æŒ‰batchåˆ†ç‰‡".format(x.shape))
            print("   w1: {} â†’ æŒ‰hiddenåˆ†ç‰‡".format(w1.shape))
            print("   w2: {} â†’ æŒ‰hiddenåˆ†ç‰‡".format(w2.shape))
            
            # æ‰§è¡Œæ¨ç†
            print("\nâš¡ æ‰§è¡Œåˆ†å¸ƒå¼æ¨ç†...")
            start_time = time.time()
            result = jit_inference(x_sharded, w1_sharded, w2_sharded)
            jax.block_until_ready(result)
            inference_time = time.time() - start_time
            
            print("âœ… æ¨ç†å®Œæˆï¼")
            print("   è¾“å…¥: {}".format(x.shape))
            print("   è¾“å‡º: {}".format(result.shape))
            print("   æ—¶é—´: {:.2f}ms".format(inference_time * 1000))
            print("   è®¾å¤‡: {}ä¸ªGPUå¹¶è¡Œ".format(len(self.devices)))
    
    def explain_communication_patterns(self):
        """è§£é‡Šé€šä¿¡æ¨¡å¼"""
        print("\nğŸ“¡ åˆ†å¸ƒå¼æ¨ç†ä¸­çš„é€šä¿¡æ¨¡å¼")
        print("="*80)
        
        print("ğŸ”„ JAXè‡ªåŠ¨é€šä¿¡æœºåˆ¶ï¼š")
        print("   JAXä¼šåœ¨éœ€è¦æ—¶è‡ªåŠ¨æ’å…¥é€šä¿¡æ“ä½œ")
        print("   ç”¨æˆ·æ— éœ€æ‰‹åŠ¨ç®¡ç†é€šä¿¡")
        
        print("\nğŸ“‹ å¸¸è§é€šä¿¡æ“ä½œï¼š")
        
        print("\n1ï¸âƒ£ All-Reduce (å…¨å½’çº¦):")
        print("   åœºæ™¯: éœ€è¦å¯¹æ‰€æœ‰GPUçš„ç»“æœæ±‚å’Œ/å¹³å‡")
        print("   ç¤ºä¾‹: æŸå¤±å‡½æ•°è®¡ç®—ã€æ¢¯åº¦èšåˆ")
        print("   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”")
        print("   â”‚ GPU0â”‚â”€â”€â–¶â”‚ SUM â”‚")
        print("   â””â”€â”€â”€â”€â”€â”˜   â”‚     â”‚")
        print("   â”Œâ”€â”€â”€â”€â”€â”   â”‚     â”‚   â”Œâ”€â”€â”€â”€â”€â”")
        print("   â”‚ GPU1â”‚â”€â”€â–¶â”‚     â”‚â”€â”€â–¶â”‚ ALL â”‚")
        print("   â””â”€â”€â”€â”€â”€â”˜   â”‚     â”‚   â””â”€â”€â”€â”€â”€â”˜")
        print("   â”Œâ”€â”€â”€â”€â”€â”   â”‚     â”‚")
        print("   â”‚ GPU2â”‚â”€â”€â–¶â”‚     â”‚")
        print("   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜")
        
        print("\n2ï¸âƒ£ All-Gather (å…¨æ”¶é›†):")
        print("   åœºæ™¯: éœ€è¦æ”¶é›†æ‰€æœ‰GPUçš„æ•°æ®ç‰‡æ®µ")
        print("   ç¤ºä¾‹: åºåˆ—é•¿åº¦åˆ†ç‰‡åçš„é‡ç»„")
        print("   GPU0: [A] â”€â”€â”")
        print("   GPU1: [B] â”€â”€â”¼â”€â”€â–¶ [A,B,C,D] (å…¨éƒ¨)")
        print("   GPU2: [C] â”€â”€â”˜")
        print("   GPU3: [D] â”€â”€â”˜")
        
        print("\n3ï¸âƒ£ Reduce-Scatter (å½’çº¦åˆ†æ•£):")
        print("   åœºæ™¯: å½’çº¦åæŒ‰æ–°ç­–ç•¥åˆ†ç‰‡")
        print("   ç¤ºä¾‹: æ³¨æ„åŠ›è®¡ç®—åçš„é‡åˆ†å¸ƒ")
        
        print("\nğŸš€ é€šä¿¡ä¼˜åŒ–æŠ€æœ¯ï¼š")
        print("   â€¢ é€šä¿¡/è®¡ç®—é‡å ï¼šè¾¹ç®—è¾¹ä¼ ")
        print("   â€¢ æ“ä½œèåˆï¼šå‡å°‘é€šä¿¡æ¬¡æ•°")
        print("   â€¢ å¸¦å®½ä¼˜åŒ–ï¼šé«˜æ•ˆåˆ©ç”¨ç½‘ç»œ")
        print("   â€¢ æ‹“æ‰‘æ„ŸçŸ¥ï¼šåŸºäºç¡¬ä»¶å¸ƒå±€ä¼˜åŒ–")
    
    def demonstrate_performance_analysis(self):
        """æ¼”ç¤ºæ€§èƒ½åˆ†æ"""
        print("\nğŸ“ˆ åˆ†å¸ƒå¼æ¨ç†æ€§èƒ½åˆ†æ")
        print("="*80)
        
        print("âš¡ æ€§èƒ½å½±å“å› ç´ ï¼š")
        
        print("\n1ï¸âƒ£ å¹¶è¡Œæ•ˆç‡ï¼š")
        print("   ç†æƒ³æƒ…å†µ: 4ä¸ªGPU = 4å€é€Ÿåº¦")
        print("   å®é™…æƒ…å†µ: é€šä¿¡å¼€é”€å¯¼è‡´æ•ˆç‡ä¸‹é™")
        print("   ç›®æ ‡: æœ€å¤§åŒ–è®¡ç®—/é€šä¿¡æ¯”")
        
        print("\n2ï¸âƒ£ å†…å­˜æ•ˆç‡ï¼š")
        print("   æ¨¡å‹å¹¶è¡Œ: èŠ‚çœå†…å­˜ï¼Œå¢åŠ é€šä¿¡")
        print("   æ•°æ®å¹¶è¡Œ: å¤åˆ¶æ¨¡å‹ï¼ŒèŠ‚çœé€šä¿¡")
        print("   æ··åˆç­–ç•¥: å¹³è¡¡å†…å­˜å’Œé€šä¿¡")
        
        print("\n3ï¸âƒ£ æ‰¹æ¬¡å¤§å°å½±å“ï¼š")
        print("   æ›´å¤§æ‰¹æ¬¡: æ›´å¥½çš„GPUåˆ©ç”¨ç‡")
        print("   åˆ†ç‰‡æ‰¹æ¬¡: æ¯GPUå¤„ç†éƒ¨åˆ†æ•°æ®")
        print("   æƒè¡¡: å»¶è¿Ÿ vs ååé‡")
        
        if self.mesh and len(self.devices) >= 2:
            self._performance_comparison()
    
    def _performance_comparison(self):
        """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
        print("\nğŸ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("-" * 40)
        
        def simple_matmul(x, w):
            return jnp.dot(x, w)
        
        jit_matmul = jax.jit(simple_matmul)
        
        key = jax.random.PRNGKey(42)
        x = jax.random.normal(key, (64, 1024))
        w = jax.random.normal(key, (1024, 2048))
        
        with self.mesh:
            # æµ‹è¯•ä¸åŒåˆ†ç‰‡ç­–ç•¥çš„æ€§èƒ½
            strategies = [
                ("å¤åˆ¶ç­–ç•¥", PartitionSpec(), PartitionSpec()),
                ("æ•°æ®åˆ†ç‰‡", PartitionSpec('data', None), PartitionSpec()),
                ("æ¨¡å‹åˆ†ç‰‡", PartitionSpec(), PartitionSpec(None, 'model')),
            ]
            
            print("æµ‹è¯•é…ç½®: {} @ {}".format(x.shape, w.shape))
            print("\næ€§èƒ½å¯¹æ¯”:")
            
            for name, x_spec, w_spec in strategies:
                try:
                    x_sharding = NamedSharding(self.mesh, x_spec)
                    w_sharding = NamedSharding(self.mesh, w_spec)
                    
                    x_sharded = jax.device_put(x, x_sharding)
                    w_sharded = jax.device_put(w, w_sharding)
                    
                    # é¢„çƒ­
                    for _ in range(3):
                        result = jit_matmul(x_sharded, w_sharded)
                        jax.block_until_ready(result)
                    
                    # è®¡æ—¶
                    times = []
                    for _ in range(5):
                        start = time.time()
                        result = jit_matmul(x_sharded, w_sharded)
                        jax.block_until_ready(result)
                        times.append(time.time() - start)
                    
                    avg_time = np.mean(times) * 1000
                    print("   {}: {:.2f}ms".format(name, avg_time))
                    
                except Exception as e:
                    print("   {}: å¤±è´¥ - {}".format(name, str(e)))
    
    def explain_practical_considerations(self):
        """è§£é‡Šå®é™…åº”ç”¨è€ƒè™‘"""
        print("\nğŸ’¡ å®é™…åº”ç”¨è€ƒè™‘")
        print("="*80)
        
        print("ğŸ¯ é€‰æ‹©åˆ†ç‰‡ç­–ç•¥çš„åŸåˆ™ï¼š")
        
        print("\n1ï¸âƒ£ æ ¹æ®æ¨¡å‹å¤§å°ï¼š")
        print("   å°æ¨¡å‹(< 1B): æ•°æ®å¹¶è¡Œä¼˜å…ˆ")
        print("   ä¸­æ¨¡å‹(1B-10B): æ··åˆå¹¶è¡Œ")
        print("   å¤§æ¨¡å‹(> 10B): æ¨¡å‹å¹¶è¡Œä¸ºä¸»")
        
        print("\n2ï¸âƒ£ æ ¹æ®ç¡¬ä»¶é…ç½®ï¼š")
        print("   é«˜å†…å­˜GPU: å¯ç”¨æ•°æ®å¹¶è¡Œ")
        print("   å¤šGPUç³»ç»Ÿ: è€ƒè™‘æ··åˆå¹¶è¡Œ")
        print("   ç½‘ç»œå¸¦å®½: å½±å“é€šä¿¡ç­–ç•¥")
        
        print("\n3ï¸âƒ£ æ ¹æ®åº”ç”¨åœºæ™¯ï¼š")
        print("   åœ¨çº¿æ¨ç†: ä½å»¶è¿Ÿä¼˜å…ˆ")
        print("   æ‰¹å¤„ç†: é«˜ååé‡ä¼˜å…ˆ")
        print("   å®æ—¶åº”ç”¨: ç¨³å®šæ€§ä¼˜å…ˆ")
        
        print("\nğŸ”§ æœ€ä½³å®è·µï¼š")
        print("   âœ… ä»ç®€å•ç­–ç•¥å¼€å§‹")
        print("   âœ… æµ‹è¯•ä¸åŒé…ç½®")
        print("   âœ… ç›‘æ§GPUåˆ©ç”¨ç‡")
        print("   âœ… è€ƒè™‘å†…å­˜é™åˆ¶")
        print("   âœ… ä¼˜åŒ–æ‰¹æ¬¡å¤§å°")
        
        print("\nâš ï¸ å¸¸è§é™·é˜±ï¼š")
        print("   âŒ è¿‡åº¦åˆ†ç‰‡å¯¼è‡´é€šä¿¡å¼€é”€")
        print("   âŒ è´Ÿè½½ä¸å‡è¡¡")
        print("   âŒ å¿½ç•¥å†…å­˜ç¢ç‰‡")
        print("   âŒ æœªè€ƒè™‘æ‰©å±•æ€§")
    
    def comprehensive_tutorial(self):
        """å®Œæ•´æ•™ç¨‹"""
        print("ğŸ“ JAXåˆ†å¸ƒå¼æ¨ç†å®Œæ•´æ•™ç¨‹")
        print("="*80)
        
        # é€æ­¥è®²è§£
        self.explain_distributed_concepts()
        
        if self.demonstrate_device_mesh_creation():
            self.explain_sharding_strategies()
            self.explain_inference_patterns()
            self.explain_communication_patterns()
            self.demonstrate_performance_analysis()
        
        self.explain_practical_considerations()
        
        # æ€»ç»“
        print("\nğŸ¯ æ•™ç¨‹æ€»ç»“")
        print("="*60)
        print("âœ… æ ¸å¿ƒæ¦‚å¿µ:")
        print("   â€¢ è®¾å¤‡ç½‘æ ¼: ç»„ç»‡GPUçš„é€»è¾‘ç»“æ„")
        print("   â€¢ åˆ†ç‰‡ç­–ç•¥: å†³å®šæ•°æ®åˆ†å¸ƒæ–¹å¼")
        print("   â€¢ è‡ªåŠ¨å¹¶è¡Œ: JAXå¤„ç†åˆ†å¸ƒå¼ç»†èŠ‚")
        print("   â€¢ é€šä¿¡ä¼˜åŒ–: æœ€å°åŒ–æ•°æ®ä¼ è¾“å¼€é”€")
        
        print("\nğŸš€ å…³é”®ä¼˜åŠ¿:")
        print("   â€¢ å†…å­˜æ‰©å±•: çªç ´å•GPUé™åˆ¶")
        print("   â€¢ æ€§èƒ½æå‡: å¤šGPUå¹¶è¡ŒåŠ é€Ÿ")
        print("   â€¢ æ˜“ç”¨æ€§: ä»£ç æ”¹åŠ¨æœ€å°")
        print("   â€¢ çµæ´»æ€§: æ”¯æŒå¤šç§å¹¶è¡Œæ¨¡å¼")
        
        print("\nğŸ’¡ æˆåŠŸç§˜è¯€:")
        print("   â€¢ ç†è§£æ¨¡å‹ç‰¹æ€§é€‰æ‹©ç­–ç•¥")
        print("   â€¢ å¹³è¡¡è®¡ç®—å’Œé€šä¿¡å¼€é”€")
        print("   â€¢ å……åˆ†æµ‹è¯•ä¸åŒé…ç½®")
        print("   â€¢ æŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡")
        
        print("\nğŸ‰ ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†JAXåˆ†å¸ƒå¼æ¨ç†çš„æ ¸å¿ƒç­–ç•¥ï¼")

def main():
    """ä¸»å‡½æ•°"""
    tutorial = JAXDistributedInferenceTutorial()
    tutorial.comprehensive_tutorial()

if __name__ == "__main__":
    main()
