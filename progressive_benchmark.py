#!/usr/bin/env python3
"""
æ¸è¿›å¼GPT-1.5B JAXæ¨ç†æµ‹è¯•
é€æ­¥å¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œé¿å…æ­»é”é—®é¢˜
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from functools import partial

# è®¾ç½®JAXç¯å¢ƒï¼ˆå¿…é¡»åœ¨å¯¼å…¥JAXä¹‹å‰ï¼‰
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
# æ¸…ç†å¯èƒ½å­˜åœ¨çš„XLA_FLAGS
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

# å¯¼å…¥JAXç›¸å…³åŒ…
try:
    import jax
    import jax.numpy as jnp
    import flax.linen as nn
    import numpy as np
    print(f"âœ… JAX {jax.__version__} åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)


@dataclass
class ProgressiveGPTConfig:
    """æ¸è¿›å¼GPTé…ç½®"""
    vocab_size: int = 50257
    n_positions: int = 1024
    n_embd: int = 768
    n_layer: int = 12
    n_head: int = 12
    dropout: float = 0.1
    use_bias: bool = True
    
    def get_param_count(self) -> int:
        """ä¼°ç®—å‚æ•°é‡"""
        # åµŒå…¥å±‚: vocab_size * n_embd + n_positions * n_embd
        embed_params = self.vocab_size * self.n_embd + self.n_positions * self.n_embd
        
        # æ¯ä¸ªtransformerå±‚çš„å‚æ•°
        # æ³¨æ„åŠ›: 4 * n_embd^2 (qkv + output projection)
        # MLP: 2 * n_embd * (4 * n_embd) = 8 * n_embd^2
        # LayerNorm: 2 * n_embd * 2 = 4 * n_embd
        layer_params = (4 * self.n_embd * self.n_embd + 
                       8 * self.n_embd * self.n_embd + 
                       4 * self.n_embd)
        
        # æ€»å‚æ•° = åµŒå…¥ + å±‚æ•° * æ¯å±‚å‚æ•° + æœ€ç»ˆLMå¤´
        total_params = embed_params + self.n_layer * layer_params + self.vocab_size * self.n_embd
        return total_params


class ProgressiveMultiHeadAttention(nn.Module):
    """æ¸è¿›å¼å¤šå¤´æ³¨æ„åŠ›"""
    config: ProgressiveGPTConfig
    
    @nn.compact
    def __call__(self, x, mask=None):
        B, T, C = x.shape
        
        # QKVæŠ•å½±
        qkv = nn.Dense(3 * self.config.n_embd, use_bias=self.config.use_bias)(x)
        qkv = qkv.reshape(B, T, 3, self.config.n_head, C // self.config.n_head)
        qkv = qkv.transpose(2, 0, 3, 1, 4)  # (3, B, nh, T, hs)
        
        q, k, v = qkv[0], qkv[1], qkv[2]  # æ¯ä¸ªçš„å½¢çŠ¶: (B, nh, T, hs)
        
        # æ³¨æ„åŠ›æƒé‡ - æ­£ç¡®çš„è½¬ç½®ç»´åº¦
        # k.shape = (B, nh, T, hs), æˆ‘ä»¬æƒ³è½¬ç½®æœ€åä¸¤ä¸ªç»´åº¦ T å’Œ hs
        att = (q @ k.transpose(0, 1, 3, 2)) * (1.0 / jnp.sqrt(k.shape[-1]))
        
        # åº”ç”¨å› æœæ©ç 
        if mask is not None:
            att = jnp.where(mask, att, -jnp.inf)
        
        att = jax.nn.softmax(att, axis=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›åˆ°å€¼
        y = att @ v  # (B, nh, T, hs)
        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)
        
        # è¾“å‡ºæŠ•å½±
        return nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)(y)


class ProgressiveMLP(nn.Module):
    """æ¸è¿›å¼MLP"""
    config: ProgressiveGPTConfig
    
    @nn.compact
    def __call__(self, x):
        hidden = nn.Dense(4 * self.config.n_embd, use_bias=self.config.use_bias)(x)
        hidden = jax.nn.gelu(hidden)
        return nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)(hidden)


class ProgressiveTransformerBlock(nn.Module):
    """æ¸è¿›å¼Transformerå—"""
    config: ProgressiveGPTConfig
    
    @nn.compact
    def __call__(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_out = ProgressiveMultiHeadAttention(self.config)(nn.LayerNorm()(x), mask)
        x = x + attn_out
        
        # MLP + æ®‹å·®è¿æ¥
        mlp_out = ProgressiveMLP(self.config)(nn.LayerNorm()(x))
        x = x + mlp_out
        
        return x


class ProgressiveGPT(nn.Module):
    """æ¸è¿›å¼GPTæ¨¡å‹"""
    config: ProgressiveGPTConfig
    
    @nn.compact
    def __call__(self, input_ids, training: bool = False):
        B, T = input_ids.shape
        
        # TokenåµŒå…¥
        token_embed = nn.Embed(self.config.vocab_size, self.config.n_embd)(input_ids)
        
        # ä½ç½®åµŒå…¥
        pos_embed = nn.Embed(self.config.n_positions, self.config.n_embd)(
            jnp.arange(T)[None, :] % self.config.n_positions
        )
        
        # ç»„åˆåµŒå…¥
        x = token_embed + pos_embed
        
        # åˆ›å»ºå› æœæ©ç 
        mask = jnp.tril(jnp.ones((T, T)))[None, None, :, :]
        mask = mask.astype(jnp.bool_)
        
        # Transformerå±‚
        for i in range(self.config.n_layer):
            x = ProgressiveTransformerBlock(self.config)(x, mask)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        x = nn.LayerNorm()(x)
        
        # è¯­è¨€æ¨¡å‹å¤´
        logits = nn.Dense(self.config.vocab_size, use_bias=False)(x)
        
        return logits


class ProgressiveInferenceEngine:
    """æ¸è¿›å¼æ¨ç†å¼•æ“"""
    
    def __init__(self, config: ProgressiveGPTConfig):
        self.config = config
        self.devices = jax.devices()
        
        print(f"ğŸ”§ åˆå§‹åŒ–æ¸è¿›å¼GPTæ¨ç†å¼•æ“")
        print(f"   è®¾å¤‡æ•°é‡: {len(self.devices)}")
        for i, device in enumerate(self.devices):
            print(f"   è®¾å¤‡ {i}: {device}")
        
        # ä¼°ç®—å‚æ•°é‡
        estimated_params = config.get_param_count()
        print(f"   ä¼°ç®—å‚æ•°é‡: {estimated_params:,} ({estimated_params/1e6:.1f}M)")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = ProgressiveGPT(config)
        self._init_parameters()
    
    def _init_parameters(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        print("ğŸ”„ åˆå§‹åŒ–æ¨¡å‹å‚æ•°...")
        
        key = jax.random.PRNGKey(42)
        dummy_input = jnp.ones((1, 32), dtype=jnp.int32)
        
        # åˆå§‹åŒ–å‚æ•°
        start_time = time.time()
        self.params = self.model.init(key, dummy_input, training=False)
        init_time = time.time() - start_time
        
        # è®¡ç®—å®é™…å‚æ•°é‡
        param_count = sum(x.size for x in jax.tree_util.tree_leaves(self.params))
        print(f"ğŸ“Š å®é™…å‚æ•°é‡: {param_count:,} ({param_count/1e6:.1f}M)")
        print(f"â±ï¸ å‚æ•°åˆå§‹åŒ–æ—¶é—´: {init_time:.2f}s")
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—
        param_memory = param_count * 4 / (1024**3)  # å‡è®¾float32
        print(f"ğŸ’¾ å‚æ•°å†…å­˜ä½¿ç”¨: {param_memory:.2f} GB")
    
    @partial(jax.jit, static_argnums=(0,))
    def forward_pass(self, params, input_ids):
        """JITç¼–è¯‘çš„å‰å‘ä¼ æ’­"""
        return self.model.apply(params, input_ids, training=False)
    
    def warmup(self, input_ids: jnp.ndarray) -> float:
        """é¢„çƒ­JITç¼–è¯‘"""
        print("ğŸ”¥ é¢„çƒ­JITç¼–è¯‘...")
        start_time = time.time()
        
        # æ‰§è¡Œå‡ æ¬¡å‰å‘ä¼ æ’­ä»¥å®ŒæˆJITç¼–è¯‘
        for _ in range(3):
            logits = self.forward_pass(self.params, input_ids)
            jax.block_until_ready(logits)
        
        warmup_time = time.time() - start_time
        print(f"â±ï¸ JITç¼–è¯‘æ—¶é—´: {warmup_time:.2f}s")
        return warmup_time
    
    def benchmark_forward_pass(self, input_ids: jnp.ndarray, num_runs: int = 10) -> Dict[str, float]:
        """åŸºå‡†æµ‹è¯•å‰å‘ä¼ æ’­"""
        print(f"ğŸƒ è¿è¡Œå‰å‘ä¼ æ’­åŸºå‡†æµ‹è¯• ({num_runs}æ¬¡)...")
        
        times = []
        for i in range(num_runs):
            start_time = time.time()
            logits = self.forward_pass(self.params, input_ids)
            jax.block_until_ready(logits)
            end_time = time.time()
            
            times.append(end_time - start_time)
            if (i + 1) % 5 == 0:
                print(f"  å®Œæˆ {i + 1}/{num_runs} æ¬¡")
        
        return {
            'mean_time': np.mean(times),
            'std_time': np.std(times),
            'min_time': np.min(times),
            'max_time': np.max(times),
            'times': times
        }
    
    def benchmark_generation(self, input_ids: jnp.ndarray, max_new_tokens: int = 32) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
        print(f"ğŸ“ è¿è¡Œæ–‡æœ¬ç”ŸæˆåŸºå‡†æµ‹è¯• (ç”Ÿæˆ{max_new_tokens}ä¸ªtokens)...")
        
        current_ids = input_ids
        generation_times = []
        
        start_time = time.time()
        
        for step in range(max_new_tokens):
            # é˜²æ­¢åºåˆ—è¿‡é•¿
            if current_ids.shape[1] >= self.config.n_positions:
                current_ids = current_ids[:, -(self.config.n_positions-1):]
            
            # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
            step_start = time.time()
            logits = self.forward_pass(self.params, current_ids)
            next_token = jnp.argmax(logits[:, -1, :], axis=-1, keepdims=True)
            current_ids = jnp.concatenate([current_ids, next_token], axis=1)
            jax.block_until_ready(current_ids)
            step_time = time.time() - step_start
            
            generation_times.append(step_time)
            
            if (step + 1) % 10 == 0:
                print(f"  ç”Ÿæˆ {step + 1}/{max_new_tokens} tokens")
        
        total_time = time.time() - start_time
        
        return {
            'total_time': total_time,
            'tokens_generated': max_new_tokens,
            'throughput': max_new_tokens / total_time,
            'mean_token_time': np.mean(generation_times),
            'std_token_time': np.std(generation_times),
            'final_sequence_length': current_ids.shape[1],
            'output_ids': current_ids
        }


def get_test_configurations() -> List[ProgressiveGPTConfig]:
    """è·å–æµ‹è¯•é…ç½®åˆ—è¡¨"""
    configs = [
        # å°å‹æ¨¡å‹ (~100Må‚æ•°)
        ProgressiveGPTConfig(
            n_embd=768, n_layer=12, n_head=12, n_positions=512
        ),
        # ä¸­å‹æ¨¡å‹ (~300Må‚æ•°)
        ProgressiveGPTConfig(
            n_embd=1024, n_layer=24, n_head=16, n_positions=1024
        ),
        # å¤§å‹æ¨¡å‹ (~800Må‚æ•°)
        ProgressiveGPTConfig(
            n_embd=1280, n_layer=36, n_head=20, n_positions=1024
        ),
        # è¶…å¤§å‹æ¨¡å‹ (~1.5Bå‚æ•°)
        ProgressiveGPTConfig(
            n_embd=1600, n_layer=48, n_head=25, n_positions=2048
        ),
    ]
    return configs


def run_progressive_benchmark():
    """è¿è¡Œæ¸è¿›å¼åŸºå‡†æµ‹è¯•"""
    print("\nğŸš€ å¼€å§‹æ¸è¿›å¼GPTæ¨ç†æµ‹è¯•")
    print("=" * 60)
    
    configs = get_test_configurations()
    results = []
    
    # å‡†å¤‡æµ‹è¯•è¾“å…¥
    test_input = jnp.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=jnp.int32)
    
    for i, config in enumerate(configs):
        print(f"\nğŸ“‹ æµ‹è¯•é…ç½® {i+1}/{len(configs)}")
        print(f"   åµŒå…¥ç»´åº¦: {config.n_embd}")
        print(f"   å±‚æ•°: {config.n_layer}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {config.n_head}")
        print(f"   æœ€å¤§ä½ç½®: {config.n_positions}")
        print("-" * 40)
        
        try:
            # åˆ›å»ºæ¨ç†å¼•æ“
            engine = ProgressiveInferenceEngine(config)
            
            # é¢„çƒ­
            warmup_time = engine.warmup(test_input)
            
            # å‰å‘ä¼ æ’­åŸºå‡†æµ‹è¯•
            forward_results = engine.benchmark_forward_pass(test_input, num_runs=5)
            
            # æ–‡æœ¬ç”ŸæˆåŸºå‡†æµ‹è¯•
            generation_results = engine.benchmark_generation(test_input, max_new_tokens=16)
            
            # æ”¶é›†ç»“æœ
            result = {
                'config': {
                    'n_embd': config.n_embd,
                    'n_layer': config.n_layer,
                    'n_head': config.n_head,
                    'n_positions': config.n_positions,
                    'estimated_params': config.get_param_count()
                },
                'warmup_time': warmup_time,
                'forward_pass': forward_results,
                'generation': generation_results
            }
            results.append(result)
            
            # æ‰“å°ç»“æœæ‘˜è¦
            print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
            print(f"   é¢„çƒ­æ—¶é—´: {warmup_time:.2f}s")
            print(f"   å‰å‘ä¼ æ’­: {forward_results['mean_time']*1000:.1f}ms Â± {forward_results['std_time']*1000:.1f}ms")
            print(f"   ç”Ÿæˆååé‡: {generation_results['throughput']:.1f} tokens/s")
            print(f"   æ¯tokenæ—¶é—´: {generation_results['mean_token_time']*1000:.1f}ms")
            
        except Exception as e:
            print(f"âŒ é…ç½® {i+1} æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            break  # å¦‚æœå½“å‰é…ç½®å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•æ›´å¤§çš„é…ç½®
    
    # ä¿å­˜ç»“æœ
    if results:
        results_file = Path("progressive_benchmark_results.json")
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ‰“å°æœ€ç»ˆæ‘˜è¦
        print(f"\nğŸ¯ æ¸è¿›å¼æµ‹è¯•å®Œæˆ!")
        print(f"   æˆåŠŸæµ‹è¯•é…ç½®æ•°: {len(results)}")
        print(f"   æœ€å¤§å‚æ•°é‡: {max(r['config']['estimated_params'] for r in results)/1e6:.1f}M")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ¸è¿›å¼GPT-1.5B JAXæ¨ç†æµ‹è¯•ç³»ç»Ÿ")
    print("=" * 60)
    print(f"ğŸ“¦ JAXç‰ˆæœ¬: {jax.__version__}")
    print(f"ğŸ–¥ï¸ è®¾å¤‡æ•°é‡: {len(jax.devices())}")
    
    for i, device in enumerate(jax.devices()):
        print(f"   è®¾å¤‡ {i}: {device}")
    
    try:
        results = run_progressive_benchmark()
        if results:
            print("\nâœ… æ¸è¿›å¼æµ‹è¯•å®Œæˆï¼")
            print("ğŸ’¡ æŸ¥çœ‹ progressive_benchmark_results.json è·å–è¯¦ç»†ç»“æœ")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
