#!/usr/bin/env python3
"""
GPT-1.5Bæ¨ç†DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰å¯è§†åŒ–
è·å–å¹¶ç»˜åˆ¶å®Œæ•´çš„è®¡ç®—å›¾ç»“æ„ï¼ŒåŒ…æ‹¬åˆ†ç‰‡ä¿¡æ¯å’Œæ•°æ®æµ
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Set
from dataclasses import dataclass
from functools import partial

# è®¾ç½®JAXç¯å¢ƒ
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

try:
    import jax
    import jax.numpy as jnp
    from jax import random, devices, make_jaxpr
    from jax.sharding import Mesh, PartitionSpec, NamedSharding
    from jax.experimental import mesh_utils
    import flax.linen as nn
    import numpy as np
    print(f"âœ… JAX {jax.__version__} DAGå¯è§†åŒ–æ¨¡å¼")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

@dataclass
class GPTConfig:
    """GPT-1.5Bé…ç½®"""
    vocab_size: int = 50257
    n_positions: int = 2048
    n_embd: int = 1600
    n_layer: int = 48
    n_head: int = 25
    dropout: float = 0.0
    use_bias: bool = True

class GPTLayer(nn.Module):
    """å•ä¸ªGPT Transformerå±‚"""
    config: GPTConfig
    
    def setup(self):
        self.ln_1 = nn.LayerNorm()
        self.attn = MultiHeadAttention(self.config)
        self.ln_2 = nn.LayerNorm()
        self.mlp = MLP(self.config)
    
    def __call__(self, x, mask=None):
        # æ®‹å·®è¿æ¥ + æ³¨æ„åŠ›
        x = x + self.attn(self.ln_1(x), mask)
        # æ®‹å·®è¿æ¥ + MLP
        x = x + self.mlp(self.ln_2(x))
        return x

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›"""
    config: GPTConfig
    
    def setup(self):
        self.c_attn = nn.Dense(3 * self.config.n_embd, use_bias=self.config.use_bias)
        self.c_proj = nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)
        self.n_head = self.config.n_head
        self.n_embd = self.config.n_embd
        
    def __call__(self, x, mask=None):
        B, T, C = x.shape
        
        # QKVæŠ•å½±
        qkv = self.c_attn(x)
        qkv = qkv.reshape(B, T, 3, self.n_head, C // self.n_head)
        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]
        
        # æ³¨æ„åŠ›è®¡ç®—
        q = q.transpose(0, 2, 1, 3)  # (B, n_head, T, head_dim)
        k = k.transpose(0, 2, 1, 3)
        v = v.transpose(0, 2, 1, 3)
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        att = (q @ k.transpose(0, 1, 3, 2)) * (1.0 / jnp.sqrt(k.shape[-1]))
        
        if mask is not None:
            att = jnp.where(mask, att, -jnp.inf)
        
        att = jax.nn.softmax(att, axis=-1)
        y = att @ v  # (B, n_head, T, head_dim)
        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)
        
        return self.c_proj(y)

class MLP(nn.Module):
    """MLPå±‚"""
    config: GPTConfig
    
    def setup(self):
        self.c_fc = nn.Dense(4 * self.config.n_embd, use_bias=self.config.use_bias)
        self.c_proj = nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)
    
    def __call__(self, x):
        x = self.c_fc(x)
        x = jax.nn.gelu(x)
        return self.c_proj(x)

class SimplifiedGPT(nn.Module):
    """ç®€åŒ–çš„GPTæ¨¡å‹ç”¨äºDAGå¯è§†åŒ–"""
    config: GPTConfig
    
    def setup(self):
        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)
        self.wpe = nn.Embed(self.config.n_positions, self.config.n_embd)
        # åªä½¿ç”¨å‰å‡ å±‚è¿›è¡Œæ¼”ç¤º
        self.layers = [GPTLayer(self.config) for _ in range(4)]  # ç®€åŒ–ä¸º4å±‚
        self.ln_f = nn.LayerNorm()
        self.lm_head = nn.Dense(self.config.vocab_size, use_bias=False)
    
    def __call__(self, input_ids):
        B, T = input_ids.shape
        
        # Token embedding
        token_emb = self.wte(input_ids)
        
        # Position embedding
        pos = jnp.arange(0, T)[None, :]
        pos_emb = self.wpe(pos)
        
        # Combined embedding
        x = token_emb + pos_emb
        
        # Transformer layers
        for layer in self.layers:
            x = layer(x, mask=None)
        
        # Final layer norm
        x = self.ln_f(x)
        
        # Language modeling head
        logits = self.lm_head(x)
        
        return logits

class DAGNode:
    """DAGèŠ‚ç‚¹"""
    def __init__(self, name: str, op_type: str, shape: Tuple[int, ...], 
                 sharding: Optional[str] = None, device: Optional[str] = None):
        self.name = name
        self.op_type = op_type
        self.shape = shape
        self.sharding = sharding
        self.device = device
        self.inputs: List['DAGNode'] = []
        self.outputs: List['DAGNode'] = []
        self.computation_cost = 0
        self.memory_cost = 0

class DAGVisualizer:
    """DAGå¯è§†åŒ–å™¨"""
    
    def __init__(self, config: GPTConfig):
        self.config = config
        self.devices = jax.devices()
        self.mesh = None
        self.nodes: Dict[str, DAGNode] = {}
        self.edges: List[Tuple[str, str]] = []
        
    def create_mesh(self):
        """åˆ›å»ºè®¾å¤‡ç½‘æ ¼"""
        if len(self.devices) >= 4:
            devices_array = np.array(self.devices[:4]).reshape(2, 2)
            self.mesh = Mesh(devices_array, axis_names=('data', 'model'))
            print(f"âœ… åˆ›å»º2x2è®¾å¤‡ç½‘æ ¼")
        elif len(self.devices) >= 2:
            devices_array = np.array(self.devices[:2]).reshape(2, 1)
            self.mesh = Mesh(devices_array, axis_names=('data', 'model'))
            print(f"âœ… åˆ›å»º2x1è®¾å¤‡ç½‘æ ¼")
        else:
            devices_array = np.array(self.devices).reshape(1, 1)
            self.mesh = Mesh(devices_array, axis_names=('data',))
            print(f"âœ… åˆ›å»ºå•è®¾å¤‡ç½‘æ ¼")
    
    def extract_dag_from_jaxpr(self):
        """ä»JAXè¡¨è¾¾å¼æå–DAG"""
        print(f"\nğŸ” æå–è®¡ç®—å›¾DAG")
        print("-" * 50)
        
        # åˆ›å»ºæ¨¡å‹
        model = SimplifiedGPT(self.config)
        
        # åˆ›å»ºè¾“å…¥æ•°æ®
        key = jax.random.PRNGKey(42)
        input_ids = jax.random.randint(key, (2, 128), 0, self.config.vocab_size)
        
        # åˆå§‹åŒ–å‚æ•°
        params = model.init(key, input_ids)
          # åˆ›å»ºæ¨ç†å‡½æ•°
        def inference_fn(params, input_ids):
            return model.apply(params, input_ids)
        
        # è·å–JAXè¡¨è¾¾å¼ï¼ˆè®¡ç®—å›¾ï¼‰
        closed_jaxpr = make_jaxpr(inference_fn)(params, input_ids)
        jaxpr = closed_jaxpr.jaxpr  # ä»ClosedJaxprä¸­è·å–å®é™…çš„Jaxpr
        
        print(f"ğŸ“Š è®¡ç®—å›¾ç»Ÿè®¡:")
        print(f"   è¾“å…¥: {input_ids.shape}")
        print(f"   åŸå§‹æ–¹ç¨‹æ•°é‡: {len(jaxpr.eqns)}")
        print(f"   è¾“å…¥å˜é‡æ•°é‡: {len(jaxpr.invars)}")
        print(f"   è¾“å‡ºå˜é‡æ•°é‡: {len(jaxpr.outvars)}")
        
        # è§£æJAXPRæ„å»ºDAG
        self._parse_jaxpr_to_dag(jaxpr, input_ids.shape)
        
        return closed_jaxpr
    
    def _parse_jaxpr_to_dag(self, jaxpr, input_shape):
        """è§£æJAXPRæ„å»ºDAG"""
        print(f"\nğŸ”§ è§£æJAXPRæ„å»ºDAG")
        print("-" * 30)
        
        # åˆ›å»ºè¾“å…¥èŠ‚ç‚¹
        input_node = DAGNode(
            name="input_ids",
            op_type="input",
            shape=input_shape,
            sharding="PartitionSpec('data', None)"
        )
        self.nodes["input_ids"] = input_node
        
        # è§£ææ¯ä¸ªæ–¹ç¨‹
        node_counter = 0
        var_to_node = {}
        
        for i, eqn in enumerate(jaxpr.eqns):
            node_name = f"op_{i}_{eqn.primitive.name}"
            
            # ä¼°ç®—è¾“å‡ºå½¢çŠ¶ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            if hasattr(eqn, 'outvars') and eqn.outvars:
                # è¿™é‡Œæ˜¯ç®€åŒ–çš„å½¢çŠ¶æ¨æ–­
                if 'dot_general' in eqn.primitive.name:
                    output_shape = self._estimate_matmul_shape(eqn)
                elif 'add' in eqn.primitive.name:
                    output_shape = input_shape  # ç®€åŒ–å‡è®¾
                elif 'reshape' in eqn.primitive.name:
                    output_shape = self._estimate_reshape_shape(eqn)
                else:
                    output_shape = input_shape  # é»˜è®¤
            else:
                output_shape = input_shape
            
            # ç¡®å®šåˆ†ç‰‡ç­–ç•¥
            sharding = self._determine_sharding(eqn.primitive.name, output_shape)
            
            # åˆ›å»ºèŠ‚ç‚¹
            node = DAGNode(
                name=node_name,
                op_type=eqn.primitive.name,
                shape=output_shape,
                sharding=sharding
            )
            
            # è®¡ç®—æˆæœ¬ä¼°ç®—
            node.computation_cost = self._estimate_computation_cost(eqn.primitive.name, output_shape)
            node.memory_cost = np.prod(output_shape) * 4  # float32
            
            self.nodes[node_name] = node
            node_counter += 1
            
            # ç®€åŒ–çš„ä¾èµ–å…³ç³»ï¼ˆå®é™…å®ç°ä¼šæ›´å¤æ‚ï¼‰
            if i > 0:
                prev_node_name = f"op_{i-1}_{jaxpr.eqns[i-1].primitive.name}"
                if prev_node_name in self.nodes:
                    self.edges.append((prev_node_name, node_name))
        
        print(f"   åˆ›å»ºèŠ‚ç‚¹æ•°é‡: {len(self.nodes)}")
        print(f"   åˆ›å»ºè¾¹æ•°é‡: {len(self.edges)}")
    
    def _estimate_matmul_shape(self, eqn):
        """ä¼°ç®—çŸ©é˜µä¹˜æ³•è¾“å‡ºå½¢çŠ¶"""
        # ç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦è§£ædimension_numbers
        return (2, 128, self.config.n_embd)
    
    def _estimate_reshape_shape(self, eqn):
        """ä¼°ç®—reshapeè¾“å‡ºå½¢çŠ¶"""
        # ç®€åŒ–å¤„ç†
        return (2, 128, self.config.n_embd)
    
    def _determine_sharding(self, op_name: str, shape: Tuple[int, ...]):
        """ç¡®å®šæ“ä½œçš„åˆ†ç‰‡ç­–ç•¥"""
        if 'dot_general' in op_name:
            return "PartitionSpec('data', None, 'model')"
        elif 'add' in op_name:
            return "PartitionSpec('data', None, None)"
        elif len(shape) >= 2:
            return "PartitionSpec('data', None)"
        else:
            return "PartitionSpec()"
    
    def _estimate_computation_cost(self, op_name: str, shape: Tuple[int, ...]):
        """ä¼°ç®—è®¡ç®—æˆæœ¬ï¼ˆFLOPSï¼‰"""
        size = np.prod(shape)
        
        if 'dot_general' in op_name:
            return size * self.config.n_embd  # ç®€åŒ–çš„çŸ©é˜µä¹˜æ³•æˆæœ¬
        elif 'add' in op_name:
            return size
        elif 'mul' in op_name:
            return size
        elif 'exp' in op_name or 'log' in op_name:
            return size * 10  # æŒ‡æ•°/å¯¹æ•°æ“ä½œæ›´æ˜‚è´µ
        else:
            return size
    
    def visualize_dag_structure(self):
        """å¯è§†åŒ–DAGç»“æ„"""
        print(f"\nğŸ¨ DAGç»“æ„å¯è§†åŒ–")
        print("=" * 80)
        
        print(f"ğŸ“Š èŠ‚ç‚¹è¯¦æƒ…:")
        for name, node in self.nodes.items():
            print(f"   {name}:")
            print(f"     ç±»å‹: {node.op_type}")
            print(f"     å½¢çŠ¶: {node.shape}")
            print(f"     åˆ†ç‰‡: {node.sharding}")
            print(f"     è®¡ç®—æˆæœ¬: {node.computation_cost:,} FLOPs")
            print(f"     å†…å­˜æˆæœ¬: {node.memory_cost/1024/1024:.2f} MB")
            print()
        
        print(f"ğŸ”— è¾¹è¿æ¥å…³ç³»:")
        for i, (src, dst) in enumerate(self.edges):
            print(f"   {i+1}. {src} â†’ {dst}")
        
        # åˆ›å»ºç®€åŒ–çš„ASCIIå›¾
        self._draw_ascii_dag()
    
    def _draw_ascii_dag(self):
        """ç»˜åˆ¶ASCIIæ ¼å¼çš„DAG"""
        print(f"\nğŸ“ˆ ç®€åŒ–DAGæµç¨‹å›¾:")
        print("=" * 60)
        
        # æŒ‰å±‚çº§ç»„ç»‡èŠ‚ç‚¹
        levels = self._compute_dag_levels()
        
        for level, nodes in enumerate(levels):
            print(f"Level {level}:")
            for node_name in nodes:
                node = self.nodes[node_name]
                print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                print(f"  â”‚ {node.op_type:<19} â”‚")
                print(f"  â”‚ {str(node.shape):<19} â”‚")
                print(f"  â”‚ {node.sharding[:19]:<19} â”‚")
                print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
                
                # æ˜¾ç¤ºè¿æ¥
                if level < len(levels) - 1:
                    print(f"           â”‚")
                    print(f"           â–¼")
            print()
    
    def _compute_dag_levels(self):
        """è®¡ç®—DAGçš„å±‚çº§ç»“æ„"""
        # ç®€åŒ–çš„å±‚çº§è®¡ç®—
        levels = []
        
        # ç¬¬0å±‚ï¼šè¾“å…¥
        levels.append(['input_ids'])
        
        # å…¶ä»–å±‚ï¼šæŒ‰æ“ä½œé¡ºåºç®€å•åˆ†ç»„
        remaining_nodes = [name for name in self.nodes.keys() if name != 'input_ids']
        
        # ç®€å•åˆ†ç»„ï¼ˆå®é™…å®ç°éœ€è¦æ‹“æ‰‘æ’åºï¼‰
        chunk_size = max(1, len(remaining_nodes) // 4)
        for i in range(0, len(remaining_nodes), chunk_size):
            chunk = remaining_nodes[i:i+chunk_size]
            if chunk:
                levels.append(chunk)
        
        return levels
    
    def analyze_dag_properties(self):
        """åˆ†æDAGå±æ€§"""
        print(f"\nğŸ“Š DAGå±æ€§åˆ†æ")
        print("=" * 50)
        
        total_nodes = len(self.nodes)
        total_edges = len(self.edges)
        total_computation = sum(node.computation_cost for node in self.nodes.values())
        total_memory = sum(node.memory_cost for node in self.nodes.values())
        
        print(f"ğŸ”¢ å›¾ç»Ÿè®¡:")
        print(f"   èŠ‚ç‚¹æ€»æ•°: {total_nodes}")
        print(f"   è¾¹æ€»æ•°: {total_edges}")
        print(f"   å›¾å¯†åº¦: {total_edges / (total_nodes * (total_nodes - 1)) * 100:.2f}%")
        
        print(f"\nâš¡ è®¡ç®—ç»Ÿè®¡:")
        print(f"   æ€»è®¡ç®—é‡: {total_computation:,} FLOPs")
        print(f"   æ€»å†…å­˜éœ€æ±‚: {total_memory/1024/1024:.2f} MB")
        print(f"   å¹³å‡èŠ‚ç‚¹è®¡ç®—é‡: {total_computation/total_nodes:,.0f} FLOPs")
        
        # æ‰¾å‡ºå…³é”®è·¯å¾„
        critical_nodes = self._find_critical_nodes()
        print(f"\nğŸ¯ å…³é”®èŠ‚ç‚¹ (é«˜è®¡ç®—æˆæœ¬):")
        for node_name in critical_nodes[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            node = self.nodes[node_name]
            print(f"   {node_name}: {node.computation_cost:,} FLOPs")
        
        # åˆ†ç‰‡åˆ†æ
        self._analyze_sharding_distribution()
    
    def _find_critical_nodes(self):
        """æ‰¾å‡ºè®¡ç®—æˆæœ¬æœ€é«˜çš„èŠ‚ç‚¹"""
        return sorted(self.nodes.keys(), 
                     key=lambda name: self.nodes[name].computation_cost, 
                     reverse=True)
    
    def _analyze_sharding_distribution(self):
        """åˆ†æåˆ†ç‰‡ç­–ç•¥åˆ†å¸ƒ"""
        print(f"\nğŸ”€ åˆ†ç‰‡ç­–ç•¥åˆ†æ:")
        
        sharding_counts = {}
        for node in self.nodes.values():
            sharding = node.sharding or "æ— åˆ†ç‰‡"
            sharding_counts[sharding] = sharding_counts.get(sharding, 0) + 1
        
        for sharding, count in sharding_counts.items():
            percentage = (count / len(self.nodes)) * 100
            print(f"   {sharding}: {count} ä¸ªèŠ‚ç‚¹ ({percentage:.1f}%)")
    
    def demonstrate_parallel_execution(self):
        """æ¼”ç¤ºå¹¶è¡Œæ‰§è¡Œ"""
        print(f"\nğŸš€ å¹¶è¡Œæ‰§è¡Œæ¼”ç¤º")
        print("=" * 50)
        
        if not self.mesh:
            print("âš ï¸ æœªåˆ›å»ºmeshï¼Œæ— æ³•æ¼”ç¤ºå¹¶è¡Œæ‰§è¡Œ")
            return
        
        # åˆ›å»ºå®é™…çš„åˆ†ç‰‡æ¨ç†
        model = SimplifiedGPT(self.config)
        key = jax.random.PRNGKey(42)
        input_ids = jax.random.randint(key, (4, 128), 0, self.config.vocab_size)
        
        # åˆå§‹åŒ–å‚æ•°
        params = model.init(key, input_ids)
        
        with self.mesh:
            # å®šä¹‰åˆ†ç‰‡ç­–ç•¥
            input_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
            
            # åˆ†ç‰‡è¾“å…¥
            input_ids_sharded = jax.device_put(input_ids, input_sharding)
            
            # JITç¼–è¯‘æ¨ç†å‡½æ•°
            @jax.jit
            def sharded_inference(params, input_ids):
                return model.apply(params, input_ids)
            
            # æ‰§è¡Œæ¨ç†
            print(f"ğŸ¬ æ‰§è¡Œåˆ†ç‰‡æ¨ç†...")
            start_time = time.time()
            logits = sharded_inference(params, input_ids_sharded)
            jax.block_until_ready(logits)
            end_time = time.time()
            
            print(f"âœ… æ¨ç†å®Œæˆ:")
            print(f"   è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
            print(f"   è¾“å‡ºå½¢çŠ¶: {logits.shape}")
            print(f"   æ‰§è¡Œæ—¶é—´: {(end_time - start_time)*1000:.2f}ms")
            print(f"   ä½¿ç”¨è®¾å¤‡: {len(self.devices)} ä¸ªGPU")
            
            # åˆ†æå¹¶è¡Œæ•ˆç‡
            total_computation = sum(node.computation_cost for node in self.nodes.values())
            if total_computation > 0:
                sequential_estimate = total_computation / 1e12  # å‡è®¾1T FLOPS/s
                parallel_efficiency = sequential_estimate / (end_time - start_time) / len(self.devices)
                
                print(f"\nğŸ“ˆ å¹¶è¡Œæ•ˆç‡åˆ†æ:")
                print(f"   ä¼°ç®—ä¸²è¡Œæ—¶é—´: {sequential_estimate*1000:.2f}ms")
                print(f"   å®é™…å¹¶è¡Œæ—¶é—´: {(end_time - start_time)*1000:.2f}ms")
                print(f"   å¹¶è¡Œæ•ˆç‡: {parallel_efficiency*100:.1f}%")
            else:
                print(f"\nğŸ“ˆ å¹¶è¡Œæ•ˆç‡åˆ†æ:")
                print(f"   å®é™…å¹¶è¡Œæ—¶é—´: {(end_time - start_time)*1000:.2f}ms")
                print(f"   è®¡ç®—æˆæœ¬ä¿¡æ¯ä¸å¯ç”¨ï¼Œæ— æ³•ä¼°ç®—ç†è®ºåŠ é€Ÿæ¯”")
    
    def export_dag_data(self):
        """å¯¼å‡ºDAGæ•°æ®"""
        print(f"\nğŸ’¾ å¯¼å‡ºDAGæ•°æ®")
        print("-" * 30)
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        dag_data = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'config': {
                'vocab_size': self.config.vocab_size,
                'n_embd': self.config.n_embd,
                'n_layer': self.config.n_layer,
                'n_head': self.config.n_head
            },
            'mesh_info': {
                'shape': list(self.mesh.shape) if self.mesh else None,
                'axis_names': list(self.mesh.axis_names) if self.mesh else None,
                'device_count': len(self.devices)
            },
            'nodes': {},
            'edges': self.edges,            'statistics': {
                'total_nodes': len(self.nodes),
                'total_edges': len(self.edges),
                'total_computation': int(sum(node.computation_cost for node in self.nodes.values())),
                'total_memory': int(sum(node.memory_cost for node in self.nodes.values()))
            }
        }
          # å¯¼å‡ºèŠ‚ç‚¹ä¿¡æ¯
        for name, node in self.nodes.items():
            dag_data['nodes'][name] = {
                'op_type': node.op_type,
                'shape': [int(x) for x in node.shape],  # è½¬æ¢ä¸ºPython int
                'sharding': node.sharding,
                'computation_cost': int(node.computation_cost),  # è½¬æ¢ä¸ºPython int
                'memory_cost': int(node.memory_cost)  # è½¬æ¢ä¸ºPython int
            }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        dag_file = Path("gpt15b_dag_analysis.json")
        with open(dag_file, 'w', encoding='utf-8') as f:
            json.dump(dag_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… DAGæ•°æ®å·²ä¿å­˜: {dag_file}")
        
        # åˆ›å»ºDOTæ ¼å¼å›¾æ–‡ä»¶
        self._export_dot_format()
        
        return dag_data
    
    def _export_dot_format(self):
        """å¯¼å‡ºDOTæ ¼å¼å›¾æ–‡ä»¶"""
        dot_file = Path("gpt15b_dag.dot")
        
        with open(dot_file, 'w', encoding='utf-8') as f:
            f.write("digraph GPT15B_DAG {\n")
            f.write("  rankdir=TB;\n")
            f.write("  node [shape=box, style=filled];\n\n")
            
            # å†™å…¥èŠ‚ç‚¹
            for name, node in self.nodes.items():
                color = self._get_node_color(node.op_type)
                label = f"{node.op_type}\\n{node.shape}\\n{node.sharding or ''}"
                f.write(f'  "{name}" [label="{label}", fillcolor="{color}"];\n')
            
            f.write("\n")
            
            # å†™å…¥è¾¹
            for src, dst in self.edges:
                f.write(f'  "{src}" -> "{dst}";\n')
            
            f.write("}\n")
        
        print(f"âœ… DOTå›¾æ–‡ä»¶å·²ä¿å­˜: {dot_file}")
        print(f"   ä½¿ç”¨Graphvizå¯è§†åŒ–: dot -Tpng {dot_file} -o gpt15b_dag.png")
    
    def _get_node_color(self, op_type: str) -> str:
        """æ ¹æ®æ“ä½œç±»å‹è·å–èŠ‚ç‚¹é¢œè‰²"""
        color_map = {
            'input': 'lightgreen',
            'dot_general': 'lightblue',
            'add': 'lightyellow',
            'mul': 'lightcoral',
            'reshape': 'lightgray',
            'transpose': 'lightpink',
            'exp': 'orange',
            'log': 'purple'
        }
        return color_map.get(op_type, 'white')
    
    def comprehensive_dag_analysis(self):
        """å®Œæ•´çš„DAGåˆ†æ"""
        print(f"ğŸ¯ GPT-1.5Bæ¨ç†DAGå®Œæ•´åˆ†æ")
        print("=" * 80)
        
        # 1. åˆ›å»ºè®¾å¤‡ç½‘æ ¼
        self.create_mesh()
        
        # 2. æå–è®¡ç®—å›¾
        jaxpr = self.extract_dag_from_jaxpr()
        
        # 3. å¯è§†åŒ–DAGç»“æ„
        self.visualize_dag_structure()
        
        # 4. åˆ†æDAGå±æ€§
        self.analyze_dag_properties()
        
        # 5. æ¼”ç¤ºå¹¶è¡Œæ‰§è¡Œ
        self.demonstrate_parallel_execution()
        
        # 6. å¯¼å‡ºDAGæ•°æ®
        dag_data = self.export_dag_data()
        
        # 7. æ€»ç»“
        print(f"\nğŸ‰ DAGåˆ†æå®Œæˆ!")
        print("=" * 60)
        print(f"âœ… å…³é”®æˆæœ:")
        print(f"   â€¢ æˆåŠŸæå–GPT-1.5Bæ¨ç†è®¡ç®—å›¾")
        print(f"   â€¢ ç”Ÿæˆ{len(self.nodes)}ä¸ªè®¡ç®—èŠ‚ç‚¹")
        print(f"   â€¢ è¯†åˆ«{len(self.edges)}ä¸ªæ•°æ®ä¾èµ–å…³ç³»")
        print(f"   â€¢ åˆ†æåˆ†ç‰‡ç­–ç•¥å’Œå¹¶è¡Œæœºä¼š")
        print(f"   â€¢ å¯¼å‡ºDOTæ ¼å¼å›¾æ–‡ä»¶ç”¨äºå¯è§†åŒ–")
        
        print(f"\nğŸ’¡ å¯è§†åŒ–å»ºè®®:")
        print(f"   â€¢ å®‰è£…Graphviz: https://graphviz.org/")
        print(f"   â€¢ ç”Ÿæˆå›¾ç‰‡: dot -Tpng gpt15b_dag.dot -o gpt15b_dag.png")
        print(f"   â€¢ æˆ–ä½¿ç”¨åœ¨çº¿å·¥å…·: http://magjac.com/graphviz-visual-editor/")

def main():
    """ä¸»å‡½æ•°"""
    config = GPTConfig()
    visualizer = DAGVisualizer(config)
    
    try:
        visualizer.comprehensive_dag_analysis()
    except Exception as e:
        print(f"âŒ DAGåˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
