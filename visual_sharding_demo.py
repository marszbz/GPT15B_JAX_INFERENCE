#!/usr/bin/env python3
"""
JAXè‡ªåŠ¨åˆ†ç‰‡å’Œå›¾åˆ’åˆ†å¯è§†åŒ–æ¼”ç¤º
è¯¦ç»†å±•ç¤ºåˆ†ç‰‡æœºåˆ¶çš„å·¥ä½œåŸç†
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass

# è®¾ç½®JAXç¯å¢ƒ
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

try:
    import jax
    import jax.numpy as jnp
    from jax import random, devices
    from jax.sharding import Mesh, PartitionSpec, NamedSharding
    from jax.experimental import mesh_utils
    import flax.linen as nn
    import numpy as np
    print(f"âœ… JAX {jax.__version__} å¯è§†åŒ–æ¼”ç¤ºæ¨¡å¼")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

class ShardingVisualizer:
    """åˆ†ç‰‡å¯è§†åŒ–å™¨"""
    
    def __init__(self):
        self.devices = jax.devices()
        self.mesh = None
        
    def step1_understand_tensors(self):
        """ç¬¬1æ­¥ï¼šç†è§£å¼ é‡å’Œåˆ†ç‰‡çš„åŸºæœ¬æ¦‚å¿µ"""
        print(f"\n" + "="*60)
        print(f"ğŸ¯ ç¬¬1æ­¥ï¼šç†è§£å¼ é‡åˆ†ç‰‡çš„åŸºæœ¬æ¦‚å¿µ")
        print(f"="*60)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„çŸ©é˜µ
        key = jax.random.PRNGKey(42)
        matrix = jax.random.normal(key, (8, 8))
        
        print(f"ğŸ“Š åŸå§‹çŸ©é˜µ (8x8):")
        print(f"   å½¢çŠ¶: {matrix.shape}")
        print(f"   æ•°æ®ç±»å‹: {matrix.dtype}")
        print(f"   æ€»å…ƒç´ æ•°: {matrix.size}")
        print(f"   å†…å­˜å¤§å°: {matrix.nbytes} bytes")
        
        # æ˜¾ç¤ºçŸ©é˜µå†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        print(f"\nğŸ“‹ çŸ©é˜µå†…å®¹ï¼ˆå‰4x4ï¼‰:")
        for i in range(4):
            row_str = "   "
            for j in range(4):
                row_str += f"{matrix[i,j]:.2f} "
            print(row_str + "...")
        print("   ...")
        
        print(f"\nğŸ’¡ åˆ†ç‰‡æ¦‚å¿µ:")
        print(f"   åˆ†ç‰‡ = å°†å¤§çŸ©é˜µåˆ†å‰²æˆå°å—ï¼Œåˆ†å¸ƒåˆ°ä¸åŒGPU")
        print(f"   ç›®çš„ = çªç ´å•GPUå†…å­˜é™åˆ¶ï¼Œå®ç°å¹¶è¡Œè®¡ç®—")
        print(f"   æ–¹å¼ = æŒ‰è¡Œåˆ†ç‰‡ã€æŒ‰åˆ—åˆ†ç‰‡ã€æŒ‰å—åˆ†ç‰‡")
        
        return matrix
    
    def step2_create_mesh_visualization(self):
        """ç¬¬2æ­¥ï¼šå¯è§†åŒ–è®¾å¤‡ç½‘æ ¼åˆ›å»º"""
        print(f"\n" + "="*60)
        print(f"ğŸ”§ ç¬¬2æ­¥ï¼šè®¾å¤‡ç½‘æ ¼åˆ›å»ºè¿‡ç¨‹")
        print(f"="*60)
        
        print(f"ğŸ“± å¯ç”¨è®¾å¤‡:")
        for i, device in enumerate(self.devices):
            print(f"   GPU {i}: {device}")
        
        # åˆ›å»º2x2ç½‘æ ¼
        if len(self.devices) >= 4:
            devices_array = np.array(self.devices[:4]).reshape(2, 2)
            self.mesh = Mesh(devices_array, axis_names=('data', 'model'))
            
            print(f"\nğŸ•¸ï¸ åˆ›å»º2x2è®¾å¤‡ç½‘æ ¼:")
            print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print(f"   â”‚ GPU 0   â”‚ GPU 1   â”‚  â† dataè½´ ç¬¬0è¡Œ")
            print(f"   â”‚ {self.mesh.devices[0,0]} â”‚ {self.mesh.devices[0,1]} â”‚")
            print(f"   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
            print(f"   â”‚ GPU 2   â”‚ GPU 3   â”‚  â† dataè½´ ç¬¬1è¡Œ")
            print(f"   â”‚ {self.mesh.devices[1,0]} â”‚ {self.mesh.devices[1,1]} â”‚")
            print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            print(f"      â†‘         â†‘")
            print(f"   modelè½´    modelè½´")
            print(f"    ç¬¬0åˆ—      ç¬¬1åˆ—")
            
            print(f"\nğŸ“ ç½‘æ ¼é…ç½®:")
            print(f"   å½¢çŠ¶: {dict(self.mesh.shape)}")
            print(f"   è½´åç§°: {self.mesh.axis_names}")
            print(f"   dataè½´: ç”¨äºæ•°æ®å¹¶è¡Œ (batchç»´åº¦åˆ†ç‰‡)")
            print(f"   modelè½´: ç”¨äºæ¨¡å‹å¹¶è¡Œ (å‚æ•°ç»´åº¦åˆ†ç‰‡)")
            
        elif len(self.devices) == 1:
            devices_array = np.array(self.devices).reshape(1, 1)
            self.mesh = Mesh(devices_array, axis_names=('data',))
            print(f"\nğŸ•¸ï¸ å•è®¾å¤‡ç½‘æ ¼:")
            print(f"   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            print(f"   â”‚ GPU 0   â”‚")
            print(f"   â”‚ {self.mesh.devices[0]} â”‚")
            print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
        return self.mesh
    
    def step3_partition_spec_explanation(self):
        """ç¬¬3æ­¥ï¼šè¯¦è§£PartitionSpecåˆ†ç‰‡è§„èŒƒ"""
        print(f"\n" + "="*60)
        print(f"ğŸ“‹ ç¬¬3æ­¥ï¼šPartitionSpecåˆ†ç‰‡è§„èŒƒè¯¦è§£")
        print(f"="*60)
        
        if not self.mesh:
            print("âš ï¸ éœ€è¦å…ˆåˆ›å»ºmesh")
            return
        
        print(f"ğŸ” PartitionSpecè¯­æ³•è§£æ:")
        print(f"   PartitionSpec(axis0, axis1, axis2, ...)")
        print(f"   - None: ä¸åˆ†ç‰‡ï¼Œå¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡")
        print(f"   - 'data': æ²¿dataè½´åˆ†ç‰‡")
        print(f"   - 'model': æ²¿modelè½´åˆ†ç‰‡")
        
        # æ¼”ç¤ºä¸åŒçš„åˆ†ç‰‡ç­–ç•¥
        examples = [
            {
                'name': 'ä¸åˆ†ç‰‡',
                'spec': PartitionSpec(),
                'description': 'å¼ é‡å¤åˆ¶åˆ°æ‰€æœ‰è®¾å¤‡',
                'use_case': 'LayerNormå‚æ•°ã€biasç­‰å°å‚æ•°'
            },
            {
                'name': 'batchåˆ†ç‰‡',
                'spec': PartitionSpec('data', None),
                'description': 'ç¬¬0ç»´(batch)æ²¿dataè½´åˆ†ç‰‡',
                'use_case': 'è¾“å…¥æ•°æ®ã€æ¿€æ´»å€¼'
            },
            {
                'name': 'å‚æ•°åˆ†ç‰‡',
                'spec': PartitionSpec('model', None),
                'description': 'ç¬¬0ç»´æ²¿modelè½´åˆ†ç‰‡',
                'use_case': 'åµŒå…¥å±‚æƒé‡ã€è¾“å‡ºå±‚æƒé‡'
            },
            {
                'name': 'æ³¨æ„åŠ›å¤´åˆ†ç‰‡',
                'spec': PartitionSpec(None, 'model'),
                'description': 'ç¬¬1ç»´æ²¿modelè½´åˆ†ç‰‡',
                'use_case': 'QKVæŠ•å½±æƒé‡'
            },
            {
                'name': 'äºŒç»´åˆ†ç‰‡',
                'spec': PartitionSpec('data', 'model'),
                'description': 'ç¬¬0ç»´æ²¿dataè½´ï¼Œç¬¬1ç»´æ²¿modelè½´åˆ†ç‰‡',
                'use_case': 'å¤§å‹çŸ©é˜µçš„å—åˆ†ç‰‡'
            }
        ]
        
        for i, example in enumerate(examples, 1):
            print(f"\n   {i}. {example['name']}:")
            print(f"      è§„èŒƒ: {example['spec']}")
            print(f"      å«ä¹‰: {example['description']}")
            print(f"      ç”¨é€”: {example['use_case']}")
        
        return examples
    
    def step4_visual_sharding_demo(self):
        """ç¬¬4æ­¥ï¼šå¯è§†åŒ–åˆ†ç‰‡æ¼”ç¤º"""
        print(f"\n" + "="*60)
        print(f"ğŸ¬ ç¬¬4æ­¥ï¼šå®é™…åˆ†ç‰‡è¿‡ç¨‹å¯è§†åŒ–")
        print(f"="*60)
        
        if not self.mesh:
            print("âš ï¸ éœ€è¦å…ˆåˆ›å»ºmesh")
            return
        
        # åˆ›å»ºæµ‹è¯•çŸ©é˜µ
        key = jax.random.PRNGKey(42)
        
        # æ¼”ç¤º1ï¼šbatchåˆ†ç‰‡
        print(f"\nğŸ“Š æ¼”ç¤º1ï¼šBatchåˆ†ç‰‡")
        print(f"-" * 30)
        
        batch_data = jax.random.normal(key, (8, 4))  # 8ä¸ªbatchï¼Œ4ä¸ªç‰¹å¾
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {batch_data.shape}")
        
        with self.mesh:
            # batchç»´åº¦åˆ†ç‰‡
            batch_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
            sharded_batch = jax.device_put(batch_data, batch_sharding)
            
            print(f"åˆ†ç‰‡ç­–ç•¥: PartitionSpec('data', None)")
            print(f"åˆ†ç‰‡ç»“æœ:")
            
            if len(self.devices) >= 4:
                print(f"   GPU 0,2: batch 0-3 (shape: 4x4)")
                print(f"   GPU 1,3: batch 4-7 (shape: 4x4)")
                print(f"   â”‚ batch0 batch1 batch2 batch3 â”‚ â†’ GPU 0,2")
                print(f"   â”‚ batch4 batch5 batch6 batch7 â”‚ â†’ GPU 1,3")
            
        # æ¼”ç¤º2ï¼šå‚æ•°åˆ†ç‰‡
        print(f"\nğŸ“Š æ¼”ç¤º2ï¼šå‚æ•°åˆ†ç‰‡")
        print(f"-" * 30)
        
        weight_matrix = jax.random.normal(key, (8, 8))
        print(f"æƒé‡çŸ©é˜µå½¢çŠ¶: {weight_matrix.shape}")
        
        with self.mesh:
            # ç¬¬0ç»´åˆ†ç‰‡
            weight_sharding = NamedSharding(self.mesh, PartitionSpec('model', None))
            sharded_weight = jax.device_put(weight_matrix, weight_sharding)
            
            print(f"åˆ†ç‰‡ç­–ç•¥: PartitionSpec('model', None)")
            print(f"åˆ†ç‰‡ç»“æœ:")
            
            if len(self.devices) >= 4:
                print(f"   GPU 0,1: è¡Œ 0-3 (shape: 4x8)")
                print(f"   GPU 2,3: è¡Œ 4-7 (shape: 4x8)")
                print(f"   â”Œâ”€ è¡Œ0-3 â”€â” â†’ GPU 0,1")
                print(f"   â”‚ weight  â”‚")
                print(f"   â”œâ”€ è¡Œ4-7 â”€â”¤ â†’ GPU 2,3")
                print(f"   â”‚ weight  â”‚")
                print(f"   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        # æ¼”ç¤º3ï¼šæ³¨æ„åŠ›å¤´åˆ†ç‰‡
        print(f"\nğŸ“Š æ¼”ç¤º3ï¼šæ³¨æ„åŠ›å¤´åˆ†ç‰‡")
        print(f"-" * 30)
        
        qkv_weight = jax.random.normal(key, (8, 12))  # 8ä¸ªè¾“å…¥ï¼Œ12ä¸ªè¾“å‡º(3*4å¤´)
        print(f"QKVæƒé‡å½¢çŠ¶: {qkv_weight.shape}")
        
        with self.mesh:
            # ç¬¬1ç»´åˆ†ç‰‡ï¼ˆæ³¨æ„åŠ›å¤´ï¼‰
            qkv_sharding = NamedSharding(self.mesh, PartitionSpec(None, 'model'))
            sharded_qkv = jax.device_put(qkv_weight, qkv_sharding)
            
            print(f"åˆ†ç‰‡ç­–ç•¥: PartitionSpec(None, 'model')")
            print(f"åˆ†ç‰‡ç»“æœ:")
            
            if len(self.devices) >= 4:
                print(f"   GPU 0,2: è¾“å‡º 0-5 (Q,Kçš„ä¸€åŠ)")
                print(f"   GPU 1,3: è¾“å‡º 6-11 (K,Vçš„ä¸€åŠ)")
                print(f"   â”Œâ”€Qå¤´â”€â”¬â”€Kå¤´â”€â” â”Œâ”€Kå¤´â”€â”¬â”€Vå¤´â”€â”")
                print(f"   â”‚ 0-5 â”‚ GPU0,2 â”‚ 6-11â”‚ GPU1,3 â”‚")
                print(f"   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜")
        
        return sharded_batch, sharded_weight, sharded_qkv
    
    def step5_automatic_sharding_mechanism(self):
        """ç¬¬5æ­¥ï¼šè‡ªåŠ¨åˆ†ç‰‡æœºåˆ¶æ·±åº¦è§£æ"""
        print(f"\n" + "="*60)
        print(f"ğŸ¤– ç¬¬5æ­¥ï¼šJAXè‡ªåŠ¨åˆ†ç‰‡æœºåˆ¶è§£æ")
        print(f"="*60)
        
        print(f"ğŸ§  JAXè‡ªåŠ¨åˆ†ç‰‡çš„å·¥ä½œåŸç†:")
        print(f"\n1ï¸âƒ£ ç¼–è¯‘æ—¶åˆ†æ:")
        print(f"   - æ‰«æè®¡ç®—å›¾ä¸­çš„æ‰€æœ‰æ“ä½œ")
        print(f"   - åˆ†æå¼ é‡çš„å½¢çŠ¶å’Œä½¿ç”¨æ¨¡å¼")
        print(f"   - æ ¹æ®PartitionSpecæ¨æ–­æœ€ä¼˜åˆ†ç‰‡")
        
        print(f"\n2ï¸âƒ£ åˆ†ç‰‡æ¨æ–­:")
        print(f"   - è¾“å…¥åˆ†ç‰‡ â†’ è‡ªåŠ¨æ¨æ–­è¾“å‡ºåˆ†ç‰‡")
        print(f"   - è€ƒè™‘æ“ä½œè¯­ä¹‰ï¼ˆçŸ©é˜µä¹˜æ³•ã€å…ƒç´ wiseç­‰ï¼‰")
        print(f"   - æœ€å°åŒ–è®¾å¤‡é—´é€šä¿¡")
        
        print(f"\n3ï¸âƒ£ é€šä¿¡æ’å…¥:")
        print(f"   - æ£€æµ‹åˆ†ç‰‡ä¸åŒ¹é…çš„åœ°æ–¹")
        print(f"   - è‡ªåŠ¨æ’å…¥reshape/é€šä¿¡æ“ä½œ")
        print(f"   - ä¼˜åŒ–é€šä¿¡æ¨¡å¼ï¼ˆAllReduceã€AllGatherç­‰ï¼‰")
        
        print(f"\n4ï¸âƒ£ ä»£ç ç”Ÿæˆ:")
        print(f"   - ç”Ÿæˆæ¯ä¸ªè®¾å¤‡çš„æœ¬åœ°è®¡ç®—ä»£ç ")
        print(f"   - æ’å…¥å¿…è¦çš„åŒæ­¥ç‚¹")
        print(f"   - ä¼˜åŒ–å†…å­˜å¸ƒå±€")
        
        # å®é™…æ¼”ç¤ºè‡ªåŠ¨åˆ†ç‰‡æ¨æ–­
        if self.mesh:
            self._demonstrate_automatic_inference()
    
    def _demonstrate_automatic_inference(self):
        """æ¼”ç¤ºè‡ªåŠ¨æ¨æ–­è¿‡ç¨‹"""
        print(f"\nğŸ” è‡ªåŠ¨æ¨æ–­æ¼”ç¤º:")
        print(f"-" * 30)
        
        key = jax.random.PRNGKey(42)
        
        with self.mesh:
            # å®šä¹‰ä¸€ä¸ªç®€å•çš„è®¡ç®—
            def matrix_multiply(x, w):
                return jnp.dot(x, w)
            
            # åˆ›å»ºè¾“å…¥
            x = jax.random.normal(key, (4, 8))  # batch=4, features=8
            w = jax.random.normal(key, (8, 16))  # features=8, hidden=16
            
            # å®šä¹‰è¾“å…¥åˆ†ç‰‡
            x_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
            w_sharding = NamedSharding(self.mesh, PartitionSpec(None, 'model'))
            
            # åˆ†ç‰‡è¾“å…¥
            x_sharded = jax.device_put(x, x_sharding)
            w_sharded = jax.device_put(w, w_sharding)
            
            print(f"è¾“å…¥å¼ é‡:")
            print(f"   x: {x.shape} â†’ åˆ†ç‰‡ç­–ç•¥: ('data', None)")
            print(f"   w: {w.shape} â†’ åˆ†ç‰‡ç­–ç•¥: (None, 'model')")
            
            # æ‰§è¡Œè®¡ç®—
            result = matrix_multiply(x_sharded, w_sharded)
            
            print(f"\nè®¡ç®—: y = x @ w")
            print(f"è¾“å‡ºå¼ é‡:")
            print(f"   y: {result.shape}")
            print(f"   è‡ªåŠ¨æ¨æ–­çš„åˆ†ç‰‡: ('data', 'model')")
            
            print(f"\nğŸ¯ æ¨æ–­é€»è¾‘:")
            print(f"   x(4,8) @ w(8,16) = y(4,16)")
            print(f"   xåˆ†ç‰‡: (data, None)")
            print(f"   wåˆ†ç‰‡: (None, model)")
            print(f"   â†“ çŸ©é˜µä¹˜æ³•è§„åˆ™")
            print(f"   yåˆ†ç‰‡: (data, model) â† è‡ªåŠ¨æ¨æ–­!")
            
            print(f"\nğŸ“¡ é€šä¿¡åˆ†æ:")
            print(f"   - xçš„ç¬¬1ç»´ä¸wçš„ç¬¬0ç»´æ”¶ç¼© â†’ éœ€è¦AllReduce")
            print(f"   - ç»“æœä¿æŒxçš„ç¬¬0ç»´åˆ†ç‰‡(data)")
            print(f"   - ç»“æœä¿æŒwçš„ç¬¬1ç»´åˆ†ç‰‡(model)")
    
    def step6_graph_partitioning_visualization(self):
        """ç¬¬6æ­¥ï¼šå›¾åˆ’åˆ†å¯è§†åŒ–"""
        print(f"\n" + "="*60)
        print(f"ğŸ•¸ï¸ ç¬¬6æ­¥ï¼šè®¡ç®—å›¾åˆ’åˆ†è¿‡ç¨‹")
        print(f"="*60)
        
        print(f"ğŸ“Š è®¡ç®—å›¾åˆ’åˆ†çš„å±‚æ¬¡:")
        print(f"\n1ï¸âƒ£ æ“ä½œçº§åˆ’åˆ†:")
        print(f"   åŸå§‹å›¾: A â†’ B â†’ C â†’ D")
        print(f"   åˆ’åˆ†å: ")
        print(f"   GPU0: Aâ‚€ â†’ Bâ‚€ â†’ Câ‚€ â†’ Dâ‚€")
        print(f"   GPU1: Aâ‚ â†’ Bâ‚ â†’ Câ‚ â†’ Dâ‚")
        print(f"   GPU2: Aâ‚‚ â†’ Bâ‚‚ â†’ Câ‚‚ â†’ Dâ‚‚")
        print(f"   GPU3: Aâ‚ƒ â†’ Bâ‚ƒ â†’ Câ‚ƒ â†’ Dâ‚ƒ")
        
        print(f"\n2ï¸âƒ£ æ•°æ®æµåˆ’åˆ†:")
        print(f"   è¾“å…¥æ•°æ® â†’ åˆ†ç‰‡åˆ°å„GPU")
        print(f"   è®¡ç®—è¿‡ç¨‹ â†’ å¹¶è¡Œæ‰§è¡Œ")
        print(f"   ä¸­é—´ç»“æœ â†’ æ ¹æ®éœ€è¦é€šä¿¡")
        print(f"   æœ€ç»ˆè¾“å‡º â†’ èšåˆæˆ–ä¿æŒåˆ†ç‰‡")
        
        print(f"\n3ï¸âƒ£ é€šä¿¡æ¨¡å¼:")
        
        # ASCIIè‰ºæœ¯å±•ç¤ºé€šä¿¡æ¨¡å¼
        print(f"   AllReduce (å…¨å½’çº¦):")
        print(f"   GPU0 â†â†’ GPU1")
        print(f"    â†•    â†—â†™ â†•")
        print(f"   GPU2 â†â†’ GPU3")
        
        print(f"\n   AllGather (å…¨æ”¶é›†):")
        print(f"   GPU0 â†’ æ”¶é›†æ‰€æœ‰ç‰‡æ®µ")
        print(f"   GPU1 â†’ æ”¶é›†æ‰€æœ‰ç‰‡æ®µ")
        print(f"   GPU2 â†’ æ”¶é›†æ‰€æœ‰ç‰‡æ®µ")
        print(f"   GPU3 â†’ æ”¶é›†æ‰€æœ‰ç‰‡æ®µ")
        
        print(f"\n   Scatter (åˆ†æ•£):")
        print(f"   ä¸»GPU â†’ åˆ†å‘åˆ°å„GPU")
        print(f"   GPU0 â† ç‰‡æ®µ0")
        print(f"   GPU1 â† ç‰‡æ®µ1")
        print(f"   GPU2 â† ç‰‡æ®µ2")
        print(f"   GPU3 â† ç‰‡æ®µ3")
        
        # å®é™…æ¼”ç¤ºå›¾åˆ’åˆ†
        if self.mesh:
            self._demonstrate_graph_partitioning()
    
    def _demonstrate_graph_partitioning(self):
        """æ¼”ç¤ºå®é™…çš„å›¾åˆ’åˆ†"""
        print(f"\nğŸ¬ å®é™…å›¾åˆ’åˆ†æ¼”ç¤º:")
        print(f"-" * 30)
        
        # å®šä¹‰ä¸€ä¸ªå¤šæ­¥éª¤è®¡ç®—
        def multi_step_computation(x, w1, w2):
            """å¤šæ­¥éª¤è®¡ç®—ï¼šçº¿æ€§ â†’ æ¿€æ´» â†’ çº¿æ€§"""
            h1 = jnp.dot(x, w1)  # ç¬¬ä¸€ä¸ªçº¿æ€§å±‚
            h2 = jax.nn.relu(h1)  # æ¿€æ´»å‡½æ•°
            y = jnp.dot(h2, w2)   # ç¬¬äºŒä¸ªçº¿æ€§å±‚
            return y
        
        # JITç¼–è¯‘ä»¥è§‚å¯Ÿå›¾åˆ’åˆ†
        jit_computation = jax.jit(multi_step_computation)
        
        key = jax.random.PRNGKey(42)
        
        with self.mesh:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            x = jax.random.normal(key, (8, 16))   # batch=8, input=16
            w1 = jax.random.normal(key, (16, 32)) # input=16, hidden=32
            w2 = jax.random.normal(key, (32, 8))  # hidden=32, output=8
            
            # å®šä¹‰åˆ†ç‰‡ç­–ç•¥
            x_sharding = NamedSharding(self.mesh, PartitionSpec('data', None))
            w1_sharding = NamedSharding(self.mesh, PartitionSpec(None, 'model'))
            w2_sharding = NamedSharding(self.mesh, PartitionSpec('model', None))
            
            # åº”ç”¨åˆ†ç‰‡
            x_sharded = jax.device_put(x, x_sharding)
            w1_sharded = jax.device_put(w1, w1_sharding)
            w2_sharded = jax.device_put(w2, w2_sharding)
            
            print(f"è®¡ç®—å›¾ç»“æ„:")
            print(f"   x(8,16) â†’ Linear1 â†’ ReLU â†’ Linear2 â†’ y(8,8)")
            print(f"   â†“åˆ†ç‰‡")
            print(f"   x: ('data', None)")
            print(f"   w1: (None, 'model')")
            print(f"   w2: ('model', None)")
            
            # æ‰§è¡Œè®¡ç®—
            result = jit_computation(x_sharded, w1_sharded, w2_sharded)
            
            print(f"\nå›¾åˆ’åˆ†ç»“æœ:")
            print(f"   æ­¥éª¤1: x @ w1 â†’ h1('data', 'model')")
            print(f"   æ­¥éª¤2: ReLU(h1) â†’ h2('data', 'model')")
            print(f"   æ­¥éª¤3: h2 @ w2 â†’ y('data', None)")
            
            print(f"\né€šä¿¡å¼€é”€:")
            print(f"   æ­¥éª¤1â†’2: æ— é€šä¿¡ (åˆ†ç‰‡å…¼å®¹)")
            print(f"   æ­¥éª¤2â†’3: AllReduce (modelç»´åº¦å½’çº¦)")
            print(f"   æ€»é€šä¿¡: æœ€å°åŒ–!")
    
    def step7_performance_implications(self):
        """ç¬¬7æ­¥ï¼šæ€§èƒ½å½±å“åˆ†æ"""
        print(f"\n" + "="*60)
        print(f"âš¡ ç¬¬7æ­¥ï¼šåˆ†ç‰‡ç­–ç•¥å¯¹æ€§èƒ½çš„å½±å“")
        print(f"="*60)
        
        print(f"ğŸ“ˆ æ€§èƒ½å› ç´ åˆ†æ:")
        
        print(f"\n1ï¸âƒ£ è®¡ç®—å¹¶è¡Œåº¦:")
        print(f"   æ•°æ®å¹¶è¡Œ: çº¿æ€§åŠ é€Ÿ (ç†æƒ³æƒ…å†µ)")
        print(f"   æ¨¡å‹å¹¶è¡Œ: å—é€šä¿¡é™åˆ¶")
        print(f"   æ··åˆå¹¶è¡Œ: å¹³è¡¡è®¡ç®—å’Œé€šä¿¡")
        
        print(f"\n2ï¸âƒ£ å†…å­˜æ•ˆç‡:")
        print(f"   ä¸åˆ†ç‰‡: å†…å­˜å¤åˆ¶ï¼Œè®¡ç®—å¹¶è¡Œ")
        print(f"   åˆ†ç‰‡: å†…å­˜èŠ‚çœï¼Œå¯èƒ½å¢åŠ é€šä¿¡")
        print(f"   ä¼˜åŒ–ç›®æ ‡: æœ€å¤§åŒ–å†…å­˜åˆ©ç”¨ç‡")
        
        print(f"\n3ï¸âƒ£ é€šä¿¡å¼€é”€:")
        print(f"   AllReduce: O(æ•°æ®é‡)")
        print(f"   AllGather: O(æ•°æ®é‡ Ã— è®¾å¤‡æ•°)")
        print(f"   ç‚¹å¯¹ç‚¹: O(æ•°æ®é‡/è®¾å¤‡æ•°)")
        
        print(f"\n4ï¸âƒ£ è´Ÿè½½å‡è¡¡:")
        print(f"   ç†æƒ³: æ¯ä¸ªè®¾å¤‡è®¡ç®—é‡ç›¸ç­‰")
        print(f"   ç°å®: å¯èƒ½å­˜åœ¨ä¸å¹³è¡¡")
        print(f"   è§£å†³: åŠ¨æ€åˆ†ç‰‡è°ƒæ•´")
        
        # å®é™…æ€§èƒ½æµ‹è¯•
        if self.mesh:
            self._performance_comparison()
    
    def _performance_comparison(self):
        """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
        print(f"\nğŸ æ€§èƒ½å¯¹æ¯”æµ‹è¯•:")
        print(f"-" * 30)
        
        key = jax.random.PRNGKey(42)
        
        # æµ‹è¯•ä¸åŒåˆ†ç‰‡ç­–ç•¥çš„æ€§èƒ½
        def simple_computation(x, w):
            return jnp.dot(x, w)
        
        jit_comp = jax.jit(simple_computation)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = jax.random.normal(key, (64, 512))
        w = jax.random.normal(key, (512, 2048))
        
        with self.mesh:
            # ç­–ç•¥1: ä¸åˆ†ç‰‡
            x_replicated = jax.device_put(x, NamedSharding(self.mesh, PartitionSpec()))
            w_replicated = jax.device_put(w, NamedSharding(self.mesh, PartitionSpec()))
            
            # ç­–ç•¥2: batchåˆ†ç‰‡
            x_data_sharded = jax.device_put(x, NamedSharding(self.mesh, PartitionSpec('data', None)))
            w_replicated2 = jax.device_put(w, NamedSharding(self.mesh, PartitionSpec()))
            
            # ç­–ç•¥3: æ¨¡å‹åˆ†ç‰‡
            x_replicated3 = jax.device_put(x, NamedSharding(self.mesh, PartitionSpec()))
            w_model_sharded = jax.device_put(w, NamedSharding(self.mesh, PartitionSpec(None, 'model')))
            
            print(f"æµ‹è¯•é…ç½®: x{x.shape} @ w{w.shape}")
            
            # é¢„çƒ­
            for strategy_name, x_test, w_test in [
                ("å¤åˆ¶ç­–ç•¥", x_replicated, w_replicated),
                ("æ•°æ®åˆ†ç‰‡", x_data_sharded, w_replicated2),
                ("æ¨¡å‹åˆ†ç‰‡", x_replicated3, w_model_sharded)
            ]:
                for _ in range(3):
                    result = jit_comp(x_test, w_test)
                    jax.block_until_ready(result)
                
                # è®¡æ—¶
                times = []
                for _ in range(5):
                    start = time.time()
                    result = jit_comp(x_test, w_test)
                    jax.block_until_ready(result)
                    times.append(time.time() - start)
                
                avg_time = np.mean(times) * 1000
                print(f"   {strategy_name}: {avg_time:.2f}ms")
    
    def comprehensive_demo(self):
        """å®Œæ•´æ¼”ç¤ºæµç¨‹"""
        print(f"ğŸ¯ JAXè‡ªåŠ¨åˆ†ç‰‡å’Œå›¾åˆ’åˆ†å®Œæ•´æ¼”ç¤º")
        print(f"="*60)
        print(f"ğŸ“š æœ¬æ¼”ç¤ºå°†è¯¦ç»†å±•ç¤º:")
        print(f"   1. å¼ é‡åˆ†ç‰‡çš„åŸºæœ¬æ¦‚å¿µ")
        print(f"   2. è®¾å¤‡ç½‘æ ¼çš„åˆ›å»ºè¿‡ç¨‹")
        print(f"   3. PartitionSpecåˆ†ç‰‡è§„èŒƒ")
        print(f"   4. å®é™…åˆ†ç‰‡è¿‡ç¨‹å¯è§†åŒ–")
        print(f"   5. è‡ªåŠ¨åˆ†ç‰‡æœºåˆ¶è§£æ")
        print(f"   6. è®¡ç®—å›¾åˆ’åˆ†è¿‡ç¨‹")
        print(f"   7. æ€§èƒ½å½±å“åˆ†æ")
        
        # é€æ­¥æ‰§è¡Œæ¼”ç¤º
        matrix = self.step1_understand_tensors()
        mesh = self.step2_create_mesh_visualization()
        examples = self.step3_partition_spec_explanation()
        sharded_tensors = self.step4_visual_sharding_demo()
        self.step5_automatic_sharding_mechanism()
        self.step6_graph_partitioning_visualization()
        self.step7_performance_implications()
        
        # æ€»ç»“
        print(f"\n" + "="*60)
        print(f"ğŸ“ æ€»ç»“ï¼šJAXåˆ†ç‰‡æœºåˆ¶çš„æ ¸å¿ƒè¦ç‚¹")
        print(f"="*60)
        print(f"âœ… å…³é”®æ¦‚å¿µ:")
        print(f"   â€¢ è®¾å¤‡ç½‘æ ¼: å°†GPUç»„ç»‡ä¸ºé€»è¾‘ç½‘æ ¼")
        print(f"   â€¢ PartitionSpec: æŒ‡å®šå¼ é‡å¦‚ä½•åˆ†ç‰‡")
        print(f"   â€¢ è‡ªåŠ¨æ¨æ–­: JAXè‡ªåŠ¨æ¨æ–­æœ€ä¼˜åˆ†ç‰‡")
        print(f"   â€¢ é€šä¿¡ä¼˜åŒ–: æœ€å°åŒ–è®¾å¤‡é—´æ•°æ®ä¼ è¾“")
        
        print(f"\nğŸ”§ æŠ€æœ¯ä¼˜åŠ¿:")
        print(f"   â€¢ é€æ˜æ€§: ç”¨æˆ·åªéœ€æŒ‡å®šåˆ†ç‰‡ç­–ç•¥")
        print(f"   â€¢ è‡ªåŠ¨åŒ–: ç¼–è¯‘å™¨å¤„ç†æ‰€æœ‰ç»†èŠ‚")
        print(f"   â€¢ é«˜æ•ˆæ€§: æ™ºèƒ½é€šä¿¡ä¼˜åŒ–")
        print(f"   â€¢ çµæ´»æ€§: æ”¯æŒå¤šç§å¹¶è¡Œæ¨¡å¼")
        
        print(f"\nğŸ’¡ æœ€ä½³å®è·µ:")
        print(f"   â€¢ æ ¹æ®æ¨¡å‹å¤§å°é€‰æ‹©åˆ†ç‰‡ç­–ç•¥")
        print(f"   â€¢ å¹³è¡¡è®¡ç®—å’Œé€šä¿¡å¼€é”€")
        print(f"   â€¢ åˆ©ç”¨æ··åˆå¹¶è¡Œè·å¾—æœ€ä½³æ€§èƒ½")
        print(f"   â€¢ æµ‹è¯•ä¸åŒç­–ç•¥æ‰¾åˆ°æœ€ä¼˜é…ç½®")

def main():
    """ä¸»å‡½æ•°"""
    visualizer = ShardingVisualizer()
    visualizer.comprehensive_demo()

if __name__ == "__main__":
    main()
