#!/usr/bin/env python3
"""
GPT-1.5B JAXæ¨ç†æ€§èƒ½æµ‹è¯• - ç‹¬ç«‹è¿è¡Œè„šæœ¬
è§£å†³æ‰€æœ‰å¯¼å…¥å’Œç¯å¢ƒé—®é¢˜
"""

import os
import sys
import argparse
import time
import json
from pathlib import Path

# è®¾ç½®JAXç¯å¢ƒï¼ˆå¿…é¡»åœ¨å¯¼å…¥JAXä¹‹å‰ï¼‰
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8'
# æ¸…ç†å¯èƒ½å­˜åœ¨çš„XLA_FLAGS
if 'XLA_FLAGS' in os.environ:
    del os.environ['XLA_FLAGS']

# å¯¼å…¥JAXç›¸å…³åŒ…
try:
    import jax
    import jax.numpy as jnp
    import flax.linen as nn
    import numpy as np
    print(f"âœ… JAX {jax.__version__} åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"âŒ JAXå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œï¼špip install jax==0.6.1 jaxlib==0.6.1+cuda118 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html")
    sys.exit(1)

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from functools import partial


@dataclass
class GPTConfig:
    """GPT-1.5Bæ¨¡å‹é…ç½®"""
    vocab_size: int = 50257
    n_positions: int = 2048
    n_embd: int = 1600
    n_layer: int = 48
    n_head: int = 25
    dropout: float = 0.1
    use_bias: bool = True


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, x, training: bool = False):
        B, T, C = x.shape
        head_dim = C // self.config.n_head
        
        # QKVæŠ•å½±
        qkv = nn.Dense(3 * C, use_bias=self.config.use_bias)(x)
        q, k, v = jnp.split(qkv, 3, axis=-1)
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        q = q.reshape(B, T, self.config.n_head, head_dim).transpose(0, 2, 1, 3)
        k = k.reshape(B, T, self.config.n_head, head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(B, T, self.config.n_head, head_dim).transpose(0, 2, 1, 3)
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scale = 1.0 / jnp.sqrt(head_dim)
        attn_weights = jnp.matmul(q, k.transpose(0, 1, 3, 2)) * scale
        
        # å› æœæ©ç 
        mask = jnp.tril(jnp.ones((T, T)))
        attn_weights = jnp.where(mask, attn_weights, -1e10)
        attn_weights = jax.nn.softmax(attn_weights, axis=-1)
        
        # Dropout
        if training:
            attn_weights = nn.Dropout(rate=self.config.dropout)(
                attn_weights, deterministic=not training
            )
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        out = jnp.matmul(attn_weights, v)
        out = out.transpose(0, 2, 1, 3).reshape(B, T, C)
        
        # è¾“å‡ºæŠ•å½±
        out = nn.Dense(C, use_bias=self.config.use_bias)(out)
        if training:
            out = nn.Dropout(rate=self.config.dropout)(out, deterministic=not training)
        
        return out


class MLP(nn.Module):
    """å‰é¦ˆç¥ç»ç½‘ç»œ"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, x, training: bool = False):
        # ç¬¬ä¸€å±‚ï¼šå‡ç»´åˆ°4*n_embd
        x = nn.Dense(4 * self.config.n_embd, use_bias=self.config.use_bias)(x)
        x = jax.nn.gelu(x)
        
        # ç¬¬äºŒå±‚ï¼šé™ç»´å›n_embd
        x = nn.Dense(self.config.n_embd, use_bias=self.config.use_bias)(x)
        
        if training:
            x = nn.Dropout(rate=self.config.dropout)(x, deterministic=not training)
        
        return x


class TransformerBlock(nn.Module):
    """Transformerè§£ç å™¨å—"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, x, training: bool = False):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        attn_out = MultiHeadAttention(self.config)(x, training)
        x = nn.LayerNorm()(x + attn_out)
        
        # MLP + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        mlp_out = MLP(self.config)(x, training)
        x = nn.LayerNorm()(x + mlp_out)
        
        return x


class GPTModel(nn.Module):
    """GPT-1.5Bä¸»æ¨¡å‹"""
    config: GPTConfig
    
    @nn.compact
    def __call__(self, input_ids, training: bool = False):
        B, T = input_ids.shape
        
        # TokenåµŒå…¥
        token_embed = nn.Embed(self.config.vocab_size, self.config.n_embd)(input_ids)
        
        # ä½ç½®åµŒå…¥
        pos_embed = nn.Embed(self.config.n_positions, self.config.n_embd)(
            jnp.arange(T)[None, :]
        )
        
        # ç»„åˆåµŒå…¥
        x = token_embed + pos_embed
        
        if training:
            x = nn.Dropout(rate=self.config.dropout)(x, deterministic=not training)
        
        # 48ä¸ªTransformerå—
        for _ in range(self.config.n_layer):
            x = TransformerBlock(self.config)(x, training)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        x = nn.LayerNorm()(x)
        
        # è¯­è¨€æ¨¡å‹å¤´
        logits = nn.Dense(self.config.vocab_size, use_bias=False)(x)
        
        return logits


class GraphPartitionedGPT:
    """å›¾åˆ†å‰²GPTæ¨ç†å¼•æ“"""
    
    def __init__(self, config: GPTConfig):
        self.config = config
        self.devices = jax.devices()
        self.num_devices = len(self.devices)
        
        print(f"ğŸ”§ åˆå§‹åŒ–å›¾åˆ†å‰²GPTæ¨ç†å¼•æ“")
        print(f"   GPUæ•°é‡: {self.num_devices}")
        for i, device in enumerate(self.devices):
            print(f"   GPU {i}: {device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = GPTModel(config)
        self._init_model_parameters()
        
        # è®¾ç½®å¤šGPUåˆ†ç‰‡
        if self.num_devices > 1:
            self._setup_graph_partitioning()
        else:
            print("â„¹ï¸ å•GPUæ¨¡å¼")
            self.sharded_params = self.params
    
    def _init_model_parameters(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        print("ğŸ”„ åˆå§‹åŒ–GPT-1.5Bæ¨¡å‹å‚æ•°...")
        
        key = jax.random.PRNGKey(42)
        dummy_input = jnp.ones((1, 128), dtype=jnp.int32)
        
        # åˆå§‹åŒ–å‚æ•°
        self.params = self.model.init(key, dummy_input, training=False)
        
        # è®¡ç®—å‚æ•°é‡
        param_count = sum(x.size for x in jax.tree_util.tree_leaves(self.params))
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {param_count:,} ({param_count/1e9:.2f}B)")
    
    def _setup_graph_partitioning(self):
        """è®¾ç½®å›¾åˆ†å‰²å’Œå‚æ•°åˆ†ç‰‡"""
        print("ğŸ•¸ï¸ è®¾ç½®å›¾åˆ†å‰²å’Œå¤šGPUå¹¶è¡Œ...")
        
        # åˆ›å»ºè®¾å¤‡ç½‘æ ¼
        self.mesh = jax.make_mesh((self.num_devices,), ('model',))
        
        # å®šä¹‰åˆ†ç‰‡ç­–ç•¥
        def get_partition_spec(param):
            """ä¸ºä¸åŒå‚æ•°å®šä¹‰åˆ†ç‰‡ç­–ç•¥"""
            if param.ndim >= 2:
                # å¤§çš„æƒé‡çŸ©é˜µæŒ‰ç¬¬ä¸€ä¸ªç»´åº¦åˆ†ç‰‡
                if param.shape[0] >= 512:
                    return jax.sharding.PartitionSpec('model', None)
                elif param.shape[1] >= 512:
                    return jax.sharding.PartitionSpec(None, 'model')
                else:
                    return jax.sharding.PartitionSpec()
            else:
                # 1Då‚æ•°ä¸åˆ†ç‰‡
                return jax.sharding.PartitionSpec()
        
        # åº”ç”¨åˆ†ç‰‡è§„èŒƒ
        self.param_spec = jax.tree_util.tree_map(get_partition_spec, self.params)
        
        # åˆ›å»ºåˆ†ç‰‡å¹¶åˆ†å¸ƒå‚æ•°
        sharding = jax.sharding.NamedSharding(self.mesh, self.param_spec)
        self.sharded_params = jax.device_put(self.params, sharding)
        
        print(f"âœ… å›¾åˆ†å‰²å®Œæˆï¼Œå‚æ•°å·²åˆ†å¸ƒåˆ° {self.num_devices} ä¸ªGPU")
    
    @partial(jax.jit, static_argnums=(0,))
    def forward_pass(self, params, input_ids):
        """JITç¼–è¯‘çš„å‰å‘ä¼ æ’­"""
        return self.model.apply(params, input_ids, training=False)
    
    @partial(jax.jit, static_argnums=(0,))
    def generate_next_token(self, params, input_ids):
        """ç”Ÿæˆä¸‹ä¸€ä¸ªtokenï¼ˆJITç¼–è¯‘ï¼‰"""
        logits = self.forward_pass(params, input_ids)
        # è´ªå©ªè§£ç ï¼šé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token
        next_token = jnp.argmax(logits[:, -1, :], axis=-1, keepdims=True)
        return next_token
    
    def generate_text(self, input_ids: jnp.ndarray, max_new_tokens: int = 32) -> jnp.ndarray:
        """è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ"""
        current_ids = input_ids
        params = self.sharded_params
        
        for step in range(max_new_tokens):
            # é˜²æ­¢åºåˆ—è¿‡é•¿
            if current_ids.shape[1] >= self.config.n_positions:
                # æˆªæ–­ä¿ç•™æœ€æ–°çš„éƒ¨åˆ†
                current_ids = current_ids[:, -(self.config.n_positions-1):]
            
            # ç”Ÿæˆä¸‹ä¸€ä¸ªtoken
            next_token = self.generate_next_token(params, current_ids)
            
            # æ‹¼æ¥æ–°token
            current_ids = jnp.concatenate([current_ids, next_token], axis=1)
        
        return current_ids


class SimpleTokenizer:
    """ç®€åŒ–çš„æ–‡æœ¬åˆ†è¯å™¨"""
    
    def __init__(self, vocab_size: int = 50257):
        self.vocab_size = vocab_size
        self.pad_token_id = 0
    
    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå­—ç¬¦çš„ç¼–ç 
        tokens = []
        for char in text.lower():
            token_id = min(ord(char), self.vocab_size - 1)
            tokens.append(token_id)
        
        # å¤„ç†é•¿åº¦é™åˆ¶
        if max_length:
            if len(tokens) > max_length:
                tokens = tokens[:max_length]
            else:
                # å¡«å……åˆ°æŒ‡å®šé•¿åº¦
                tokens.extend([self.pad_token_id] * (max_length - len(tokens)))
        
        return tokens
    
    def decode(self, tokens: List[int]) -> str:
        """è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬"""
        chars = []
        for token_id in tokens:
            if token_id != self.pad_token_id and token_id > 0:
                chars.append(chr(min(token_id, 127)))  # é™åˆ¶åœ¨ASCIIèŒƒå›´å†…
        return ''.join(chars)


class DatasetLoader:
    """æ•°æ®é›†åŠ è½½å™¨ - å¤„ç†JSONLæ ¼å¼æ•°æ®"""
    
    def __init__(self, dataset_dir: str):
        self.dataset_dir = Path(dataset_dir)
        self.datasets = {}
        self._load_all_datasets()
    
    def _load_all_datasets(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†é…ç½®"""
        print("ğŸ“ åŠ è½½æ•°æ®é›†...")
        
        if not self.dataset_dir.exists():
            print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {self.dataset_dir}")
            return
        
        # æŸ¥æ‰¾æ‰€æœ‰é…ç½®æ–‡ä»¶
        config_files = list(self.dataset_dir.glob("benchmark_dataset_config_*.jsonl"))
        print(f"ğŸ” æ‰¾åˆ° {len(config_files)} ä¸ªé…ç½®æ–‡ä»¶")
        
        for config_file in config_files:
            config_id = config_file.stem.split('_')[-1]
            samples = []
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
            if config_file.stat().st_size == 0:
                print(f"âš ï¸ é…ç½® {config_id} æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # è¯»å–JSONLæ–‡ä»¶
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        if line:
                            try:
                                sample = json.loads(line)
                                samples.append(sample)
                            except json.JSONDecodeError as e:
                                print(f"âš ï¸ é…ç½® {config_id} ç¬¬ {line_num} è¡ŒJSONè§£æé”™è¯¯: {e}")
                                continue
                
                if samples:
                    self.datasets[config_id] = samples
                    print(f"ğŸ“Š é…ç½® {config_id}: {len(samples)} ä¸ªæ ·æœ¬")
                    
                    # æ˜¾ç¤ºç¤ºä¾‹
                    if samples:
                        sample = samples[0]
                        print(f"   ç¤ºä¾‹: prompt_length={sample.get('prompt_length', 'N/A')}, "
                              f"generation_length={sample.get('generation_length', 'N/A')}")
                else:
                    print(f"âš ï¸ é…ç½® {config_id} æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
                    
            except Exception as e:
                print(f"âŒ åŠ è½½é…ç½® {config_id} å¤±è´¥: {e}")
    
    def get_valid_datasets(self) -> Dict[str, List[Dict]]:
        """è·å–æ‰€æœ‰æœ‰æ•ˆæ•°æ®é›†"""
        return {k: v for k, v in self.datasets.items() if v}
    
    def get_dataset_stats(self) -> Dict[str, Dict]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for config_id, samples in self.datasets.items():
            if not samples:
                continue
                
            prompt_lengths = [s.get('prompt_length', 0) for s in samples]
            gen_lengths = [s.get('generation_length', 0) for s in samples]
            
            stats[config_id] = {
                'sample_count': len(samples),
                'avg_prompt_length': np.mean(prompt_lengths) if prompt_lengths else 0,
                'avg_generation_length': np.mean(gen_lengths) if gen_lengths else 0,
                'prompt_length_range': (min(prompt_lengths), max(prompt_lengths)) if prompt_lengths else (0, 0),
                'generation_length_range': (min(gen_lengths), max(gen_lengths)) if gen_lengths else (0, 0),
                'source_types': list(set(s.get('source_type', 'unknown') for s in samples))
            }
        
        return stats
    
    def print_dataset_summary(self):
        """æ‰“å°æ•°æ®é›†æ‘˜è¦"""
        stats = self.get_dataset_stats()
        print("\nğŸ“Š æ•°æ®é›†æ‘˜è¦:")
        print("-" * 50)
        
        for config_id, stat in stats.items():
            print(f"é…ç½® {config_id}:")
            print(f"  æ ·æœ¬æ•°: {stat['sample_count']}")
            print(f"  å¹³å‡prompté•¿åº¦: {stat['avg_prompt_length']:.1f}")
            print(f"  å¹³å‡ç”Ÿæˆé•¿åº¦: {stat['avg_generation_length']:.1f}")
            print(f"  Prompté•¿åº¦èŒƒå›´: {stat['prompt_length_range']}")
            print(f"  ç”Ÿæˆé•¿åº¦èŒƒå›´: {stat['generation_length_range']}")
            print(f"  æ•°æ®æºç±»å‹: {', '.join(stat['source_types'])}")
            print()


class InferenceBenchmark:
    """æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, model: GraphPartitionedGPT):
        self.model = model
        self.tokenizer = SimpleTokenizer()
        
    def benchmark_single_sample(self, sample: Dict) -> Dict:
        """æµ‹è¯•å•ä¸ªæ ·æœ¬çš„æ¨ç†æ€§èƒ½"""
        prompt = sample['prompt']
        generation_length = sample.get('generation_length', 32)
        sample_id = sample.get('id', 'unknown')
        
        # ç¼–ç è¾“å…¥
        prompt_tokens = self.tokenizer.encode(prompt, max_length=512)
        input_ids = jnp.array([prompt_tokens])  # æ·»åŠ batchç»´åº¦
        
        # é¢„çƒ­ï¼ˆç¡®ä¿JITç¼–è¯‘å®Œæˆï¼‰
        _ = self.model.generate_text(input_ids, max_new_tokens=8)
        
        # æ­£å¼æ¨ç†è®¡æ—¶
        start_time = time.time()
        output_ids = self.model.generate_text(input_ids, max_new_tokens=generation_length)
        
        # ç¡®ä¿è®¡ç®—å®Œæˆ
        jax.block_until_ready(output_ids)
        end_time = time.time()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        inference_time = end_time - start_time
        input_length = len([t for t in prompt_tokens if t != 0])  # å»é™¤padding
        total_tokens = output_ids.shape[1]
        generated_tokens = total_tokens - input_length
        
        throughput = generated_tokens / inference_time if inference_time > 0 else 0
        
        return {
            'sample_id': sample_id,
            'input_length': input_length,
            'generated_tokens': generated_tokens,
            'total_tokens': total_tokens,
            'inference_time': inference_time,
            'throughput_tokens_per_sec': throughput,
            'latency_per_token': inference_time / generated_tokens if generated_tokens > 0 else 0
        }
    
    def benchmark_config(self, dataset: List[Dict], config_id: str, max_samples: int = 10) -> Dict:
        """æµ‹è¯•ç‰¹å®šé…ç½®çš„æ€§èƒ½"""
        if not dataset:
            return {}
        
        print(f"\nğŸ§ª æµ‹è¯•é…ç½® {config_id}")
        print(f"   æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
        print(f"   æµ‹è¯•æ ·æœ¬æ•°: {min(max_samples, len(dataset))}")
        
        results = []
        test_samples = min(max_samples, len(dataset))
        
        for i in range(test_samples):
            sample = dataset[i]
            
            try:
                result = self.benchmark_single_sample(sample)
                results.append(result)
                
                # æ¯3ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 3 == 0:
                    recent_results = results[-3:]
                    avg_time = np.mean([r['inference_time'] for r in recent_results])
                    avg_throughput = np.mean([r['throughput_tokens_per_sec'] for r in recent_results])
                    print(f"   è¿›åº¦ {i+1}/{test_samples}: å¹³å‡å»¶è¿Ÿ {avg_time:.3f}s, "
                          f"å¹³å‡ååé‡ {avg_throughput:.1f} tokens/s")
                
            except Exception as e:
                print(f"âš ï¸ æ ·æœ¬ {i} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if not results:
            return {}
        
        inference_times = [r['inference_time'] for r in results]
        throughputs = [r['throughput_tokens_per_sec'] for r in results]
        latencies = [r['latency_per_token'] for r in results]
        
        summary = {
            'config_id': config_id,
            'samples_tested': len(results),
            'avg_inference_time': np.mean(inference_times),
            'std_inference_time': np.std(inference_times),
            'avg_throughput': np.mean(throughputs),
            'max_throughput': max(throughputs),
            'min_throughput': min(throughputs),
            'avg_latency_per_token': np.mean(latencies),
            'total_tokens_generated': sum(r['generated_tokens'] for r in results),
            'total_time': sum(inference_times),
            'detailed_results': results
        }
        
        print(f"âœ… é…ç½® {config_id} æµ‹è¯•å®Œæˆ:")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {summary['avg_inference_time']:.3f}Â±{summary['std_inference_time']:.3f}s")
        print(f"   å¹³å‡ååé‡: {summary['avg_throughput']:.1f} tokens/s")
        print(f"   å¹³å‡æ¯tokenå»¶è¿Ÿ: {summary['avg_latency_per_token']:.4f}s")
        
        return summary
    
    def run_full_benchmark(self, datasets: Dict[str, List[Dict]], max_samples_per_config: int = 10) -> Dict:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print("\nğŸš€ å¼€å§‹GPT-1.5B JAXæ¨ç†æ€§èƒ½æµ‹è¯•")
        print("=" * 60)
        print(f"ğŸ“Š æ•°æ®é›†é…ç½®æ•°: {len(datasets)}")
        print(f"ğŸ”§ GPUæ•°é‡: {len(jax.devices())}")
        print(f"ğŸ“¦ JAXç‰ˆæœ¬: {jax.__version__}")
        print(f"ğŸ—ï¸ æ¨¡å‹è§„æ¨¡: {self.model.config.n_layer}å±‚, {self.model.config.n_head}å¤´, {self.model.config.n_embd}ç»´")
        
        all_results = {}
        total_start_time = time.time()
        
        # é€ä¸ªæµ‹è¯•é…ç½®
        for config_id, dataset in datasets.items():
            config_result = self.benchmark_config(dataset, config_id, max_samples_per_config)
            if config_result:
                all_results[config_id] = config_result
        
        total_time = time.time() - total_start_time
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        benchmark_summary = {
            'benchmark_info': {
                'total_execution_time': total_time,
                'configs_tested': len(all_results),
                'gpu_count': len(jax.devices()),
                'jax_version': jax.__version__,
                'model_config': {
                    'n_layer': self.model.config.n_layer,
                    'n_head': self.model.config.n_head,
                    'n_embd': self.model.config.n_embd,
                    'vocab_size': self.model.config.vocab_size
                },
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'platform': 'Windows',
                'cuda_version': '11.8'
            },
            'results': all_results
        }
        
        return benchmark_summary


def save_benchmark_results(results: Dict, output_dir: str = 'results') -> tuple:
    """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†JSONç»“æœ
    json_file = output_path / f"gpt15b_benchmark_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆå¯è¯»æ€§æŠ¥å‘Š
    report_file = output_path / f"performance_report_{timestamp}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        
        # åŸºæœ¬ä¿¡æ¯
        info = results['benchmark_info']
        f.write(f"æµ‹è¯•æ—¶é—´: {info['timestamp']}\n")
        f.write(f"æ€»æ‰§è¡Œæ—¶é—´: {info['total_execution_time']:.2f}ç§’\n")
        f.write(f"GPUæ•°é‡: {info['gpu_count']}\n")
        f.write(f"JAXç‰ˆæœ¬: {info['jax_version']}\n")
        f.write(f"CUDAç‰ˆæœ¬: {info['cuda_version']}\n")
        f.write(f"å¹³å°: {info['platform']}\n\n")
        
        # æ¨¡å‹é…ç½®
        model_cfg = info['model_config']
        f.write("æ¨¡å‹é…ç½®:\n")
        f.write(f"  å±‚æ•°: {model_cfg['n_layer']}\n")
        f.write(f"  æ³¨æ„åŠ›å¤´æ•°: {model_cfg['n_head']}\n")
        f.write(f"  åµŒå…¥ç»´åº¦: {model_cfg['n_embd']}\n")
        f.write(f"  è¯æ±‡è¡¨å¤§å°: {model_cfg['vocab_size']}\n\n")
        
        # æ€§èƒ½ç»“æœ
        f.write("æ€§èƒ½æµ‹è¯•ç»“æœ:\n")
        f.write("-" * 40 + "\n")
        
        all_throughputs = []
        all_latencies = []
        
        for config_id, result in results['results'].items():
            f.write(f"\né…ç½® {config_id}:\n")
            f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {result['samples_tested']}\n")
            f.write(f"  å¹³å‡æ¨ç†æ—¶é—´: {result['avg_inference_time']:.3f}Â±{result['std_inference_time']:.3f}s\n")
            f.write(f"  å¹³å‡ååé‡: {result['avg_throughput']:.1f} tokens/s\n")
            f.write(f"  ååé‡èŒƒå›´: {result['min_throughput']:.1f} - {result['max_throughput']:.1f} tokens/s\n")
            f.write(f"  å¹³å‡æ¯tokenå»¶è¿Ÿ: {result['avg_latency_per_token']:.4f}s\n")
            f.write(f"  æ€»ç”Ÿæˆtokenæ•°: {result['total_tokens_generated']}\n")
            
            all_throughputs.append(result['avg_throughput'])
            all_latencies.append(result['avg_latency_per_token'])
        
        # æ€»ä½“ç»Ÿè®¡
        if all_throughputs:
            f.write(f"\næ€»ä½“æ€§èƒ½ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡ååé‡: {np.mean(all_throughputs):.1f} tokens/s\n")
            f.write(f"  æœ€é«˜ååé‡: {max(all_throughputs):.1f} tokens/s\n")
            f.write(f"  å¹³å‡å»¶è¿Ÿ: {np.mean(all_latencies):.4f}s/token\n")
            f.write(f"  æœ€ä½å»¶è¿Ÿ: {min(all_latencies):.4f}s/token\n")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   è¯¦ç»†ç»“æœ: {json_file}")
    print(f"   æ€§èƒ½æŠ¥å‘Š: {report_file}")
    
    return json_file, report_file


def check_gpu_setup():
    """æ£€æŸ¥GPUè®¾ç½®"""
    devices = jax.devices()
    print(f"ğŸ” GPUç¯å¢ƒæ£€æŸ¥:")
    print(f"   æ£€æµ‹åˆ° {len(devices)} ä¸ªè®¾å¤‡")
    
    for i, device in enumerate(devices):
        print(f"   è®¾å¤‡ {i}: {device}")
    
    if len(devices) == 0:
        print("âŒ æœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼Œè¯·æ£€æŸ¥CUDAå’ŒJAXå®‰è£…")
        return False
    
    print(f"âœ… GPUè®¾ç½®æ­£å¸¸ï¼Œå…± {len(devices)} ä¸ªè®¾å¤‡å¯ç”¨")
    return True


def get_gpu_memory_info():
    """è·å–GPUå†…å­˜ä¿¡æ¯"""
    try:
        import GPUtil
        gpus = GPUtil.getGPUs()
        
        memory_info = []
        for gpu in gpus:
            info = {
                'id': gpu.id,
                'name': gpu.name,
                'memory_used': gpu.memoryUsed,
                'memory_total': gpu.memoryTotal,
                'memory_util': gpu.memoryUtil,
                'load': gpu.load,
                'temperature': gpu.temperature
            }
            memory_info.append(info)
        
        return memory_info
    except ImportError:
        print("âš ï¸ GPUtilæœªå®‰è£…ï¼Œæ— æ³•è·å–è¯¦ç»†GPUä¿¡æ¯")
        return []


def print_gpu_status():
    """æ‰“å°GPUçŠ¶æ€ä¿¡æ¯"""
    memory_info = get_gpu_memory_info()
    
    if memory_info:
        print("\nğŸ’¾ GPUå†…å­˜çŠ¶æ€:")
        print("-" * 50)
        
        for gpu in memory_info:
            print(f"GPU {gpu['id']} ({gpu['name']}):")
            print(f"  å†…å­˜ä½¿ç”¨: {gpu['memory_used']}MB / {gpu['memory_total']}MB ({gpu['memory_util']*100:.1f}%)")
            print(f"  GPUåˆ©ç”¨ç‡: {gpu['load']*100:.1f}%")
            print(f"  æ¸©åº¦: {gpu['temperature']}Â°C")
            print()
    else:
        print("â„¹ï¸ æ— æ³•è·å–è¯¦ç»†GPUä¿¡æ¯")


def print_performance_summary(results: Dict):
    """æ‰“å°æ€§èƒ½æ‘˜è¦"""
    print(f"\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®æ•°: {len(results['results'])}")
    print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {results['benchmark_info']['total_execution_time']:.2f}ç§’")
    
    # æ˜¾ç¤ºæ€§èƒ½äº®ç‚¹
    if results['results']:
        all_throughputs = [r['avg_throughput'] for r in results['results'].values()]
        all_latencies = [r['avg_latency_per_token'] for r in results['results'].values()]
        
        print(f"\nğŸ† æ€§èƒ½äº®ç‚¹:")
        print(f"   æœ€é«˜ååé‡: {max(all_throughputs):.1f} tokens/s")
        print(f"   æœ€ä½å»¶è¿Ÿ: {min(all_latencies):.4f}s/token")
        print(f"   å¹³å‡ååé‡: {np.mean(all_throughputs):.1f} tokens/s")


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description='GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•')
    parser.add_argument('--dataset-dir', default='datasets', help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', default='results', help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--config', type=str, help='æµ‹è¯•ç‰¹å®šé…ç½®ID (ä¾‹å¦‚: 0,1,3)')
    parser.add_argument('--max-samples', type=int, default=5, help='æ¯ä¸ªé…ç½®çš„æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--show-gpu-info', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†GPUä¿¡æ¯')
    
    args = parser.parse_args()
    
    print("ğŸ¯ GPT-1.5B JAX æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    print(f"ğŸ’» å¹³å°: Windows")
    print(f"ğŸ Python: 3.10")
    print(f"âš¡ CUDA: 11.8")
    print(f"ğŸ“¦ JAX: {jax.__version__}")
    
    # æ£€æŸ¥GPUç¯å¢ƒ
    if not check_gpu_setup():
        print("âŒ GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    if args.show_gpu_info:
        print_gpu_status()
    
    # åˆå§‹åŒ–æ¨¡å‹
    print(f"\nğŸ—ï¸ åˆå§‹åŒ–GPT-1.5Bæ¨¡å‹...")
    config = GPTConfig()
    model = GraphPartitionedGPT(config)
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®é›†ä»: {args.dataset_dir}")
    dataset_loader = DatasetLoader(args.dataset_dir)
    datasets = dataset_loader.get_valid_datasets()
    
    if not datasets:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®é›†ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶")
        return
    
    # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡
    dataset_loader.print_dataset_summary()
    
    # è¿‡æ»¤ç‰¹å®šé…ç½®
    if args.config:
        config_ids = [c.strip() for c in args.config.split(',')]
        filtered_datasets = {k: v for k, v in datasets.items() if k in config_ids}
        if filtered_datasets:
            datasets = filtered_datasets
            print(f"\nğŸ¯ åªæµ‹è¯•æŒ‡å®šé…ç½®: {list(datasets.keys())}")
        else:
            print(f"âŒ æŒ‡å®šçš„é…ç½® {config_ids} ä¸å­˜åœ¨")
            return
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = InferenceBenchmark(model)
    results = benchmark.run_full_benchmark(datasets, args.max_samples)
    
    # ä¿å­˜ç»“æœ
    json_file, report_file = save_benchmark_results(results, args.output_dir)
    
    # æ˜¾ç¤ºæœ€ç»ˆæ‘˜è¦
    print_performance_summary(results)
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   {report_file.name}")
    print(f"   {json_file.name}")


if __name__ == "__main__":
    main()
